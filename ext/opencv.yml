--- 
cv::gpu::gemm: |
  Performs generalized matrix multiplication.
  
  The function performs generalized matrix multiplication similar to the
  `gemm` functions in BLAS level 3. For example,
  `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)`
  corresponds to
  
  $$\texttt{dst} =  \texttt{alpha} \cdot \texttt{src1} ^T  \cdot \texttt{src2} +  \texttt{beta} \cdot \texttt{src3} ^T$$
  
  **note**
  
  Transposition operation doesn't support `CV_64FC2` input type.

cv::gpu::mulSpectrums: |-
  Performs a per-element multiplication of two Fourier spectrums.
  
  Only full (not packed) `CV_32FC2` complex spectrums in the interleaved
  format are supported for now.
cv::gpu::mulAndScaleSpectrums: |-
  Performs a per-element multiplication of two Fourier spectrums and
  scales the result.
  
  Only full (not packed) `CV_32FC2` complex spectrums in the interleaved
  format are supported for now.
cv::gpu::dft: |
  Performs a forward or inverse discrete Fourier transform (1D or 2D) of
  the floating point matrix.
  
  Use to handle real matrices ( `CV32FC1` ) and complex matrices in the
  interleaved format ( `CV32FC2` ).
  
  The source matrix should be continuous, otherwise reallocation and data
  copying is performed. The function chooses an operation mode depending
  on the flags, size, and channel count of the source matrix:
  
    If the source matrix is complex and the output is not specified as
  real, the destination matrix is complex and has the `dft_size`
  size and `CV_32FC2` type. The destination matrix contains a full
  result of the DFT (forward or inverse).
  
  
    If the source matrix is complex and the output is specified as
  real, the function assumes that its input is the result of the
  forward transform (see the next item). The destination matrix has
  the `dft_size` size and `CV_32FC1` type. It contains the result of
  the inverse DFT.
  
  
    If the source matrix is real (its type is `CV_32FC1` ), forward
  DFT is performed. The result of the DFT is packed into complex (
  `CV_32FC2` ) matrix. So, the width of the destination matrix is
  `dft_size.width / 2 + 1` . But if the source is a single column,
  the height is reduced instead of the width.

cv::gpu::Convolution: "\n\n\
  Base class for convolution (or cross-correlation) operator. :\n\n\
  ```c++\n\
  class CV_EXPORTS Convolution : public Algorithm\n\
  {\n\
  public:\n    virtual void convolve(InputArray image, InputArray templ, OutputArray result, bool ccorr = false, Stream& stream = Stream::Null()) = 0;\n\
  };\n\
  ```\n"
cv::gpu::Convolution::convolve: Computes a convolution (or cross-correlation) of two images.
cv::gpu::createConvolution: Creates implementation for :ocvgpu::Convolution .
cv::detail::focalsFromHomography: |-
  Tries to estimate focal lengths from the given homography under the
  assumption that the camera undergoes rotations around its centre only.
cv::detail::estimateFocal: Estimates focal lengths for each given camera.
cv::gpu::FGDStatModel: |-
  Class used for background/foreground segmentation. :
  
  ```c++
  class FGDStatModel
  {
  public:
      struct Params
      {
          ...
      };
  
      explicit FGDStatModel(int out_cn = 3);
      explicit FGDStatModel(const cv::gpu::GpuMat& firstFrame, const Params& params = Params(), int out_cn = 3);
  
      ~FGDStatModel();
  
      void create(const cv::gpu::GpuMat& firstFrame, const Params& params = Params());
      void release();
  
      int update(const cv::gpu::GpuMat& curFrame);
  
      //8UC3 or 8UC4 reference background image
      cv::gpu::GpuMat background;
  
      //8UC1 foreground image
      cv::gpu::GpuMat foreground;
  
      std::vector< std::vector<cv::Point> > foreground_regions;
  };
  ```
  
  
  The class discriminates between foreground and background pixels by
  building and maintaining a model of the background. Any pixel which
  does not fit this model is then deemed to be foreground. The class
  implements algorithm described in [FGD2003]_.
  
  The results are available through the class fields:
cv::gpu::FGDStatModel::FGDStatModel: "\n\n\
  Constructors."
cv::gpu::FGDStatModel::create: Initializes background model.
cv::gpu::FGDStatModel::release: Releases all inner buffer's memory.
cv::gpu::FGDStatModel::update: Updates the background model and returns foreground regions count.
cv::gpu::MOG_GPU: |-
  Gaussian Mixture-based Backbround/Foreground Segmentation Algorithm. :
  
  ```c++
  class MOG_GPU
  {
  public:
      MOG_GPU(int nmixtures = -1);
  
      void initialize(Size frameSize, int frameType);
  
      void operator()(const GpuMat& frame, GpuMat& fgmask, float learningRate = 0.0f, Stream& stream = Stream::Null());
  
      void getBackgroundImage(GpuMat& backgroundImage, Stream& stream = Stream::Null()) const;
  
      void release();
  
      int history;
      float varThreshold;
      float backgroundRatio;
      float noiseSigma;
  };
  ```
  
  
  The class discriminates between foreground and background pixels by
  building and maintaining a model of the background. Any pixel which does
  not fit this model is then deemed to be foreground. The class implements
  algorithm described in [MOG2001]_.
cv::gpu::MOG_GPU::MOG_GPU: |-
  The constructor.
  
  Default constructor sets all parameters to default values.
cv::gpu::MOG_GPU::operator(): Updates the background model and returns the foreground mask.
cv::gpu::MOG_GPU::getBackgroundImage: Computes a background image.
cv::gpu::MOG_GPU::release: Releases all inner buffer's memory.
cv::gpu::MOG2_GPU: |-
  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. :
  
  ```c++
  class MOG2_GPU
  {
  public:
      MOG2_GPU(int nmixtures = -1);
  
      void initialize(Size frameSize, int frameType);
  
      void operator()(const GpuMat& frame, GpuMat& fgmask, float learningRate = 0.0f, Stream& stream = Stream::Null());
  
      void getBackgroundImage(GpuMat& backgroundImage, Stream& stream = Stream::Null()) const;
  
      void release();
  
      // parameters
      ...
  };
  ```
  
  
  The class discriminates between foreground and background pixels by
  building and maintaining a model of the background. Any pixel which
  does not fit this model is then deemed to be foreground. The class
  implements algorithm described in [MOG2004]_.
  
  Here are important members of the class that control the algorithm,
  which you can set after constructing the class instance:
cv::gpu::MOG2_GPU::MOG2_GPU: "\n\n\
  The constructor.\n\n\
  Default constructor sets all parameters to default values."
cv::gpu::MOG2_GPU::operator(): Updates the background model and returns the foreground mask.
cv::gpu::MOG2_GPU::getBackgroundImage: Computes a background image.
cv::gpu::MOG2_GPU::release: Releases all inner buffer's memory.
cv::gpu::GMG_GPU::GMG_GPU: |-
  The default constructor.
  
  Default constructor sets all parameters to default values.
cv::gpu::GMG_GPU::initialize: Initialize background model and allocates all inner buffers.
cv::gpu::GMG_GPU::operator(): Updates the background model and returns the foreground mask
cv::gpu::GMG_GPU::release: Releases all inner buffer's memory.
cv::DataType: |-
  Template "trait" class for OpenCV primitive data types. A primitive
  OpenCV data type is one of `unsigned char`, `bool`, `signed char`,
  `unsigned short`, `signed short`, `int`, `float`, `double`, or a tuple
  of values of one of these types, where all the values in the tuple have
  the same type. Any primitive type from the list can be defined by an
  identifier in the form `CV_<bit-depth>{U|S|F}C(<number_of_channels>)`,
  for example: `uchar` ~ `CV_8UC1`, 3-element floating-point tuple ~
  `CV_32FC3`, and so on. A universal OpenCV structure that is able to
  store a single instance of such a primitive data type is :ocvVec.
  Multiple instances of such a type can be stored in a `std::vector`,
  `Mat`, `Mat_`, `SparseMat`, `SparseMat_`, or any other container that is
  able to store `Vec` instances.
  
  The `DataType` class is basically used to provide a description of such
  primitive data types without adding any fields or methods to the
  corresponding classes (and it is actually impossible to add anything to
  primitive C/C++ data types). This technique is known in C++ as class
  traits. It is not `DataType` itself that is used but its specialized
  versions, such as: :
  
  ```c++
  template<> class DataType<uchar>
  {
      typedef uchar value_type;
      typedef int work_type;
      typedef uchar channel_type;
      enum { channel_type = CV_8U, channels = 1, fmt='u', type = CV_8U };
  };
  ...
  template<typename _Tp> DataType<std::complex<_Tp> >
  {
      typedef std::complex<_Tp> value_type;
      typedef std::complex<_Tp> work_type;
      typedef _Tp channel_type;
      // DataDepth is another helper trait class
      enum { depth = DataDepth<_Tp>::value, channels=2,
          fmt=(channels-1)*256+DataDepth<_Tp>::fmt,
          type=CV_MAKETYPE(depth, channels) };
  };
  ...
  ```
  
  
  The main purpose of this class is to convert compilation-time type
  information to an OpenCV-compatible data type identifier, for example: :
  
  ```c++
  // allocates a 30x40 floating-point matrix
  Mat A(30, 40, DataType<float>::type);
  
  Mat B = Mat_<std::complex<double> >(3, 3);
  // the statement below will print 6, 2 /*, that is depth == CV_64F, channels == 2 */
  cout << B.depth() << ", " << B.channels() << endl;
  ```
  
  
  So, such traits are used to tell OpenCV which data type you are working
  with, even if such a type is not native to OpenCV. For example, the
  matrix `B` initialization above is compiled because OpenCV defines the
  proper specialized template class `DataType<complex<_Tp> >` . This
  mechanism is also useful (and used in OpenCV this way) for generic
  algorithms implementations.
cv::Point_: |
  Template class for 2D points specified by its coordinates $x$ and $y$ .
  An instance of the class is interchangeable with C structures, `CvPoint`
  and `CvPoint2D32f` . There is also a cast operator to convert point
  coordinates to the specified type. The conversion from floating-point
  coordinates to integer coordinates is done by rounding. Commonly, the
  conversion uses this operation for each of the coordinates. Besides the
  class members listed in the declaration above, the following operations
  on points are implemented: :
  
  ```c++
  pt1 = pt2 + pt3;
  pt1 = pt2 - pt3;
  pt1 = pt2 * a;
  pt1 = a * pt2;
  pt1 += pt2;
  pt1 -= pt2;
  pt1 *= a;
  double value = norm(pt); // L2 norm
  pt1 == pt2;
  pt1 != pt2;
  ```
  
  
  For your convenience, the following type aliases are defined: :
  
  ```c++
  typedef Point_<int> Point2i;
  typedef Point2i Point;
  typedef Point_<float> Point2f;
  typedef Point_<double> Point2d;
  ```
  
  
  Example: :
  
  ```c++
  Point2f a(0.3f, 0.f), b(0.f, 0.4f);
  Point pt = (a + b)*10.f;
  cout << pt.x << ", " << pt.y << endl;
  ```

cv::Point3_: |
  Template class for 3D points specified by its coordinates $x$, $y$ and
  $z$ . An instance of the class is interchangeable with the C structure
  `CvPoint2D32f` . Similarly to `Point_` , the coordinates of 3D points
  can be converted to another type. The vector arithmetic and comparison
  operations are also supported.
  
  The following `Point3_<>` aliases are available: :
  
  ```c++
  typedef Point3_<int> Point3i;
  typedef Point3_<float> Point3f;
  typedef Point3_<double> Point3d;
  ```

cv::Size_: |
  Template class for specifying the size of an image or rectangle. The
  class includes two members called `width` and `height`. The structure
  can be converted to and from the old OpenCV structures `CvSize` and
  `CvSize2D32f` . The same set of arithmetic and comparison operations as
  for `Point_` is available.
  
  OpenCV defines the following `Size_<>` aliases: :
  
  ```c++
  typedef Size_<int> Size2i;
  typedef Size2i Size;
  typedef Size_<float> Size2f;
  ```

cv::Rect_: |
  Template class for 2D rectangles, described by the following parameters:
    Coordinates of the top-left corner. This is a default interpretation
  of `Rect_::x` and `Rect_::y` in OpenCV. Though, in your algorithms
  you may count `x` and `y` from the bottom-left corner.
  
    Rectangle width and height.
  
  
  OpenCV typically assumes that the top and left boundary of the rectangle
  are inclusive, while the right and bottom boundaries are not. For
  example, the method `Rect_::contains` returns `true` if
  
  $$x  \leq pt.x < x+width,
        y  \leq pt.y < y+height$$
  
  Virtually every loop over an image ROI in OpenCV (where ROI is specified
  by `Rect_<int>` ) is implemented as: :
  
  ```c++
  for(int y = roi.y; y < roi.y + rect.height; y++)
      for(int x = roi.x; x < roi.x + rect.width; x++)
      {
          // ...
      }
  ```
  
  
  In addition to the class members, the following operations on rectangles
  are implemented:
  
  *
  :   $\texttt{rect} = \texttt{rect} \pm \texttt{point}$ (shifting a
      rectangle by a certain offset)
  
  *
  :   $\texttt{rect} = \texttt{rect} \pm \texttt{size}$ (expanding or
      shrinking a rectangle by a certain amount)
  
    `rect += point, rect -= point, rect += size, rect -= size`
  (augmenting operations)
  
  
    `rect = rect1 & rect2` (rectangle intersection)
  
  
    `rect = rect1 | rect2` (minimum area rectangle containing `rect2`
  and `rect3` )
  
  
    `rect &= rect1, rect |= rect1` (and the corresponding augmenting
  operations)
  
  
    `rect == rect1, rect != rect1` (rectangle comparison)
  
  
  This is an example how the partial ordering on rectangles can be
  established (rect1 $\subseteq$ rect2): :
  
  ```c++
  template<typename _Tp> inline bool
  operator <= (const Rect_<_Tp>& r1, const Rect_<_Tp>& r2)
  {
      return (r1 & r2) == r1;
  }
  ```
  
  
  For your convenience, the `Rect_<>` alias is available: :
  
  ```c++
  typedef Rect_<int> Rect;
  ```

cv::RotatedRect: |-
  The class represents rotated (i.e. not up-right) rectangles on a plane.
  Each rectangle is specified by the center point (mass center), length of
  each side (represented by cv::Size2f structure) and the rotation angle
  in degrees.
  
  The sample below demonstrates how to use RotatedRect:
  
  ```c++
  Mat image(200, 200, CV_8UC3, Scalar(0));
  RotatedRect rRect = RotatedRect(Point2f(100,100), Size2f(100,50), 30);
  
  Point2f vertices[4];
  rRect.points(vertices);
  for (int i = 0; i < 4; i++)
      line(image, vertices[i], vertices[(i+1)%4], Scalar(0,255,0));
  
  Rect brect = rRect.boundingRect();
  rectangle(image, brect, Scalar(255,0,0));
  
  imshow("rectangles", image);
  waitKey(0);
  ```
  
  
  ![image](pics/rotatedrect.png)
cv::TermCriteria::TermCriteria: The constructors.
cv::Matx: |
  Template class for small matrices whose type and size are known at
  compilation time: :
  
  ```c++
  template<typename _Tp, int m, int n> class Matx {...};
  
  typedef Matx<float, 1, 2> Matx12f;
  typedef Matx<double, 1, 2> Matx12d;
  ...
  typedef Matx<float, 1, 6> Matx16f;
  typedef Matx<double, 1, 6> Matx16d;
  
  typedef Matx<float, 2, 1> Matx21f;
  typedef Matx<double, 2, 1> Matx21d;
  ...
  typedef Matx<float, 6, 1> Matx61f;
  typedef Matx<double, 6, 1> Matx61d;
  
  typedef Matx<float, 2, 2> Matx22f;
  typedef Matx<double, 2, 2> Matx22d;
  ...
  typedef Matx<float, 6, 6> Matx66f;
  typedef Matx<double, 6, 6> Matx66d;
  ```
  
  
  If you need a more flexible type, use :ocvMat . The elements of the
  matrix `M` are accessible using the `M(i,j)` notation. Most of the
  common matrix operations (see also MatrixExpressions ) are available. To
  do an operation on `Matx` that is not implemented, you can easily
  convert the matrix to `Mat` and backwards. :
  
  ```c++
  Matx33f m(1, 2, 3,
            4, 5, 6,
            7, 8, 9);
  cout << sum(Mat(m*m.t())) << endl;
  ```

cv::Vec: |-
  Template class for short numerical vectors, a partial case of :ocvMatx:
  :
  
  ```c++
  template<typename _Tp, int n> class Vec : public Matx<_Tp, n, 1> {...};
  
  typedef Vec<uchar, 2> Vec2b;
  typedef Vec<uchar, 3> Vec3b;
  typedef Vec<uchar, 4> Vec4b;
  
  typedef Vec<short, 2> Vec2s;
  typedef Vec<short, 3> Vec3s;
  typedef Vec<short, 4> Vec4s;
  
  typedef Vec<int, 2> Vec2i;
  typedef Vec<int, 3> Vec3i;
  typedef Vec<int, 4> Vec4i;
  
  typedef Vec<float, 2> Vec2f;
  typedef Vec<float, 3> Vec3f;
  typedef Vec<float, 4> Vec4f;
  typedef Vec<float, 6> Vec6f;
  
  typedef Vec<double, 2> Vec2d;
  typedef Vec<double, 3> Vec3d;
  typedef Vec<double, 4> Vec4d;
  typedef Vec<double, 6> Vec6d;
  ```
  
  
  It is possible to convert `Vec<T,2>` to/from `Point_`, `Vec<T,3>`
  to/from `Point3_` , and `Vec<T,4>` to :ocvCvScalar or :ocvScalar_. Use
  `operator[]` to access the elements of `Vec`.
  
  All the expected vector operations are also implemented:
    `v1 = v2 + v3`
  
    `v1 = v2 - v3`
  
    `v1 = v2 * scale`
  
    `v1 = scale * v2`
  
    `v1 = -v2`
  
    `v1 += v2` and other augmenting operations
  
    `v1 == v2, v1 != v2`
  
    `norm(v1)` (euclidean norm)
  
  
  The `Vec` class is commonly used to describe pixel types of
  multi-channel arrays. See :ocvMat for details.
cv::Scalar_: |-
  Template class for a 4-element vector derived from Vec. :
  
  ```c++
  template<typename _Tp> class Scalar_ : public Vec<_Tp, 4> { ... };
  
  typedef Scalar_<double> Scalar;
  ```
  
  
  Being derived from `Vec<_Tp, 4>` , `Scalar_` and `Scalar` can be used
  just as typical 4-element vectors. In addition, they can be converted
  to/from `CvScalar` . The type `Scalar` is widely used in OpenCV to pass
  pixel values.
cv::Range: |
  Template class specifying a continuous subsequence (slice) of a
  sequence. :
  
  ```c++
  class Range
  {
  public:
      ...
      int start, end;
  };
  ```
  
  
  The class is used to specify a row or a column span in a matrix (
  :ocvMat ) and for many other purposes. `Range(a,b)` is basically the
  same as `a:b` in Matlab or `a..b` in Python. As in Python, `start` is an
  inclusive left boundary of the range and `end` is an exclusive right
  boundary of the range. Such a half-opened interval is usually denoted as
  $[start,end)$ .
  
  The static method `Range::all()` returns a special variable that means
  "the whole sequence" or "the whole range", just like " `:` " in Matlab
  or " `...` " in Python. All the methods and functions in OpenCV that
  take `Range` support this special `Range::all()` value. But, of course,
  in case of your own custom processing, you will probably have to check
  and handle it explicitly: :
  
  ```c++
  void my_function(..., const Range& r, ....)
  {
      if(r == Range::all()) {
          // process all the data
      }
      else {
          // process [r.start, r.end)
      }
  }
  ```

cv::KeyPoint::KeyPoint: The keypoint constructors
cv::DMatch: |
  Class for matching keypoint descriptors: query descriptor index, train
  descriptor index, train image index, and distance between descriptors. :
  
  ```c++
  class DMatch
  {
  public:
      DMatch() : queryIdx(-1), trainIdx(-1), imgIdx(-1),
                 distance(std::numeric_limits<float>::max()) {}
      DMatch( int _queryIdx, int _trainIdx, float _distance ) :
              queryIdx(_queryIdx), trainIdx(_trainIdx), imgIdx(-1),
              distance(_distance) {}
      DMatch( int _queryIdx, int _trainIdx, int _imgIdx, float _distance ) :
              queryIdx(_queryIdx), trainIdx(_trainIdx), imgIdx(_imgIdx),
              distance(_distance) {}
  
      int queryIdx; // query descriptor index
      int trainIdx; // train descriptor index
      int imgIdx;   // train image index
  
      float distance;
  
      // less is better
      bool operator<( const DMatch &m ) const;
  };
  ```

cv::Ptr: |-
  Template class for smart reference-counting pointers :
  
  ```c++
  template<typename _Tp> class Ptr
  {
  public:
      // default constructor
      Ptr();
      // constructor that wraps the object pointer
      Ptr(_Tp* _obj);
      // destructor: calls release()
      ~Ptr();
      // copy constructor; increments ptr's reference counter
      Ptr(const Ptr& ptr);
      // assignment operator; decrements own reference counter
      // (with release()) and increments ptr's reference counter
      Ptr& operator = (const Ptr& ptr);
      // increments reference counter
      void addref();
      // decrements reference counter; when it becomes 0,
      // delete_obj() is called
      void release();
      // user-specified custom object deletion operation.
      // by default, "delete obj;" is called
      void delete_obj();
      // returns true if obj == 0;
      bool empty() const;
  
      // provide access to the object fields and methods
      _Tp* operator -> ();
      const _Tp* operator -> () const;
  
      // return the underlying object pointer;
      // thanks to the methods, the Ptr<_Tp> can be
      // used instead of _Tp*
      operator _Tp* ();
      operator const _Tp*() const;
  protected:
      // the encapsulated object pointer
      _Tp* obj;
      // the associated reference counter
      int* refcount;
  };
  ```
  
  
  The `Ptr<_Tp>` class is a template class that wraps pointers of the
  corresponding type. It is similar to `shared_ptr` that is part of the
  Boost library
  (<http://www.boost.org/doc/libs/1_40_0/libs/smart_ptr/shared_ptr.htm>)
  and also part of the [C++0x](http://en.wikipedia.org/wiki/C++0x)
  standard.
  
  This class provides the following options:
  
  *
  :   Default constructor, copy constructor, and assignment operator for
      an arbitrary C++ class or a C structure. For some objects, like
      files, windows, mutexes, sockets, and others, a copy constructor or
      an assignment operator are difficult to define. For some other
      objects, like complex classifiers in OpenCV, copy constructors are
      absent and not easy to implement. Finally, some of complex OpenCV
      and your own data structures may be written in C. However, copy
      constructors and default constructors can simplify programming a
      lot.Besides, they are often required (for example, by STL
      containers). By wrapping a pointer to such a complex object `TObj`
      to `Ptr<TObj>`, you automatically get all of the necessary
      constructors and the assignment operator.
  
  *
  :   *O(1)* complexity of the above-mentioned operations. While some
      structures, like `std::vector`, provide a copy constructor and an
      assignment operator, the operations may take a considerable amount
      of time if the data structures are large. But if the structures are
      put into `Ptr<>`, the overhead is small and independent of the data
      size.
  
  *
  :   Automatic destruction, even for C structures. See the example below
      with `FILE*`.
  
  *
  :   Heterogeneous collections of objects. The standard STL and most
      other C++ and OpenCV containers can store only objects of the same
      type and the same size. The classical solution to store objects of
      different types in the same container is to store pointers to the
      base class `base_class_t*` instead but then you loose the automatic
      memory management. Again, by using `Ptr<base_class_t>()` instead of
      the raw pointers, you can solve the problem.
  
  The `Ptr` class treats the wrapped object as a black box. The reference
  counter is allocated and managed separately. The only thing the pointer
  class needs to know about the object is how to deallocate it. This
  knowledge is encapsulated in the `Ptr::delete_obj()` method that is
  called when the reference counter becomes 0. If the object is a C++
  class instance, no additional coding is needed, because the default
  implementation of this method calls `delete obj;`. However, if the
  object is deallocated in a different way, the specialized method should
  be created. For example, if you want to wrap `FILE`, the `delete_obj`
  may be implemented as follows: :
  
  ```c++
  template<> inline void Ptr<FILE>::delete_obj()
  {
      fclose(obj); // no need to clear the pointer afterwards,
                   // it is done externally.
  }
  ...
  
  // now use it:
  Ptr<FILE> f(fopen("myfile.txt", "r"));
  if(f.empty())
      throw ...;
  fprintf(f, ....);
  ...
  // the file will be closed automatically by the Ptr<FILE> destructor.
  ```
  
  
  **note**
  
  The reference increment/decrement operations are implemented as atomic operations,
  :   and therefore it is normally safe to use the classes in
      multi-threaded applications. The same is true for :ocvMat and
      other C++ OpenCV classes that operate on the reference counters.
cv::Ptr::Ptr: "\n\n\
  Various Ptr constructors."
cv::Ptr::~Ptr: The Ptr destructor.
cv::Ptr::operator =: |-
  Assignment operator.
  
  Decrements own reference counter (with `release()`) and increments ptr's
  reference counter.
cv::Ptr::addref: Increments reference counter.
cv::Ptr::release: |-
  Decrements reference counter; when it becomes 0, `delete_obj()` is
  called.
cv::Ptr::delete_obj: |-
  User-specified custom object deletion operation. By default,
  `delete obj;` is called.
cv::Ptr::empty: |-
  Returns true if obj == 0;
  
  bool empty() const;
cv::Ptr::operator ->: Provide access to the object fields and methods.
cv::Ptr::operator _Tp*: |-
  Returns the underlying object pointer. Thanks to the methods, the
  `Ptr<_Tp>` can be used instead of `_Tp*`.
cv::Mat: |
  OpenCV C++ n-dimensional dense array class :
  
  ```c++
  class CV_EXPORTS Mat
  {
  public:
      // ... a lot of methods ...
      ...
  
      /*! includes several bit-fields:
           - the magic signature
           - continuity flag
           - depth
           - number of channels
       */
      int flags;
      //! the array dimensionality, >= 2
      int dims;
      //! the number of rows and columns or (-1, -1) when the array has more than 2 dimensions
      int rows, cols;
      //! pointer to the data
      uchar* data;
  
      //! pointer to the reference counter;
      // when array points to user-allocated data, the pointer is NULL
      int* refcount;
  
      // other members
      ...
  };
  ```
  
  
  The class `Mat` represents an n-dimensional dense numerical
  single-channel or multi-channel array. It can be used to store real or
  complex-valued vectors and matrices, grayscale or color images, voxel
  volumes, vector fields, point clouds, tensors, histograms (though, very
  high-dimensional histograms may be better stored in a `SparseMat` ). The
  data layout of the array $M$ is defined by the array `M.step[]`, so that
  the address of element $(i_0,...,i_{M.dims-1})$, where
  $0\leq i_k<M.size[k]$, is computed as:
  
  $$addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}$$
  
  In case of a 2-dimensional array, the above formula is reduced to:
  
  $$addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j$$
  
  Note that `M.step[i] >= M.step[i+1]` (in fact,
  `M.step[i] >= M.step[i+1]*M.size[i+1]` ). This means that 2-dimensional
  matrices are stored row-by-row, 3-dimensional matrices are stored
  plane-by-plane, and so on. `M.step[M.dims-1]` is minimal and always
  equal to the element size `M.elemSize()` .
  
  So, the data layout in `Mat` is fully compatible with `CvMat`,
  `IplImage`, and `CvMatND` types from OpenCV 1.x. It is also compatible
  with the majority of dense array types from the standard toolkits and
  SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and
  others, that is, with any array that uses *steps* (or *strides*) to
  compute the position of a pixel. Due to this compatibility, it is
  possible to make a `Mat` header for user-allocated data and process it
  in-place using OpenCV functions.
  
  There are many different ways to create a `Mat` object. The most popular
  options are listed below:
  
  *
  
  Use the `create(nrows, ncols, type)` method or the similar
  `Mat(nrows, ncols, type[, fillValue])` constructor. A new array of the
  specified size and type is allocated. `type` has the same meaning as
  in the `cvCreateMat` method. For example, `CV_8UC1` means a 8-bit
  single-channel array, `CV_32FC2` means a 2-channel (complex)
  floating-point array, and so on.
  
  ```c++
  // make a 7x7 complex matrix filled with 1+3j.
  Mat M(7,7,CV_32FC2,Scalar(1,3));
  // and now turn M to a 100x60 15-channel 8-bit matrix.
  // The old content will be deallocated
  M.create(100,60,CV_8UC(15));
  ```
  
  
  As noted in the introduction to this chapter, `create()` allocates
  only a new array when the shape or type of the current array are
  different from the specified ones.
  
  
  *
  
  Create a multi-dimensional array:
  
  ```c++
  // create a 100x100x100 8-bit array
  int sz[] = {100, 100, 100};
  Mat bigCube(3, sz, CV_8U, Scalar::all(0));
  ```
  
  
  It passes the number of dimensions =1 to the `Mat` constructor but the
  created array will be 2-dimensional with the number of columns set to
  1. So, `Mat::dims` is always >= 2 (can also be 0 when the array is
  empty).
  
  
  *
  
  Use a copy constructor or assignment operator where there can be an
  array or expression on the right side (see below). As noted in the
  introduction, the array assignment is an O(1) operation because it
  only copies the header and increases the reference counter. The
  `Mat::clone()` method can be used to get a full (deep) copy of the
  array when you need it.
  
  
  *
  
  Construct a header for a part of another array. It can be a single
  row, single column, several rows, several columns, rectangular region
  in the array (called a *minor* in algebra) or a diagonal. Such
  operations are also O(1) because the new header references the same
  data. You can actually modify a part of the array using this feature,
  for example:
  
  ```c++
  // add the 5-th row, multiplied by 3 to the 3rd row
  M.row(3) = M.row(3) + M.row(5)*3;
  
  // now copy the 7-th column to the 1-st column
  // M.col(1) = M.col(7); // this will not work
  Mat M1 = M.col(1);
  M.col(7).copyTo(M1);
  
  // create a new 320x240 image
  Mat img(Size(320,240),CV_8UC3);
  // select a ROI
  Mat roi(img, Rect(10,10,100,100));
  // fill the ROI with (0,255,0) (which is green in RGB space);
  // the original 320x240 image will be modified
  roi = Scalar(0,255,0);
  ```
  
  
  Due to the additional `datastart` and `dataend` members, it is
  possible to compute a relative sub-array position in the main
  *container* array using `locateROI()`:
  
  ```c++
  Mat A = Mat::eye(10, 10, CV_32S);
  // extracts A columns, 1 (inclusive) to 3 (exclusive).
  Mat B = A(Range::all(), Range(1, 3));
  // extracts B rows, 5 (inclusive) to 9 (exclusive).
  // that is, C ~ A(Range(5, 9), Range(1, 3))
  Mat C = B(Range(5, 9), Range::all());
  Size size; Point ofs;
  C.locateROI(size, ofs);
  // size will be (width=10,height=10) and the ofs will be (x=1, y=5)
  ```
  
  
  As in case of whole matrices, if you need a deep copy, use the
  `clone()` method of the extracted sub-matrices.
  
  
  *
  
  Make a header for user-allocated data. It can be useful to do the
  following:
  
  #.
  :   Process "foreign" data using OpenCV (for example, when you
      implement a DirectShow* filter or a processing module for
      `gstreamer`, and so on). For example:
  
  ```c++
      void process_video_frame(const unsigned char* pixels,
                               int width, int height, int step)
      {
          Mat img(height, width, CV_8UC3, pixels, step);
          GaussianBlur(img, img, Size(7,7), 1.5, 1.5);
      }
  ```
  
  
  #.
  :   Quickly initialize small matrices and/or get a super-fast element
      access.
  
  ```c++
      double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};
      Mat M = Mat(3, 3, CV_64F, m).inv();
  ```
  
  
  Partial yet very common cases of this *user-allocated data* case are
  conversions from `CvMat` and `IplImage` to `Mat`. For this purpose,
  there are special constructors taking pointers to `CvMat` or
  `IplImage` and the optional flag indicating whether to copy the data
  or not.
  
  Backward conversion from `Mat` to `CvMat` or `IplImage` is provided
  via cast operators `Mat::operator CvMat() const` and
  `Mat::operator IplImage()`. The operators do NOT copy the data.
  
  
  ```c++
  IplImage* img = cvLoadImage("greatwave.jpg", 1);
  Mat mtx(img); // convert IplImage* -> Mat
  CvMat oldmat = mtx; // convert Mat -> CvMat
  CV_Assert(oldmat.cols == img->width && oldmat.rows == img->height &&
      oldmat.data.ptr == (uchar*)img->imageData && oldmat.step == img->widthStep);
  ```
  
  
  
  *
  
  Use MATLAB-style array initializers, `zeros(), ones(), eye()`, for
  example:
  
  ```c++
  // create a double-precision identity martix and add it to M.
  M += Mat::eye(M.rows, M.cols, CV_64F);
  ```
  
  
  
  *
  
  Use a comma-separated initializer:
  
  ```c++
  // create a 3x3 double-precision identity matrix
  Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1);
  ```
  
  
  With this approach, you first call a constructor of the :ocvMat_
  class with the proper parameters, and then you just put `<<` operator
  followed by comma-separated values that can be constants, variables,
  expressions, and so on. Also, note the extra parentheses required to
  avoid compilation errors.
  
  
  Once the array is created, it is automatically managed via a
  reference-counting mechanism. If the array header is built on top of
  user-allocated data, you should handle the data by yourself. The array
  data is deallocated when no one points to it. If you want to release the
  data pointed by a array header before the array destructor is called,
  use `Mat::release()` .
  
  The next important thing to learn about the array class is element
  access. This manual already described how to compute an address of each
  array element. Normally, you are not required to use the formula
  directly in the code. If you know the array element type (which can be
  retrieved using the method `Mat::type()` ), you can access the element
  $M_{ij}$ of a 2-dimensional array as: :
  
  ```c++
  M.at<double>(i,j) += 1.f;
  ```
  
  
  assuming that M is a double-precision floating-point array. There are
  several variants of the method `at` for a different number of
  dimensions.
  
  If you need to process a whole row of a 2D array, the most efficient way
  is to get the pointer to the row first, and then just use the plain C
  operator `[]` : :
  
  ```c++
  // compute sum of positive matrix elements
  // (assuming that M isa double-precision matrix)
  double sum=0;
  for(int i = 0; i < M.rows; i++)
  {
      const double* Mi = M.ptr<double>(i);
      for(int j = 0; j < M.cols; j++)
          sum += std::max(Mi[j], 0.);
  }
  ```
  
  
  Some operations, like the one above, do not actually depend on the array
  shape. They just process elements of an array one by one (or elements
  from multiple arrays that have the same coordinates, for example, array
  addition). Such operations are called *element-wise*. It makes sense to
  check whether all the input/output arrays are continuous, namely, have
  no gaps at the end of each row. If yes, process them as a long single
  row: :
  
  ```c++
  // compute the sum of positive matrix elements, optimized variant
  double sum=0;
  int cols = M.cols, rows = M.rows;
  if(M.isContinuous())
  {
      cols *= rows;
      rows = 1;
  }
  for(int i = 0; i < rows; i++)
  {
      const double* Mi = M.ptr<double>(i);
      for(int j = 0; j < cols; j++)
          sum += std::max(Mi[j], 0.);
  }
  ```
  
  
  In case of the continuous matrix, the outer loop body is executed just
  once. So, the overhead is smaller, which is especially noticeable in
  case of small matrices.
  
  Finally, there are STL-style iterators that are smart enough to skip
  gaps between successive rows: :
  
  ```c++
  // compute sum of positive matrix elements, iterator-based variant
  double sum=0;
  MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();
  for(; it != it_end; ++it)
      sum += std::max(*it, 0.);
  ```
  
  
  The matrix iterators are random-access iterators, so they can be passed
  to any STL algorithm, including `std::sort()` .
  
  Initializes matrix header (lightweight variant).
  
  Initializes a matrix header and assigns data to it. The matrix is filled
  *row*-wise (the first `cols` elements of data form the first row of the
  matrix, etc.)
  
  This function is a fast inline substitution for :ocvInitMatHeader.
  Namely, it is equivalent to: :
  
  ```c++
  CvMat mat;
  cvInitMatHeader(&mat, rows, cols, type, data, CV_AUTOSTEP);
  ```

cv::Matrix Expressions: |-
  This is a list of implemented matrix operations that can be combined in
  arbitrary complex expressions (here `A`, `B` stand for matrices ( `Mat`
  ), `s` for a scalar ( `Scalar` ), `alpha` for a real-valued scalar (
  `double` )):
  
  *
  :   Addition, subtraction, negation: `A+B, A-B, A+s, A-s, s+A, s-A, -A`
  
  *
  :   Scaling: `A*alpha`
  
  *
  :   Per-element multiplication and division: `A.mul(B), A/B, alpha/A`
  
  *
  :   Matrix multiplication: `A*B`
  
  *
  :   Transposition: `A.t()` (means `A`^T^)
  
  *
  :   Matrix inversion and pseudo-inversion, solving linear systems and
      least-squares problems:
  
  ```c++
  `A.inv([method])` (\~ `A`^-1^) `,   A.inv([method])*B` (\~
  `X: AX=B`)
  ```
  
  
  *
  :   Comparison: `A cmpop B, A cmpop alpha, alpha cmpop A`, where `cmpop`
      is one of `:  >, >=, ==, !=, <=, <`. The result of comparison is an
      8-bit single channel mask whose elements are set to 255 (if the
      particular element or pair of elements satisfy the condition) or 0.
  
  *
  :   Bitwise logical operations:
      `A logicop B, A logicop s, s logicop A, ~A`, where `logicop` is one
      of `:  &, |, ^`.
  
  *
  :   Element-wise minimum and maximum:
      `min(A, B), min(A, alpha), max(A, B), max(A, alpha)`
  
  *
  :   Element-wise absolute value: `abs(A)`
  
  *
  :   Cross-product, dot-product: `A.cross(B)` `A.dot(B)`
  
  *
  :   Any function of matrix or matrices and scalars that returns a matrix
      or a scalar, such as `norm`, `mean`, `sum`, `countNonZero`, `trace`,
      `determinant`, `repeat`, and others.
  
  *
  :   Matrix initializers ( `Mat::eye(), Mat::zeros(), Mat::ones()` ),
      matrix comma-separated initializers, matrix constructors and
      operators that extract sub-matrices (see :ocvMat description).
  
  *
  :   `Mat_<destination_type>()` constructors to cast the result to the
      proper type.
  
  **note**
  
  Comma-separated initializers and probably some other operations may
  require additional explicit `Mat()` or `Mat_<T>()` constructor calls
  to resolve a possible ambiguity.
  
  
  Here are examples of matrix expressions:
  
  ```c++
  // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD)
  SVD svd(A);
  Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t();
  
  // compute the new vector of parameters in the Levenberg-Marquardt algorithm
  x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err);
  
  // sharpen image using "unsharp mask" algorithm
  Mat blurred; double sigma = 1, threshold = 5, amount = 1;
  GaussianBlur(img, blurred, Size(), sigma, sigma);
  Mat lowConstrastMask = abs(img - blurred) < threshold;
  Mat sharpened = img*(1+amount) + blurred*(-amount);
  img.copyTo(sharpened, lowContrastMask);
  ```
  
  
  Below is the formal description of the `Mat` methods.
cv::Mat::Mat: |-
  Various Mat constructors
  
  These are various constructors that form a matrix. As noted in the
  AutomaticAllocation, often the default constructor is enough, and the
  proper matrix will be allocated by an OpenCV function. The constructed
  matrix can further be assigned to another matrix or matrix expression or
  can be allocated with :ocvMat::create . In the former case, the old
  content is de-referenced.
cv::Mat::~Mat: |-
  The Mat destructor.
  
  The matrix destructor calls :ocvMat::release .
cv::Mat::operator =: |-
  Provides matrix assignment operators.
  
  These are available assignment operators. Since they all are very
  different, make sure to read the operator parameters description.
cv::Mat::row: |+
  Creates a matrix header for the specified matrix row.
  
  The method makes a new header for the specified matrix row and returns
  it. This is an O(1) operation, regardless of the matrix size. The
  underlying data of the new matrix is shared with the original matrix.
  Here is the example of one of the classical basic matrix processing
  operations, `axpy`, used by LU and many other algorithms: :
  
  ```c++
  inline void matrix_axpy(Mat& A, int i, int j, double alpha)
  {
      A.row(i) += A.row(j)*alpha;
  }
  ```
  
  
  **note**
  
  In the current implementation, the following code does not work as
  expected: :
  
  ```c++
  Mat A;
  ...
  A.row(i) = A.row(j); // will not work
  ```
  
  
  This happens because `A.row(i)` forms a temporary header that is
  further assigned to another header. Remember that each of these
  operations is O(1), that is, no data is copied. Thus, the above
  assignment is not true if you may have expected the j-th row to be
  copied to the i-th row. To achieve that, you should either turn this
  simple assignment into an expression or use the :ocvMat::copyTo
  method: :
  
  ```c++
  Mat A;
  ...
  // works, but looks a bit obscure.
  A.row(i) = A.row(j) + 0;
  
  // this is a bit longer, but the recommended method.
  A.row(j).copyTo(A.row(i));
  ```
  
cv::Mat::col: |-
  Creates a matrix header for the specified matrix column.
  
  The method makes a new header for the specified matrix column and
  returns it. This is an O(1) operation, regardless of the matrix size.
  The underlying data of the new matrix is shared with the original
  matrix. See also the :ocvMat::row description.
cv::Mat::rowRange: |-
  Creates a matrix header for the specified row span.
  
  The method makes a new header for the specified row span of the matrix.
  Similarly to :ocvMat::row and :ocvMat::col , this is an O(1) operation.
cv::Mat::colRange: |-
  Creates a matrix header for the specified row span.
  
  The method makes a new header for the specified column span of the
  matrix. Similarly to :ocvMat::row and :ocvMat::col , this is an O(1)
  operation.
cv::Mat::diag: "Extracts a diagonal from a matrix, or creates a diagonal matrix.\n\n\
  The method makes a new header for the specified matrix diagonal. The new\n\
  matrix is represented as a single-column matrix. Similarly to\n\
  :ocvMat::row and :ocvMat::col , this is an O(1) operation."
cv::Mat::clone: |-
  Creates a full copy of the array and the underlying data.
  
  The method creates a full copy of the array. The original `step[]` is
  not taken into account. So, the array copy is a continuous array
  occupying `total()*elemSize()` bytes.
cv::Mat::copyTo: |-
  Copies the matrix to another one.
  
  The method copies the matrix data to another matrix. Before copying the
  data, the method invokes :
  
  ```c++
  m.create(this->size(), this->type);
  ```
  
  
  so that the destination matrix is reallocated if needed. While
  `m.copyTo(m);` works flawlessly, the function does not handle the case
  of a partial overlap between the source and the destination matrices.
  
  When the operation mask is specified, and the `Mat::create` call shown
  above reallocated the matrix, the newly allocated matrix is initialized
  with all zeros before copying the data.
cv::Mat::convertTo: |-
  Converts an array to another data type with optional scaling.
  
  The method converts source pixel values to the target data type.
  `saturate_cast<>` is applied at the end to avoid possible overflows:
  
  $$m(x,y) = saturate _ cast<rType>( \alpha (*this)(x,y) +  \beta )$$
cv::Mat::assignTo: |-
  Provides a functional form of `convertTo`.
  
  This is an internally used method called by the MatrixExpressions
  engine.
cv::Mat::setTo: Sets all or some of the array elements to the specified value.
cv::Mat::reshape: |
  Changes the shape and/or the number of channels of a 2D matrix without
  copying the data.
  
  The method makes a new matrix header for `*this` elements. The new
  matrix may have a different size and/or different number of channels.
  Any combination is possible if:
  
  *
  :   No extra elements are included into the new matrix and no elements
      are excluded. Consequently, the product `rows*cols*channels()` must
      stay the same after the transformation.
  
  *
  :   No data is copied. That is, this is an O(1) operation. Consequently,
      if you change the number of rows, or the operation changes the
      indices of elements row in some other way, the matrix must be
      continuous. See :ocvMat::isContinuous .
  
  For example, if there is a set of 3D points stored as an STL vector, and
  you want to represent the points as a `3xN` matrix, do the following: :
  
  ```c++
  std::vector<Point3f> vec;
  ...
  
  Mat pointMat = Mat(vec). // convert vector to Mat, O(1) operation
                    reshape(1). // make Nx3 1-channel matrix out of Nx1 3-channel.
                                // Also, an O(1) operation
                       t(); // finally, transpose the Nx3 matrix.
                            // This involves copying all the elements
  ```

cv::Mat::t: |
  Transposes a matrix.
  
  The method performs matrix transposition by means of matrix expressions.
  It does not perform the actual transposition but returns a temporary
  matrix transposition object that can be further used as a part of more
  complex matrix expressions or can be assigned to a matrix: :
  
  ```c++
  Mat A1 = A + Mat::eye(A.size(), A.type)*lambda;
  Mat C = A1.t()*A1; // compute (A + lambda*I)^t * (A + lamda*I)
  ```

cv::Mat::inv: |-
  Inverses a matrix.
  
  The method performs a matrix inversion by means of matrix expressions.
  This means that a temporary matrix inversion object is returned by the
  method and can be used further as a part of more complex matrix
  expressions or can be assigned to a matrix.
cv::Mat::mul: |
  Performs an element-wise multiplication or division of the two matrices.
  
  The method returns a temporary object encoding per-element array
  multiplication, with optional scale. Note that this is not a matrix
  multiplication that corresponds to a simpler "*" operator.
  
  Example: :
  
  ```c++
  Mat C = A.mul(5/B); // equivalent to divide(A, B, C, 5)
  ```

cv::Mat::cross: |-
  Computes a cross-product of two 3-element vectors.
  
  The method computes a cross-product of two 3-element vectors. The
  vectors must be 3-element floating-point vectors of the same shape and
  size. The result is another 3-element vector of the same shape and type
  as operands.
cv::Mat::dot: |-
  Computes a dot-product of two vectors.
  
  The method computes a dot-product of two matrices. If the matrices are
  not single-column or single-row vectors, the top-to-bottom left-to-right
  scan ordering is used to treat them as 1D vectors. The vectors must have
  the same size and type. If the matrices have more than one channel, the
  dot products from all the channels are summed together.
cv::Mat::zeros: |-
  Returns a zero array of the specified size and type.
  
  The method returns a Matlab-style zero array initializer. It can be used
  to quickly form a constant array as a function parameter, part of a
  matrix expression, or as a matrix initializer. :
  
  ```c++
  Mat A;
  A = Mat::zeros(3, 3, CV_32F);
  ```
  
  
  In the example above, a new matrix is allocated only if `A` is not a 3x3
  floating-point matrix. Otherwise, the existing matrix `A` is filled with
  zeros.
cv::Mat::ones: "Returns an array of all 1's of the specified size and type.\n\n\
  The method returns a Matlab-style 1's array initializer, similarly to\n\
  :ocvMat::zeros. Note that using this method you can initialize an array\n\
  with an arbitrary value, using the following Matlab idiom: :\n\n\
  ```c++\n\
  Mat A = Mat::ones(100, 100, CV_8U)*3; // make 100x100 matrix filled with 3.\n\
  ```\n\n\n\
  The above operation does not form a 100x100 matrix of 1's and then\n\
  multiply it by 3. Instead, it just remembers the scale factor (3 in this\n\
  case) and use it when actually invoking the matrix initializer."
cv::Mat::eye: |
  Returns an identity matrix of the specified size and type.
  
  The method returns a Matlab-style identity matrix initializer, similarly
  to :ocvMat::zeros. Similarly to :ocvMat::ones, you can use a scale
  operation to create a scaled identity matrix efficiently: :
  
  ```c++
  // make a 4x4 diagonal matrix with 0.1's on the diagonal.
  Mat A = Mat::eye(4, 4, CV_32F)*0.1;
  ```

cv::Mat::create: |-
  Allocates new array data if needed.
  
  This is one of the key `Mat` methods. Most new-style OpenCV functions
  and methods that produce arrays call this method for each output array.
  The method uses the following algorithm:
  
  #.
  :   If the current array shape and the type match the new ones, return
      immediately. Otherwise, de-reference the previous data by calling
      :ocvMat::release.
  
  #.
  :   Initialize the new header.
  
  #.
  :   Allocate the new data of `total()*elemSize()` bytes.
  
  #.
  :   Allocate the new, associated with the data, reference counter and
      set it to 1.
  
  Such a scheme makes the memory management robust and efficient at the
  same time and helps avoid extra typing for you. This means that usually
  there is no need to explicitly allocate output arrays. That is, instead
  of writing: :
  
  ```c++
  Mat color;
  ...
  Mat gray(color.rows, color.cols, color.depth());
  cvtColor(color, gray, COLOR_BGR2GRAY);
  ```
  
  
  you can simply write: :
  
  ```c++
  Mat color;
  ...
  Mat gray;
  cvtColor(color, gray, COLOR_BGR2GRAY);
  ```
  
  
  because `cvtColor` , as well as the most of OpenCV functions, calls
  `Mat::create()` for the output array internally.
cv::Mat::addref: "Increments the reference counter.\n\n\
  The method increments the reference counter associated with the matrix\n\
  data. If the matrix header points to an external data set (see\n\
  :ocvMat::Mat ), the reference counter is NULL, and the method has no\n\
  effect in this case. Normally, to avoid memory leaks, the method should\n\
  not be called explicitly. It is called implicitly by the matrix\n\
  assignment operator. The reference counter increment is an atomic\n\
  operation on the platforms that support it. Thus, it is safe to operate\n\
  on the same matrices asynchronously in different threads."
cv::Mat::release: "Decrements the reference counter and deallocates the matrix if needed.\n\n\
  The method decrements the reference counter associated with the matrix\n\
  data. When the reference counter reaches 0, the matrix data is\n\
  deallocated and the data and the reference counter pointers are set to\n\
  NULL's. If the matrix header points to an external data set (see\n\
  :ocvMat::Mat ), the reference counter is NULL, and the method has no\n\
  effect in this case.\n\n\
  This method can be called manually to force the matrix data\n\
  deallocation. But since this method is automatically called in the\n\
  destructor, or by any other method that changes the data pointer, it is\n\
  usually not needed. The reference counter decrement and check for 0 is\n\
  an atomic operation on the platforms that support it. Thus, it is safe\n\
  to operate on the same matrices asynchronously in different threads."
cv::Mat::resize: |-
  Changes the number of matrix rows.
  
  The methods change the number of matrix rows. If the matrix is
  reallocated, the first `min(Mat::rows, sz)` rows are preserved. The
  methods emulate the corresponding methods of the STL vector class.
cv::Mat::reserve: |-
  Reserves space for the certain number of rows.
  
  The method reserves space for `sz` rows. If the matrix already has
  enough space to store `sz` rows, nothing happens. If the matrix is
  reallocated, the first `Mat::rows` rows are preserved. The method
  emulates the corresponding method of the STL vector class.
cv::Mat::push_back: |-
  Adds elements to the bottom of the matrix.
  
  The methods add one or more elements to the bottom of the matrix. They
  emulate the corresponding method of the STL vector class. When `elem` is
  `Mat` , its type and the number of columns must be the same as in the
  container matrix.
cv::Mat::pop_back: |-
  Removes elements from the bottom of the matrix.
  
  The method removes one or more rows from the bottom of the matrix.
cv::Mat::locateROI: "Locates the matrix header within a parent matrix.\n\n\
  After you extracted a submatrix from a matrix using :ocvMat::row,\n\
  :ocvMat::col, :ocvMat::rowRange, :ocvMat::colRange , and others, the\n\
  resultant submatrix points just to the part of the original big matrix.\n\
  However, each submatrix contains information (represented by `datastart`\n\
  and `dataend` fields) that helps reconstruct the original matrix size\n\
  and the position of the extracted submatrix within the original matrix.\n\
  The method `locateROI` does exactly that."
cv::Mat::adjustROI: "Adjusts a submatrix size and position within the parent matrix.\n\n\
  The method is complimentary to :ocvMat::locateROI . The typical use of\n\
  these functions is to determine the submatrix position within the parent\n\
  matrix and then shift the position somehow. Typically, it can be\n\
  required for filtering operations when pixels outside of the ROI should\n\
  be taken into account. When all the method parameters are positive, the\n\
  ROI needs to grow in all directions by the specified amount, for\n\
  example: :\n\n\
  ```c++\n\
  A.adjustROI(2, 2, 2, 2);\n\
  ```\n\n\n\
  In this example, the matrix size is increased by 4 elements in each\n\
  direction. The matrix is shifted by 2 elements to the left and 2\n\
  elements up, which brings in all the necessary pixels for the filtering\n\
  with the 5x5 kernel.\n\n\
  `adjustROI` forces the adjusted ROI to be inside of the parent matrix\n\
  that is boundaries of the adjusted ROI are constrained by boundaries of\n\
  the parent matrix. For example, if the submatrix `A` is located in the\n\
  first row of a parent matrix and you called `A.adjustROI(2, 2, 2, 2)`\n\
  then `A` will not be increased in the upward direction.\n\n\
  The function is used internally by the OpenCV filtering functions, like\n\
  :ocvfilter2D , morphological operations, and so on."
cv::Mat::operator(): "Extracts a rectangular submatrix.\n\n\
  The operators make a new header for the specified sub-array of `*this` .\n\
  They are the most generalized forms of :ocvMat::row, :ocvMat::col,\n\
  :ocvMat::rowRange, and :ocvMat::colRange . For example,\n\
  `A(Range(0, 10), Range::all())` is equivalent to `A.rowRange(0, 10)` .\n\
  Similarly to all of the above, the operators are O(1) operations, that\n\
  is, no matrix data is copied."
cv::Mat::total: |-
  Returns the total number of array elements.
  
  The method returns the number of array elements (a number of pixels if
  the array represents an image).
cv::Mat::isContinuous: |-
  Reports whether the matrix is continuous or not.
  
  The method returns `true` if the matrix elements are stored continuously
  without gaps at the end of each row. Otherwise, it returns `false`.
  Obviously, `1x1` or `1xN` matrices are always continuous. Matrices
  created with :ocvMat::create are always continuous. But if you extract a
  part of the matrix using :ocvMat::col, :ocvMat::diag , and so on, or
  constructed a matrix header for externally allocated data, such matrices
  may no longer have this property.
  
  The continuity flag is stored as a bit in the `Mat::flags` field and is
  computed automatically when you construct a matrix header. Thus, the
  continuity check is a very fast operation, though theoretically it could
  be done as follows: :
  
  ```c++
  // alternative implementation of Mat::isContinuous()
  bool myCheckMatContinuity(const Mat& m)
  {
      //return (m.flags & Mat::CONTINUOUS_FLAG) != 0;
      return m.rows == 1 || m.step == m.cols*m.elemSize();
  }
  ```
  
  
  The method is used in quite a few of OpenCV functions. The point is that
  element-wise operations (such as arithmetic and logical operations, math
  functions, alpha blending, color space transformations, and others) do
  not depend on the image geometry. Thus, if all the input and output
  arrays are continuous, the functions can process them as very long
  single-row vectors. The example below illustrates how an alpha-blending
  function can be implemented. :
  
  ```c++
  template<typename T>
  void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)
  {
      const float alpha_scale = (float)std::numeric_limits<T>::max(),
                  inv_scale = 1.f/alpha_scale;
  
      CV_Assert( src1.type() == src2.type() &&
                 src1.type() == CV_MAKETYPE(DataType<T>::depth, 4) &&
                 src1.size() == src2.size());
      Size size = src1.size();
      dst.create(size, src1.type());
  
      // here is the idiom: check the arrays for continuity and,
      // if this is the case,
      // treat the arrays as 1D vectors
      if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )
      {
          size.width *= size.height;
          size.height = 1;
      }
      size.width *= 4;
  
      for( int i = 0; i < size.height; i++ )
      {
          // when the arrays are continuous,
          // the outer loop is executed only once
          const T* ptr1 = src1.ptr<T>(i);
          const T* ptr2 = src2.ptr<T>(i);
          T* dptr = dst.ptr<T>(i);
  
          for( int j = 0; j < size.width; j += 4 )
          {
              float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale;
              dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);
              dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta);
              dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta);
              dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale);
          }
      }
  }
  ```
  
  
  This approach, while being very simple, can boost the performance of a
  simple element-operation by 10-20 percents, especially if the image is
  rather small and the operation is quite simple.
  
  Another OpenCV idiom in this function, a call of :ocvMat::create for the
  destination array, that allocates the destination array unless it
  already has the proper size and type. And while the newly allocated
  arrays are always continuous, you still need to check the destination
  array because :ocvMat::create does not always allocate a new matrix.
cv::Mat::elemSize: |-
  Returns the matrix element size in bytes.
  
  The method returns the matrix element size in bytes. For example, if the
  matrix type is `CV_16SC3` , the method returns `3*sizeof(short)` or 6.
cv::Mat::elemSize1: |-
  Returns the size of each matrix element channel in bytes.
  
  The method returns the matrix element channel size in bytes, that is, it
  ignores the number of channels. For example, if the matrix type is
  `CV_16SC3` , the method returns `sizeof(short)` or 2.
cv::Mat::type: |-
  Returns the type of a matrix element.
  
  The method returns a matrix element type. This is an identifier
  compatible with the `CvMat` type system, like `CV_16SC3` or 16-bit
  signed 3-channel array, and so on.
cv::Mat::depth: |
  Returns the depth of a matrix element.
  
  The method returns the identifier of the matrix element depth (the type
  of each individual channel). For example, for a 16-bit signed element
  array, the method returns `CV_16S` . A complete list of matrix types
  contains the following values:
  
    `CV_8U` - 8-bit unsigned integers ( `0..255` )
  
  
    `CV_8S` - 8-bit signed integers ( `-128..127` )
  
  
    `CV_16U` - 16-bit unsigned integers ( `0..65535` )
  
  
    `CV_16S` - 16-bit signed integers ( `-32768..32767` )
  
  
    `CV_32S` - 32-bit signed integers ( `-2147483648..2147483647` )
  
  
    `CV_32F` - 32-bit floating-point numbers (
  `-FLT_MAX..FLT_MAX, INF, NAN` )
  
  
    `CV_64F` - 64-bit floating-point numbers (
  `-DBL_MAX..DBL_MAX, INF, NAN` )

cv::Mat::channels: |-
  Returns the number of matrix channels.
  
  The method returns the number of matrix channels.
cv::Mat::step1: |-
  Returns a normalized step.
  
  The method returns a matrix step divided by :ocvMat::elemSize1() . It
  can be useful to quickly access an arbitrary matrix element.
cv::Mat::size: |-
  Returns a matrix size.
  
  The method returns a matrix size: `Size(cols, rows)` . When the matrix
  is more than 2-dimensional, the returned size is (-1, -1).
cv::Mat::empty: |-
  Returns `true` if the array has no elements.
  
  The method returns `true` if `Mat::total()` is 0 or if `Mat::data` is
  NULL. Because of `pop_back()` and `resize()` methods `M.total() == 0`
  does not imply that `M.data == NULL` .
cv::Mat::ptr: |-
  Returns a pointer to the specified matrix row.
  
  The methods return `uchar*` or typed pointer to the specified matrix
  row. See the sample in :ocvMat::isContinuous to know how to use these
  methods.
cv::Mat::at: |
  Returns a reference to the specified array element.
  
  The template methods return a reference to the specified array element.
  For the sake of higher performance, the index range checks are only
  performed in the Debug configuration.
  
  Note that the variants with a single index (i) can be used to access
  elements of single-row or single-column 2-dimensional arrays. That is,
  if, for example, `A` is a `1 x N` floating-point matrix and `B` is an
  `M x 1` integer matrix, you can simply write `A.at<float>(k+4)` and
  `B.at<int>(2*i+1)` instead of `A.at<float>(0,k+4)` and
  `B.at<int>(2*i+1,0)` , respectively.
  
  The example below initializes a Hilbert matrix: :
  
  ```c++
  Mat H(100, 100, CV_64F);
  for(int i = 0; i < H.rows; i++)
      for(int j = 0; j < H.cols; j++)
          H.at<double>(i,j)=1./(i+j+1);
  ```

cv::Mat::begin: |
  Returns the matrix iterator and sets it to the first matrix element.
  
  The methods return the matrix read-only or read-write iterators. The use
  of matrix iterators is very similar to the use of bi-directional STL
  iterators. In the example below, the alpha blending function is
  rewritten using the matrix iterators: :
  
  ```c++
  template<typename T>
  void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)
  {
      typedef Vec<T, 4> VT;
  
      const float alpha_scale = (float)std::numeric_limits<T>::max(),
                  inv_scale = 1.f/alpha_scale;
  
      CV_Assert( src1.type() == src2.type() &&
                 src1.type() == DataType<VT>::type &&
                 src1.size() == src2.size());
      Size size = src1.size();
      dst.create(size, src1.type());
  
      MatConstIterator_<VT> it1 = src1.begin<VT>(), it1_end = src1.end<VT>();
      MatConstIterator_<VT> it2 = src2.begin<VT>();
      MatIterator_<VT> dst_it = dst.begin<VT>();
  
      for( ; it1 != it1_end; ++it1, ++it2, ++dst_it )
      {
          VT pix1 = *it1, pix2 = *it2;
          float alpha = pix1[3]*inv_scale, beta = pix2[3]*inv_scale;
          *dst_it = VT(saturate_cast<T>(pix1[0]*alpha + pix2[0]*beta),
                       saturate_cast<T>(pix1[1]*alpha + pix2[1]*beta),
                       saturate_cast<T>(pix1[2]*alpha + pix2[2]*beta),
                       saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale));
      }
  }
  ```

cv::Mat::end: |-
  Returns the matrix iterator and sets it to the after-last matrix
  element.
  
  The methods return the matrix read-only or read-write iterators, set to
  the point following the last matrix element.
cv::Mat_: |
  Template matrix class derived from :ocvMat . :
  
  ```c++
  template<typename _Tp> class Mat_ : public Mat
  {
  public:
      // ... some specific methods
      //         and
      // no new extra fields
  };
  ```
  
  
  The class `Mat_<_Tp>` is a "thin" template wrapper on top of the `Mat`
  class. It does not have any extra data fields. Nor this class nor `Mat`
  has any virtual methods. Thus, references or pointers to these two
  classes can be freely but carefully converted one to another. For
  example: :
  
  ```c++
  // create a 100x100 8-bit matrix
  Mat M(100,100,CV_8U);
  // this will be compiled fine. no any data conversion will be done.
  Mat_<float>& M1 = (Mat_<float>&)M;
  // the program is likely to crash at the statement below
  M1(99,99) = 1.f;
  ```
  
  
  While `Mat` is sufficient in most cases, `Mat_` can be more convenient
  if you use a lot of element access operations and if you know matrix
  type at the compilation time. Note that `Mat::at<_Tp>(int y, int x)` and
  `Mat_<_Tp>::operator ()(int y, int x)` do absolutely the same and run at
  the same speed, but the latter is certainly shorter: :
  
  ```c++
  Mat_<double> M(20,20);
  for(int i = 0; i < M.rows; i++)
      for(int j = 0; j < M.cols; j++)
          M(i,j) = 1./(i+j+1);
  Mat E, V;
  eigen(M,E,V);
  cout << E.at<double>(0,0)/E.at<double>(M.rows-1,0);
  ```
  
  
  To use `Mat_` for multi-channel images/matrices, pass `Vec` as a `Mat_`
  parameter: :
  
  ```c++
  // allocate a 320x240 color image and fill it with green (in RGB space)
  Mat_<Vec3b> img(240, 320, Vec3b(0,255,0));
  // now draw a diagonal white line
  for(int i = 0; i < 100; i++)
      img(i,i)=Vec3b(255,255,255);
  // and now scramble the 2nd (red) channel of each pixel
  for(int i = 0; i < img.rows; i++)
      for(int j = 0; j < img.cols; j++)
          img(i,j)[2] ^= (uchar)(i ^ j);
  ```

cv::InputArray: |-
  This is the proxy class for passing read-only input arrays into OpenCV
  functions. It is defined as :
  
  ```c++
  typedef const _InputArray& InputArray;
  ```
  
  
  where `_InputArray` is a class that can be constructed from `Mat`,
  `Mat_<T>`, `Matx<T, m, n>`, `std::vector<T>`,
  `std::vector<std::vector<T> >` or `std::vector<Mat>`. It can also be
  constructed from a matrix expression.
  
  Since this is mostly implementation-level class, and its interface may
  change in future versions, we do not describe it in details. There are a
  few key things, though, that should be kept in mind:
  
    When you see in the reference manual or in OpenCV source code a
  function that takes `InputArray`, it means that you can actually
  pass `Mat`, `Matx`, `vector<T>` etc. (see above the complete
  list).
  
  
    Optional input arguments: If some of the input arrays may be
  empty, pass `cv::noArray()` (or simply `cv::Mat()` as you probably
  did before).
  
  
    The class is designed solely for passing parameters. That is,
  normally you *should not* declare class members, local and global
  variables of this type.
  
  
    If you want to design your own function or a class method that can
  operate of arrays of multiple types, you can use `InputArray` (or
  `OutputArray`) for the respective parameters. Inside a function
  you should use `_InputArray::getMat()` method to construct a
  matrix header for the array (without copying data).
  `_InputArray::kind()` can be used to distinguish `Mat` from
  `vector<>` etc., but normally it is not needed.
  
  
  Here is how you can use a function that takes `InputArray` :
  
  
  ```c++
  std::vector<Point2f> vec;
  // points or a circle
  for( int i = 0; i < 30; i++ )
      vec.push_back(Point2f((float)(100 + 30*cos(i*CV_PI*2/5)),
                            (float)(100 - 30*sin(i*CV_PI*2/5))));
  cv::transform(vec, vec, cv::Matx23f(0.707, -0.707, 10, 0.707, 0.707, 20));
  ```
  
  
  That is, we form an STL vector containing points, and apply in-place
  affine transformation to the vector using the 2x3 matrix created inline
  as `Matx<float, 2, 3>` instance.
  
  Here is how such a function can be implemented (for simplicity, we
  implement a very specific case of it, according to the assertion
  statement inside) :
  
  ```c++
  void myAffineTransform(InputArray _src, OutputArray _dst, InputArray _m)
  {
      // get Mat headers for input arrays. This is O(1) operation,
      // unless _src and/or _m are matrix expressions.
      Mat src = _src.getMat(), m = _m.getMat();
      CV_Assert( src.type() == CV_32FC2 && m.type() == CV_32F && m.size() == Size(3, 2) );
  
      // [re]create the output array so that it has the proper size and type.
      // In case of Mat it calls Mat::create, in case of STL vector it calls vector::resize.
      _dst.create(src.size(), src.type());
      Mat dst = _dst.getMat();
  
      for( int i = 0; i < src.rows; i++ )
          for( int j = 0; j < src.cols; j++ )
          {
              Point2f pt = src.at<Point2f>(i, j);
              dst.at<Point2f>(i, j) = Point2f(m.at<float>(0, 0)*pt.x +
                                              m.at<float>(0, 1)*pt.y +
                                              m.at<float>(0, 2),
                                              m.at<float>(1, 0)*pt.x +
                                              m.at<float>(1, 1)*pt.y +
                                              m.at<float>(1, 2));
          }
  }
  ```
  
  
  There is another related type, `InputArrayOfArrays`, which is currently
  defined as a synonym for `InputArray`: :
  
  ```c++
  typedef InputArray InputArrayOfArrays;
  ```
  
  
  It denotes function arguments that are either vectors of vectors or
  vectors of matrices. A separate synonym is needed to generate
  Python/Java etc. wrappers properly. At the function implementation level
  their use is similar, but `_InputArray::getMat(idx)` should be used to
  get header for the idx-th component of the outer vector and
  `_InputArray::size().area()` should be used to find the number of
  components (vectors/matrices) of the outer vector.
cv::OutputArray: |
  This type is very similar to `InputArray` except that it is used for
  input/output and output function parameters. Just like with
  `InputArray`, OpenCV users should not care about `OutputArray`, they
  just pass `Mat`, `vector<T>` etc. to the functions. The same limitation
  as for `InputArray`: **Do not explicitly create OutputArray instances**
  applies here too.
  
  If you want to make your function polymorphic (i.e. accept different
  arrays as output parameters), it is also not very difficult. Take the
  sample above as the reference. Note that `_OutputArray::create()` needs
  to be called before `_OutputArray::getMat()`. This way you guarantee
  that the output array is properly allocated.
  
  Optional output parameters. If you do not need certain output array to
  be computed and returned to you, pass `cv::noArray()`, just like you
  would in the case of optional input array. At the implementation level,
  use `_OutputArray::needed()` to check if certain output array needs to
  be computed or not.
  
  There are several synonyms for `OutputArray` that are used to assist
  automatic Python/Java/... wrapper generators: :
  
  ```c++
  typedef OutputArray OutputArrayOfArrays;
  typedef OutputArray InputOutputArray;
  typedef OutputArray InputOutputArrayOfArrays;
  ```

cv::NAryMatIterator: |
  n-ary multi-dimensional array iterator. :
  
  ```c++
  class CV_EXPORTS NAryMatIterator
  {
  public:
      //! the default constructor
      NAryMatIterator();
      //! the full constructor taking arbitrary number of n-dim matrices
      NAryMatIterator(const Mat** arrays, Mat* planes, int narrays=-1);
      //! the separate iterator initialization method
      void init(const Mat** arrays, Mat* planes, int narrays=-1);
  
      //! proceeds to the next plane of every iterated matrix
      NAryMatIterator& operator ++();
      //! proceeds to the next plane of every iterated matrix (postfix increment operator)
      NAryMatIterator operator ++(int);
  
      ...
      int nplanes; // the total number of planes
  };
  ```
  
  
  Use the class to implement unary, binary, and, generally, n-ary
  element-wise operations on multi-dimensional arrays. Some of the
  arguments of an n-ary function may be continuous arrays, some may be
  not. It is possible to use conventional `MatIterator` 's for each array
  but incrementing all of the iterators after each small operations may be
  a big overhead. In this case consider using `NAryMatIterator` to iterate
  through several matrices simultaneously as long as they have the same
  geometry (dimensionality and all the dimension sizes are the same). On
  each iteration `it.planes[0]`, `it.planes[1]` , ... will be the slices
  of the corresponding matrices.
  
  The example below illustrates how you can compute a normalized and
  threshold 3D color histogram: :
  
  ```c++
  void computeNormalizedColorHist(const Mat& image, Mat& hist, int N, double minProb)
  {
      const int histSize[] = {N, N, N};
  
      // make sure that the histogram has a proper size and type
      hist.create(3, histSize, CV_32F);
  
      // and clear it
      hist = Scalar(0);
  
      // the loop below assumes that the image
      // is a 8-bit 3-channel. check it.
      CV_Assert(image.type() == CV_8UC3);
      MatConstIterator_<Vec3b> it = image.begin<Vec3b>(),
                               it_end = image.end<Vec3b>();
      for( ; it != it_end; ++it )
      {
          const Vec3b& pix = *it;
          hist.at<float>(pix[0]*N/256, pix[1]*N/256, pix[2]*N/256) += 1.f;
      }
  
      minProb *= image.rows*image.cols;
      Mat plane;
      NAryMatIterator it(&hist, &plane, 1);
      double s = 0;
      // iterate through the matrix. on each iteration
      // it.planes[*] (of type Mat) will be set to the current plane.
      for(int p = 0; p < it.nplanes; p++, ++it)
      {
          threshold(it.planes[0], it.planes[0], minProb, 0, THRESH_TOZERO);
          s += sum(it.planes[0])[0];
      }
  
      s = 1./s;
      it = NAryMatIterator(&hist, &plane, 1);
      for(int p = 0; p < it.nplanes; p++, ++it)
          it.planes[0] *= s;
  }
  ```

cv::SparseMat: |
  The class `SparseMat` represents multi-dimensional sparse numerical
  arrays. Such a sparse array can store elements of any type that :ocvMat
  can store. *Sparse* means that only non-zero elements are stored
  (though, as a result of operations on a sparse matrix, some of its
  stored elements can actually become 0. It is up to you to detect such
  elements and delete them using `SparseMat::erase` ). The non-zero
  elements are stored in a hash table that grows when it is filled so that
  the search time is O(1) in average (regardless of whether element is
  there or not). Elements can be accessed using the following methods:
  
  *
  :   Query operations (`SparseMat::ptr` and the higher-level
      `SparseMat::ref`, `SparseMat::value` and `SparseMat::find`), for
      example:
  
  ```c++
      const int dims = 5;
      int size[] = {10, 10, 10, 10, 10};
      SparseMat sparse_mat(dims, size, CV_32F);
      for(int i = 0; i < 1000; i++)
      {
          int idx[dims];
          for(int k = 0; k < dims; k++)
              idx[k] = rand()
          sparse_mat.ref<float>(idx) += 1.f;
      }
  ```
  
  
  *
  :   Sparse matrix iterators. They are similar to `MatIterator` but
      different from :ocvNAryMatIterator. That is, the iteration loop is
      familiar to STL users:
  
  ```c++
      // prints elements of a sparse floating-point matrix
      // and the sum of elements.
      SparseMatConstIterator_<float>
          it = sparse_mat.begin<float>(),
          it_end = sparse_mat.end<float>();
      double s = 0;
      int dims = sparse_mat.dims();
      for(; it != it_end; ++it)
      {
          // print element indices and the element value
          const SparseMat::Node* n = it.node();
          printf("(");
          for(int i = 0; i < dims; i++)
              printf("%d%s", n->idx[i], i < dims-1 ? ", " : ")");
          printf(": %g\n", it.value<float>());
          s += *it;
      }
      printf("Element sum is %g\n", s);
  
  If you run this loop, you will notice that elements are not
  enumerated in a logical order (lexicographical, and so on). They
  come in the same order as they are stored in the hash table
  (semi-randomly). You may collect pointers to the nodes and sort them
  to get the proper ordering. Note, however, that pointers to the
  nodes may become invalid when you add more elements to the matrix.
  This may happen due to possible buffer reallocation.
  ```
  
  
  *
  :   Combination of the above 2 methods when you need to process 2 or
      more sparse matrices simultaneously. For example, this is how you
      can compute unnormalized cross-correlation of the 2 floating-point
      sparse matrices:
  
  ```c++
      double cross_corr(const SparseMat& a, const SparseMat& b)
      {
          const SparseMat *_a = &a, *_b = &b;
          // if b contains less elements than a,
          // it is faster to iterate through b
          if(_a->nzcount() > _b->nzcount())
              std::swap(_a, _b);
          SparseMatConstIterator_<float> it = _a->begin<float>(),
                                         it_end = _a->end<float>();
          double ccorr = 0;
          for(; it != it_end; ++it)
          {
              // take the next element from the first matrix
              float avalue = *it;
              const Node* anode = it.node();
              // and try to find an element with the same index in the second matrix.
              // since the hash value depends only on the element index,
              // reuse the hash value stored in the node
              float bvalue = _b->value<float>(anode->idx,&anode->hashval);
              ccorr += avalue*bvalue;
          }
          return ccorr;
      }
  ```

cv::SparseMat::SparseMat: Various SparseMat constructors.
cv::SparseMat::~SparseMat: SparseMat object destructor.
cv::SparseMat::operator=: |-
  Provides sparse matrix assignment operators.
  
  The last variant is equivalent to the corresponding constructor with
  try1d=false.
cv::SparseMat::clone: Creates a full copy of the matrix.
cv::SparseMat::copyTo: |-
  Copy all the data to the destination matrix.The destination will be
  reallocated if needed.
  
  The last variant converts 1D or 2D sparse matrix to dense 2D matrix. If
  the sparse matrix is 1D, the result will be a single-column matrix.
cv::SparceMat::convertTo: |-
  Convert sparse matrix with possible type change and scaling.
  
  The first version converts arbitrary sparse matrix to dense matrix and
  multiplies all the matrix elements by the specified scalar. The second
  versiob converts sparse matrix to dense matrix with optional type
  conversion and scaling. When rtype=-1, the destination element type will
  be the same as the sparse matrix element type. Otherwise, rtype will
  specify the depth and the number of channels will remain the same as in
  the sparse matrix.
cv::SparseMat:create: |-
  Reallocates sparse matrix. If it was already of the proper size and
  type, it is simply cleared with clear(), otherwise, the old matrix is
  released (using release()) and the new one is allocated.
cv::SparseMat::clear: Sets all the matrix elements to 0, which means clearing the hash table.
cv::SparseMat::addref: Manually increases reference counter to the header.
cv::SparseMat::release: |-
  Decreses the header reference counter when it reaches 0. The header and
  all the underlying data are deallocated.
cv::SparseMat::CvSparseMat *: |-
  Converts sparse matrix to the old-style representation. All the elements
  are copied.
cv::SparseMat::elemSize: |-
  Size of each element in bytes (the matrix nodes will be bigger because
  of element indices and other SparseMat::Node elements).
cv::SparseMat::elemSize1: elemSize()/channels().
cv::SparseMat::type: |-
  Returns the type of a matrix element.
  
  The method returns a sparse matrix element type. This is an identifier
  compatible with the `CvMat` type system, like `CV_16SC3` or 16-bit
  signed 3-channel array, and so on.
cv::SparseMat::depth: |
  Returns the depth of a sparse matrix element.
  
  The method returns the identifier of the matrix element depth (the type
  of each individual channel). For example, for a 16-bit signed 3-channel
  array, the method returns `CV_16S`
  
    `CV_8U` - 8-bit unsigned integers ( `0..255` )
  
  
    `CV_8S` - 8-bit signed integers ( `-128..127` )
  
  
    `CV_16U` - 16-bit unsigned integers ( `0..65535` )
  
  
    `CV_16S` - 16-bit signed integers ( `-32768..32767` )
  
  
    `CV_32S` - 32-bit signed integers ( `-2147483648..2147483647` )
  
  
    `CV_32F` - 32-bit floating-point numbers (
  `-FLT_MAX..FLT_MAX, INF, NAN` )
  
  
    `CV_64F` - 64-bit floating-point numbers (
  `-DBL_MAX..DBL_MAX, INF, NAN` )

cv::SparseMat::channels: |-
  Returns the number of matrix channels.
  
  The method returns the number of matrix channels.
cv::SparseMat::size: |-
  Returns the array of sizes or matrix size by i dimension and 0 if the
  matrix is not allocated.
cv::SparseMat::dims: Returns the matrix dimensionality.
cv::SparseMat::nzcount: Returns the number of non-zero elements.
cv::SparseMat::hash: Compute element hash value from the element indices.
cv::SparseMat::ptr: |-
  Low-level element-access functions, special variants for 1D, 2D, 3D
  cases, and the generic one for n-D case.
  
  Return pointer to the matrix element. If the element is there (it is
  non-zero), the pointer to it is returned. If it is not there and
  `createMissing=false`, NULL pointer is returned. If it is not there and
  `createMissing=true`, the new elementis created and initialized with 0.
  Pointer to it is returned. If the optional hashval pointer is not
  `NULL`, the element hash value is not computed but `hashval` is taken
  instead.
cv::SparseMat::erase: |-
  Erase the specified matrix element. When there is no such an element,
  the methods do nothing.
cv::SparseMat_: |
  Template sparse n-dimensional array class derived from :ocvSparseMat :
  
  ```c++
  template<typename _Tp> class SparseMat_ : public SparseMat
  {
  public:
      typedef SparseMatIterator_<_Tp> iterator;
      typedef SparseMatConstIterator_<_Tp> const_iterator;
  
      // constructors;
      // the created matrix will have data type = DataType<_Tp>::type
      SparseMat_();
      SparseMat_(int dims, const int* _sizes);
      SparseMat_(const SparseMat& m);
      SparseMat_(const SparseMat_& m);
      SparseMat_(const Mat& m);
      SparseMat_(const CvSparseMat* m);
      // assignment operators; data type conversion is done when necessary
      SparseMat_& operator = (const SparseMat& m);
      SparseMat_& operator = (const SparseMat_& m);
      SparseMat_& operator = (const Mat& m);
  
      // equivalent to the correspoding parent class methods
      SparseMat_ clone() const;
      void create(int dims, const int* _sizes);
      operator CvSparseMat*() const;
  
      // overriden methods that do extra checks for the data type
      int type() const;
      int depth() const;
      int channels() const;
  
      // more convenient element access operations.
      // ref() is retained (but <_Tp> specification is not needed anymore);
      // operator () is equivalent to SparseMat::value<_Tp>
      _Tp& ref(int i0, size_t* hashval=0);
      _Tp operator()(int i0, size_t* hashval=0) const;
      _Tp& ref(int i0, int i1, size_t* hashval=0);
      _Tp operator()(int i0, int i1, size_t* hashval=0) const;
      _Tp& ref(int i0, int i1, int i2, size_t* hashval=0);
      _Tp operator()(int i0, int i1, int i2, size_t* hashval=0) const;
      _Tp& ref(const int* idx, size_t* hashval=0);
      _Tp operator()(const int* idx, size_t* hashval=0) const;
  
      // iterators
      SparseMatIterator_<_Tp> begin();
      SparseMatConstIterator_<_Tp> begin() const;
      SparseMatIterator_<_Tp> end();
      SparseMatConstIterator_<_Tp> end() const;
  };
  ```
  
  
  `SparseMat_` is a thin wrapper on top of :ocvSparseMat created in the
  same way as `Mat_` . It simplifies notation of some operations. :
  
  ```c++
  int sz[] = {10, 20, 30};
  SparseMat_<double> M(3, sz);
  ...
  M.ref(1, 2, 3) = M(4, 5, 6) + M(7, 8, 9);
  ```

cv::Algorithm: |
  This is a base class for all more or less complex algorithms in OpenCV,
  especially for classes of algorithms, for which there can be multiple
  implementations. The examples are stereo correspondence (for which there
  are algorithms like block matching, semi-global block matching,
  graph-cut etc.), background subtraction (which can be done using
  mixture-of-gaussians models, codebook-based algorithm etc.), optical
  flow (block matching, Lucas-Kanade, Horn-Schunck etc.).
  
  The class provides the following features for all derived classes:
  
    so called "virtual constructor". That is, each Algorithm
  derivative is registered at program start and you can get the list
  of registered algorithms and create instance of a particular
  algorithm by its name (see `Algorithm::create`). If you plan to
  add your own algorithms, it is good practice to add a unique
  prefix to your algorithms to distinguish them from other
  algorithms.
  
  
    setting/retrieving algorithm parameters by name. If you used video
  capturing functionality from OpenCV highgui module, you are
  probably familar with `cvSetCaptureProperty()`,
  `cvGetCaptureProperty()`, `VideoCapture::set()` and
  `VideoCapture::get()`. `Algorithm` provides similar method where
  instead of integer id's you specify the parameter names as text
  strings. See `Algorithm::set` and `Algorithm::get` for details.
  
  
    reading and writing parameters from/to XML or YAML files. Every
  Algorithm derivative can store all its parameters and then read
  them back. There is no need to re-implement it each time.
  
  
  Here is example of SIFT use in your application via Algorithm interface:
  :
  
  
  ```c++
  #include "opencv2/opencv.hpp"
  #include "opencv2/nonfree.hpp"
  
  ...
  
  initModule_nonfree(); // to load SURF/SIFT etc.
  
  Ptr<Feature2D> sift = Algorithm::create<Feature2D>("Feature2D.SIFT");
  
  FileStorage fs("sift_params.xml", FileStorage::READ);
  if( fs.isOpened() ) // if we have file with parameters, read them
  {
      sift->read(fs["sift_params"]);
      fs.release();
  }
  else // else modify the parameters and store them; user can later edit the file to use different parameters
  {
      sift->set("contrastThreshold", 0.01f); // lower the contrast threshold, compared to the default value
  
      {
      WriteStructContext ws(fs, "sift_params", CV_NODE_MAP);
      sift->write(fs);
      }
  }
  
  Mat image = imread("myimage.png", 0), descriptors;
  vector<KeyPoint> keypoints;
  (*sift)(image, noArray(), keypoints, descriptors);
  ```

cv::Algorithm::name: Returns the algorithm name
cv::Algorithm::get: |-
  Returns the algorithm parameter
  
  The method returns value of the particular parameter. Since the compiler
  can not deduce the type of the returned parameter, you should specify it
  explicitly in angle brackets. Here are the allowed forms of get:
    myalgo.get<int>("param_name")
  
    myalgo.get<double>("param_name")
  
    myalgo.get<bool>("param_name")
  
    myalgo.get<String>("param_name")
  
    myalgo.get<Mat>("param_name")
  
    myalgo.get<vector<Mat> >("param_name")
  
    myalgo.get<Algorithm>("param_name") (it returns
  Ptr<Algorithm>).
  
  
  
  In some cases the actual type of the parameter can be cast to the
  specified type, e.g. integer parameter can be cast to double, `bool` can
  be cast to `int`. But "dangerous" transformations (string<->number,
  double->int, 1x1 Mat<->number, ...) are not performed and the method
  will throw an exception. In the case of `Mat` or `vector<Mat>`
  parameters the method does not clone the matrix data, so do not modify
  the matrices. Use `Algorithm::set` instead - slower, but more safe.
cv::Algorithm::set: |-
  Sets the algorithm parameter
  
  The method sets value of the particular parameter. Some of the algorithm
  parameters may be declared as read-only. If you try to set such a
  parameter, you will get exception with the corresponding error message.
cv::Algorithm::write: |
  Stores algorithm parameters in a file storage
  
  The method stores all the algorithm parameters (in alphabetic order) to
  the file storage. The method is virtual. If you define your own
  Algorithm derivative, your can override the method and store some extra
  information. However, it's rarely needed. Here are some examples:
  
    SIFT feature detector (from nonfree module). The class only stores
  algorithm parameters and no keypoints or their descriptors.
  Therefore, it's enough to store the algorithm parameters, which is
  what `Algorithm::write()` does. Therefore, there is no dedicated
  `SIFT::write()`.
  
  
    Background subtractor (from video module). It has the algorithm
  parameters and also it has the current background model. However,
  the background model is not stored. First, it's rather big. Then,
  if you have stored the background model, it would likely become
  irrelevant on the next run (because of shifted camera, changed
  background, different lighting etc.). Therefore,
  `BackgroundSubtractorMOG` and `BackgroundSubtractorMOG2` also rely
  on the standard `Algorithm::write()` to store just the algorithm
  parameters.
  
  
    Expectation Maximization (from ml module). The algorithm finds
  mixture of gaussians that approximates user data best of all. In
  this case the model may be re-used on the next run to test new
  data against the trained statistical model. So EM needs to store
  the model. However, since the model is described by a few
  parameters that are available as read-only algorithm parameters
  (i.e. they are available via `EM::get()`), EM also relies on
  `Algorithm::write()` to store both EM parameters and the model
  (represented by read-only algorithm parameters).

cv::Algorithm::read: "\n\n\
  Reads algorithm parameters from a file storage\n\n\
  The method reads all the algorithm parameters from the specified node of\n\
  a file storage. Similarly to `Algorithm::write()`, if you implement an\n\
  algorithm that needs to read some extra data and/or re-compute some\n\
  internal data, you may override the method."
cv::Algorithm::getList: |
  Returns the list of registered algorithms
  
  This static method returns the list of registered algorithms in
  alphabetical order. Here is how to use it :
  
  ```c++
  vector<String> algorithms;
  Algorithm::getList(algorithms);
  cout << "Algorithms: " << algorithms.size() << endl;
  for (size_t i=0; i < algorithms.size(); i++)
      cout << algorithms[i] << endl;
  ```

cv::Algorithm::create: |
  Creates algorithm instance by name
  
  This static method creates a new instance of the specified algorithm. If
  there is no such algorithm, the method will silently return null pointer
  (that can be checked by `Ptr::empty()` method). Also, you should specify
  the particular `Algorithm` subclass as `_Tp` (or simply `Algorithm` if
  you do not know it at that point). :
  
  ```c++
  Ptr<BackgroundSubtractor> bgfg = Algorithm::create<BackgroundSubtractor>("BackgroundSubtractor.MOG2");
  ```
  
  
  **note**
  
  This is important note about seemingly mysterious behavior of
  `Algorithm::create()` when it returns NULL while it should not. The
  reason is simple - `Algorithm::create()` resides in OpenCV`s core
  module and the algorithms are implemented in other modules. If you
  create algorithms dynamically, C++ linker may decide to throw away the
  modules where the actual algorithms are implemented, since you do not
  call any functions from the modules. To avoid this problem, you need
  to call `initModule_<modulename>();` somewhere in the beginning of the
  program before `Algorithm::create()`. For example, call
  `initModule_nonfree()` in order to use SURF/SIFT, call
  `initModule_ml()` to use expectation maximization etc.

cv::Creating Own Algorithms: |+
  The above methods are usually enough for users. If you want to make your
  own algorithm, derived from `Algorithm`, you should basically follow a
  few conventions and add a little semi-standard piece of code to your
  class:
    Make a class and specify `Algorithm` as its base class.
  
    The algorithm parameters should be the class members. See
  `Algorithm::get()` for the list of possible types of the
  parameters.
  
    Add public virtual method `AlgorithmInfo* info() const;` to your
  class.
  
    Add constructor function, `AlgorithmInfo` instance and implement
  the `info()` method. The simplest way is to take
  <http://code.opencv.org/projects/opencv/repository/revisions/master/entry/modules/ml/src/ml_init.cpp>
  as the reference and modify it according to the list of your
  parameters.
  
    Add some public function (e.g. `initModule_<mymodule>()`) that
  calls info() of your algorithm and put it into the same source
  file as `info()` implementation. This is to force C++ linker to
  include this object file into the target application. See
  `Algorithm::create()` for details.
  
cv::detail::Blender: |
  Base class for all blenders. :
  
  ```c++
  class CV_EXPORTS Blender
  {
  public:
      virtual ~Blender() {}
  
      enum { NO, FEATHER, MULTI_BAND };
      static Ptr<Blender> createDefault(int type, bool try_gpu = false);
  
      void prepare(const std::vector<Point> &corners, const std::vector<Size> &sizes);
      virtual void prepare(Rect dst_roi);
      virtual void feed(const Mat &img, const Mat &mask, Point tl);
      virtual void blend(Mat &dst, Mat &dst_mask);
  
  protected:
      Mat dst_, dst_mask_;
      Rect dst_roi_;
  };
  ```

cv::detail::Blender::prepare: Prepares the blender for blending.
cv::detail::Blender::feed: Processes the image.
cv::detail::Blender::blend: Blends and returns the final pano.
cv::detail::FeatherBlender: |
  Simple blender which mixes images at its borders. :
  
  ```c++
  class CV_EXPORTS FeatherBlender : public Blender
  {
  public:
      FeatherBlender(float sharpness = 0.02f) { setSharpness(sharpness); }
  
      float sharpness() const { return sharpness_; }
      void setSharpness(float val) { sharpness_ = val; }
  
      void prepare(Rect dst_roi);
      void feed(const Mat &img, const Mat &mask, Point tl);
      void blend(Mat &dst, Mat &dst_mask);
  
      // Creates weight maps for fixed set of source images by their masks and top-left corners.
      // Final image can be obtained by simple weighting of the source images.
      Rect createWeightMaps(const std::vector<Mat> &masks, const std::vector<Point> &corners,
                            std::vector<Mat> &weight_maps);
  
  private:
      /* hidden */
  };
  ```

cv::detail::MultiBandBlender: |
  Blender which uses multi-band blending algorithm (see [BA83]_). :
  
  ```c++
  class CV_EXPORTS MultiBandBlender : public Blender
  {
  public:
      MultiBandBlender(int try_gpu = false, int num_bands = 5);
      int numBands() const { return actual_num_bands_; }
      void setNumBands(int val) { actual_num_bands_ = val; }
  
      void prepare(Rect dst_roi);
      void feed(const Mat &img, const Mat &mask, Point tl);
      void blend(Mat &dst, Mat &dst_mask);
  
  private:
      /* hidden */
  };
  ```

cv::Boosting: |-
  A common machine learning task is supervised learning. In supervised
  learning, the goal is to learn the functional relationship $F: y = F(x)$
  between the input $x$ and the output $y$ . Predicting the qualitative
  output is called *classification*, while predicting the quantitative
  output is called *regression*.
  
  Boosting is a powerful learning concept that provides a solution to the
  supervised classification learning task. It combines the performance of
  many "weak" classifiers to produce a powerful committee [HTF01]_. A
  weak classifier is only required to be better than chance, and thus can
  be very simple and computationally inexpensive. However, many of them
  smartly combine results to a strong classifier that often outperforms
  most "monolithic" strong classifiers such as SVMs and Neural Networks.
  
  Decision trees are the most popular weak classifiers used in boosting
  schemes. Often the simplest decision trees with only a single split node
  per tree (called `stumps` ) are sufficient.
  
  The boosted model is based on $N$ training examples ${(x_i,y_i)}1N$ with
  $x_i \in{R^K}$ and $y_i \in{-1, +1}$ . $x_i$ is a $K$ -component vector.
  Each component encodes a feature relevant to the learning task at hand.
  The desired two-class output is encoded as -1 and +1.
  
  Different variants of boosting are known as Discrete Adaboost, Real
  AdaBoost, LogitBoost, and Gentle AdaBoost [FHT98]_. All of them are
  very similar in their overall structure. Therefore, this chapter focuses
  only on the standard two-class Discrete AdaBoost algorithm, outlined
  below. Initially the same weight is assigned to each sample (step 2).
  Then, a weak classifier $f_{m(x)}$ is trained on the weighted training
  data (step 3a). Its weighted training error and scaling factor $c_m$ is
  computed (step 3b). The weights are increased for training samples that
  have been misclassified (step 3c). All weights are then normalized, and
  the process of finding the next weak classifier continues for another
  $M$ -1 times. The final classifier $F(x)$ is the sign of the weighted
  sum over the individual weak classifiers (step 4).
  
  **Two-class Discrete AdaBoost Algorithm**
  
  #.
  :   Set $N$ examples ${(x_i,y_i)}1N$ with
      $x_i \in{R^K}, y_i \in{-1, +1}$ .
  
  #.
  :   Assign weights as $w_i = 1/N, i = 1,...,N$ .
  
  #.
  :   Repeat for $m = 1,2,...,M$ :
  
  ```c++
  3.1. Fit the classifier $f_m(x) \in{-1,1}$, using weights $w_i$ on
  the training data.
  
  3.2. Compute
  $err_m = E_w [1_{(y \neq f_m(x))}], c_m = log((1 - err_m)/err_m)$ .
  
  3.3. Set
  $w_i \Leftarrow w_i exp[c_m 1_{(y_i \neq f_m(x_i))}], i = 1,2,...,N,$
  and renormalize so that $\Sigma i w_i = 1$ .
  ```
  
   Classify new samples *x* using the formula:
  $\textrm{sign} (\Sigma m = 1M c_m f_m(x))$ .
  
  
  **note**
  
  Similar to the classical boosting methods, the current implementation
  supports two-class classifiers only. For `M > 2` classes, there is the
  **AdaBoost.MH** algorithm (described in [FHT98]_) that reduces the
  problem to the two-class problem, yet with a much larger training set.
  
  
  To reduce computation time for boosted models without substantially
  losing accuracy, the influence trimming technique can be employed. As
  the training algorithm proceeds and the number of trees in the ensemble
  is increased, a larger number of the training samples are classified
  correctly and with increasing confidence, thereby those samples receive
  smaller weights on the subsequent iterations. Examples with a very low
  relative weight have a small impact on the weak classifier training.
  Thus, such examples may be excluded during the weak classifier training
  without having much effect on the induced classifier. This process is
  controlled with the `weight_trim_rate` parameter. Only examples with the
  summary fraction `weight_trim_rate` of the total weight mass are used in
  the weak classifier training. Note that the weights for **all** training
  examples are recomputed at each training iteration. Examples deleted at
  a particular iteration may be used again for learning some of the weak
  classifiers further [FHT98]_.
cv::CvBoostParams: |-
  The structure is derived from :ocvCvDTreeParams but not all of the
  decision tree parameters are supported. In particular, cross-validation
  is not supported.
  
  All parameters are public. You can initialize them by a constructor and
  then override some of them directly if you want.
cv::CvBoostParams::CvBoostParams: |
  The constructors.
  
  See :ocvCvDTreeParams::CvDTreeParams for description of other
  parameters.
  
  Default parameters are:
  
  ```c++
  CvBoostParams::CvBoostParams()
  {
      boost_type = CvBoost::REAL;
      weak_count = 100;
      weight_trim_rate = 0.95;
      cv_folds = 0;
      max_depth = 1;
  }
  ```

cv::CvBoostTree: "The weak tree classifier, a component of the boosted tree classifier\n\
  :ocvCvBoost, is a derivative of :ocvCvDTree. Normally, there is no need\n\
  to use the weak classifiers directly. However, they can be accessed as\n\
  elements of the sequence `CvBoost::weak`, retrieved by\n\
  :ocvCvBoost::get_weak_predictors.\n\n\
  **note**\n\n\
  In case of LogitBoost and Gentle AdaBoost, each weak predictor is a\n\
  regression tree, rather than a classification tree. Even in case of\n\
  Discrete AdaBoost and Real AdaBoost, the `CvBoostTree::predict` return\n\
  value (:ocvCvDTreeNode::value) is not an output class label. A\n\
  negative value \"votes\" for class #0, a positive value - for class\n\
  #1. The votes are weighted. The weight of each individual tree may be\n\
  increased or decreased using the method `CvBoostTree::scale`.\n"
cv::CvBoost: Boosted tree classifier derived from :ocvCvStatModel.
cv::CvBoost::CvBoost: "Default and training constructors.\n\n\
  The constructors follow conventions of :ocvCvStatModel::CvStatModel. See\n\
  :ocvCvStatModel::train for parameters descriptions."
cv::CvBoost::train: |-
  Trains a boosted tree classifier.
  
  The train method follows the common template of :ocvCvStatModel::train.
  The responses must be categorical, which means that boosted trees cannot
  be built for regression, and there should be two classes.
cv::CvBoost::predict: |-
  Predicts a response for an input sample.
  
  The method runs the sample through the trees in the ensemble and returns
  the output class label based on the weighted voting.
cv::CvBoost::prune: |
  Removes the specified weak classifiers.
  
  The method removes the specified weak classifiers from the sequence.
  
  **note**
  
  Do not confuse this method with the pruning of individual decision
  trees, which is currently not supported.

cv::CvBoost::calc_error: |-
  Returns error of the boosted tree classifier.
  
  The method is identical to :ocvCvDTree::calc_error but uses the boosted
  tree classifier as predictor.
cv::CvBoost::get_weak_predictors: |-
  Returns the sequence of weak tree classifiers.
  
  The method returns the sequence of weak classifiers. Each element of the
  sequence is a pointer to the :ocvCvBoostTree class or to some of its
  derivatives.
cv::CvBoost::get_params: Returns current parameters of the boosted tree classifier.
cv::CvBoost::get_data: Returns used train data of the boosted tree classifier.
cv::gpu::solvePnPRansac: Finds the object pose from 3D-2D point correspondences.
cv::detail::CameraParams: |
  Describes camera parameters.
  
  **note**
  
  Translation is assumed to be zero during the whole stitching pipeline.
  
  
  ```c++
  struct CV_EXPORTS CameraParams
  {
      CameraParams();
      CameraParams(const CameraParams& other);
      const CameraParams& operator =(const CameraParams& other);
      Mat K() const;
  
      double focal; // Focal length
      double aspect; // Aspect ratio
      double ppx; // Principal point X
      double ppy; // Principal point Y
      Mat R; // Rotation
      Mat t; // Translation
  };
  ```

cv::Camera Calibration and 3D Reconstruction: |
  The functions in this section use a so-called pinhole camera model. In
  this model, a scene view is formed by projecting 3D points into the
  image plane using a perspective transformation.
  
  $$s  \; m' = A [R|t] M'$$
  
  or
  
  $$s  \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}
  \begin{bmatrix}
  r_{11} & r_{12} & r_{13} & t_1  \
  r_{21} & r_{22} & r_{23} & t_2  \
  r_{31} & r_{32} & r_{33} & t_3
  \end{bmatrix}
  \begin{bmatrix}
  X \
  Y \
  Z \
  1
  \end{bmatrix}$$
  
  where:
    $(X, Y, Z)$ are the coordinates of a 3D point in the world
  coordinate space
  
    $(u, v)$ are the coordinates of the projection point in pixels
  
    $A$ is a camera matrix, or a matrix of intrinsic parameters
  
    $(cx, cy)$ is a principal point that is usually at the image
  center
  
    $fx, fy$ are the focal lengths expressed in pixel units.
  
  
  
  Thus, if an image from the camera is scaled by a factor, all of these
  parameters should be scaled (multiplied/divided, respectively) by the
  same factor. The matrix of intrinsic parameters does not depend on the
  scene viewed. So, once estimated, it can be re-used as long as the focal
  length is fixed (in case of zoom lens). The joint rotation-translation
  matrix $[R|t]$ is called a matrix of extrinsic parameters. It is used to
  describe the camera motion around a static scene, or vice versa, rigid
  motion of an object in front of a still camera. That is, $[R|t]$
  translates coordinates of a point $(X, Y, Z)$ to a coordinate system,
  fixed with respect to the camera. The transformation above is equivalent
  to the following (when $z \ne 0$ ):
  
  $$\begin{array}{l}
  \vecthree{x}{y}{z} = R  \vecthree{X}{Y}{Z} + t \
  x' = x/z \
  y' = y/z \
  u = f_x*x' + c_x \
  v = f_y*y' + c_y
  \end{array}$$
  
  Real lenses usually have some distortion, mostly radial distortion and
  slight tangential distortion. So, the above model is extended as:
  
  $$\begin{array}{l} \vecthree{x}{y}{z} = R  \vecthree{X}{Y}{Z} + t \ x' = x/z \ y' = y/z \ x'' = x'  \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \ y'' = y'  \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_1 r^2 + s_2 r^4 \ \text{where} \quad r^2 = x'^2 + y'^2  \ u = f_x*x'' + c_x \ v = f_y*y'' + c_y \end{array}$$
  
  $k_1$, $k_2$, $k_3$, $k_4$, $k_5$, and $k_6$ are radial distortion
  coefficients. $p_1$ and $p_2$ are tangential distortion coefficients.
  $s_1$, $s_2$, $s_3$, and $s_4$, are the thin prism distortion
  coefficients. Higher-order coefficients are not considered in OpenCV. In
  the functions below the coefficients are passed or returned as
  
  $$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4]])$$
  
  vector. That is, if the vector contains four elements, it means that
  $k_3=0$ . The distortion coefficients do not depend on the scene viewed.
  Thus, they also belong to the intrinsic camera parameters. And they
  remain the same regardless of the captured image resolution. If, for
  example, a camera has been calibrated on images of `320 x 240`
  resolution, absolutely the same distortion coefficients can be used for
  `640 x 480` images from the same camera while $f_x$, $f_y$, $c_x$, and
  $c_y$ need to be scaled appropriately.
  
  The functions below use the above model to do the following:
  
    Project 3D points to the image plane given intrinsic and extrinsic
  parameters.
  
  
    Compute extrinsic parameters given intrinsic parameters, a few 3D
  points, and their projections.
  
  
    Estimate intrinsic and extrinsic camera parameters from several
  views of a known calibration pattern (every view is described by
  several 3D-2D point correspondences).
  
  
    Estimate the relative position and orientation of the stereo
  camera "heads" and compute the *rectification* transformation that
  makes the camera optical axes parallel.

cv::calibrateCamera: "\n\n\
  Finds the camera intrinsic and extrinsic parameters from several views\n\
  of a calibration pattern.\n\n\
  The function estimates the intrinsic camera parameters and extrinsic\n\
  parameters for each of the views. The algorithm is based on\n\
  [Zhang2000]_ and [BouguetMCT]_. The coordinates of 3D object points\n\
  and their corresponding 2D projections in each view must be specified.\n\
  That may be achieved by using an object with a known geometry and easily\n\
  detectable feature points. Such an object is called a calibration rig or\n\
  calibration pattern, and OpenCV has built-in support for a chessboard as\n\
  a calibration rig (see :ocvfindChessboardCorners ). Currently,\n\
  initialization of intrinsic parameters (when\n\
  `CV_CALIB_USE_INTRINSIC_GUESS` is not set) is only implemented for\n\
  planar calibration patterns (where Z-coordinates of the object points\n\
  must be all zeros). 3D calibration rigs can also be used as long as\n\
  initial `cameraMatrix` is provided.\n\n\
  The algorithm performs the following steps:\n\n\
  #.\n\
  :   Compute the initial intrinsic parameters (the option only available\n    for planar calibration patterns) or read them from the input\n    parameters. The distortion coefficients are all set to zeros\n    initially unless some of `CV_CALIB_FIX_K?` are specified.\n\n\
  #.\n\
  :   Estimate the initial camera pose as if the intrinsic parameters have\n    been already known. This is done using :ocvsolvePnP .\n\n\
  #.\n\
  :   Run the global Levenberg-Marquardt optimization algorithm to\n    minimize the reprojection error, that is, the total sum of squared\n    distances between the observed feature points `imagePoints` and the\n    projected (using the current estimates for camera parameters and the\n    poses) object points `objectPoints`. See :ocvprojectPoints for\n    details.\n\n\
  The function returns the final re-projection error.\n\n\
  **note**\n\n\
  If you use a non-square (=non-NxN) grid and :ocvfindChessboardCorners\n\
  for calibration, and `calibrateCamera` returns bad values (zero\n\
  distortion coefficients, an image center very far from\n\
  `(w/2-0.5,h/2-0.5)`, and/or large differences between $f_x$ and $f_y$\n\
  (ratios of 10:1 or more)), then you have probably used\n\
  `patternSize=cvSize(rows,cols)` instead of using\n\
  `patternSize=cvSize(cols,rows)` in :ocvfindChessboardCorners .\n"
cv::calibrationMatrixValues: |-
  Computes useful camera characteristics from the camera matrix.
  
  The function computes various useful camera characteristics from the
  previously estimated camera matrix.
cv::composeRT: |-
  Combines two rotation-and-shift transformations.
  
  The functions compute:
  
  $$\begin{array}{l} \texttt{rvec3} =  \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right )  \ \texttt{tvec3} =  \mathrm{rodrigues} ( \texttt{rvec2} )  \cdot \texttt{tvec1} +  \texttt{tvec2} \end{array} ,$$
  
  where $\mathrm{rodrigues}$ denotes a rotation vector to a rotation
  matrix transformation, and $\mathrm{rodrigues}^{-1}$ denotes the inverse
  transformation. See :ocvRodrigues for details.
  
  Also, the functions can compute the derivatives of the output vectors
  with regards to the input vectors (see :ocvmatMulDeriv ). The functions
  are used inside :ocvstereoCalibrate but can also be used in your own
  code where Levenberg-Marquardt or another gradient-based solver is used
  to optimize a function that contains a matrix multiplication.
cv::computeCorrespondEpilines: |-
  For points in an image of a stereo pair, computes the corresponding
  epilines in the other image.
  
  For every point in one of the two images of a stereo pair, the function
  finds the equation of the corresponding epipolar line in the other
  image.
  
  From the fundamental matrix definition (see :ocvfindFundamentalMat ),
  line $l^{(2)}_i$ in the second image for the point $p^{(1)}_i$ in the
  first image (when `whichImage=1` ) is computed as:
  
  $$l^{(2)}_i = F p^{(1)}_i$$
  
  And vice versa, when `whichImage=2`, $l^{(1)}_i$ is computed from
  $p^{(2)}_i$ as:
  
  $$l^{(1)}_i = F^T p^{(2)}_i$$
  
  Line coefficients are defined up to a scale. They are normalized so that
  $a_i^2+b_i^2=1$ .
cv::convertPointsToHomogeneous: |-
  Converts points from Euclidean to homogeneous space.
  
  The function converts points from Euclidean to homogeneous space by
  appending 1's to the tuple of point coordinates. That is, each point
  `(x1, x2, ..., xn)` is converted to `(x1, x2, ..., xn, 1)`.
cv::convertPointsFromHomogeneous: |-
  Converts points from homogeneous to Euclidean space.
  
  The function converts points homogeneous to Euclidean space using
  perspective projection. That is, each point `(x1, x2, ... x(n-1), xn)`
  is converted to `(x1/xn, x2/xn, ..., x(n-1)/xn)`. When `xn=0`, the
  output point coordinates will be `(0,0,0,...)`.
cv::convertPointsHomogeneous: "Converts points to/from homogeneous coordinates.\n\n\
  The function converts 2D or 3D points from/to homogeneous coordinates by\n\
  calling either :ocvconvertPointsToHomogeneous or\n\
  :ocvconvertPointsFromHomogeneous.\n\n\
  **note**\n\n\
  The function is obsolete. Use one of the previous two functions\n\
  instead.\n"
cv::correctMatches: |-
  Refines coordinates of corresponding points.
  
  The function implements the Optimal Triangulation Method (see Multiple
  View Geometry for details). For each given point correspondence
  points1[i] <-> points2[i], and a fundamental matrix F, it computes the
  corrected correspondences newPoints1[i] <-> newPoints2[i] that
  minimize the geometric error
  $d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2$ (where
  $d(a,b)$ is the geometric distance between points $a$ and $b$ ) subject
  to the epipolar constraint $newPoints2^T * F * newPoints1 = 0$ .
cv::decomposeProjectionMatrix: |-
  Decomposes a projection matrix into a rotation matrix and a camera
  matrix.
  
  The function computes a decomposition of a projection matrix into a
  calibration and a rotation matrix and the position of a camera.
  
  It optionally returns three rotation matrices, one for each axis, and
  three Euler angles that could be used in OpenGL. Note, there is always
  more than one sequence of rotations about the three principle axes that
  results in the same orientation of an object, eg. see [Slabaugh]_.
  Returned tree rotation matrices and corresponding three Euler angules
  are only one of the possible solutions.
  
  The function is based on :ocvRQDecomp3x3 .
cv::drawChessboardCorners: |-
  Renders the detected chessboard corners.
  
  The function draws individual chessboard corners detected either as red
  circles if the board was not found, or as colored corners connected with
  lines if the board was found.
cv::findChessboardCorners: |
  Finds the positions of internal corners of the chessboard.
  
  The function attempts to determine whether the input image is a view of
  the chessboard pattern and locate the internal chessboard corners. The
  function returns a non-zero value if all of the corners are found and
  they are placed in a certain order (row by row, left to right in every
  row). Otherwise, if the function fails to find all the corners or
  reorder them, it returns 0. For example, a regular chessboard has 8 x 8
  squares and 7 x 7 internal corners, that is, points where the black
  squares touch each other. The detected coordinates are approximate, and
  to determine their positions more accurately, the function calls
  :ocvcornerSubPix. You also may use the function :ocvcornerSubPix with
  different parameters if returned coordinates are not accurate enough.
  
  Sample usage of detecting and drawing chessboard corners: :
  
  ```c++
  Size patternsize(8,6); //interior number of corners
  Mat gray = ....; //source image
  vector<Point2f> corners; //this will be filled by the detected corners
  
  //CALIB_CB_FAST_CHECK saves a lot of time on images
  //that do not contain any chessboard corners
  bool patternfound = findChessboardCorners(gray, patternsize, corners,
          CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE
          + CALIB_CB_FAST_CHECK);
  
  if(patternfound)
    cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),
      TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
  
  drawChessboardCorners(img, patternsize, Mat(corners), patternfound);
  ```
  
  
  **note**
  
  The function requires white space (like a square-thick border, the
  wider the better) around the board to make the detection more robust
  in various environments. Otherwise, if there is no border and the
  background is dark, the outer black squares cannot be segmented
  properly and so the square grouping and ordering algorithm fails.

cv::findCirclesGrid: |
  Finds centers in the grid of circles.
  
  The function attempts to determine whether the input image contains a
  grid of circles. If it is, the function locates centers of the circles.
  The function returns a non-zero value if all of the centers have been
  found and they have been placed in a certain order (row by row, left to
  right in every row). Otherwise, if the function fails to find all the
  corners or reorder them, it returns 0.
  
  Sample usage of detecting and drawing the centers of circles: :
  
  ```c++
  Size patternsize(7,7); //number of centers
  Mat gray = ....; //source image
  vector<Point2f> centers; //this will be filled by the detected centers
  
  bool patternfound = findCirclesGrid(gray, patternsize, centers);
  
  drawChessboardCorners(img, patternsize, Mat(centers), patternfound);
  ```
  
  
  **note**
  
  The function requires white space (like a square-thick border, the
  wider the better) around the board to make the detection more robust
  in various environments.

cv::solvePnP: |-
  Finds an object pose from 3D-2D point correspondences.
  
  The function estimates the object pose given a set of object points,
  their corresponding image projections, as well as the camera matrix and
  the distortion coefficients.
cv::solvePnPRansac: "Finds an object pose from 3D-2D point correspondences using the RANSAC\n\
  scheme.\n\n\
  The function estimates an object pose given a set of object points,\n\
  their corresponding image projections, as well as the camera matrix and\n\
  the distortion coefficients. This function finds such a pose that\n\
  minimizes reprojection error, that is, the sum of squared distances\n\
  between the observed projections `imagePoints` and the projected (using\n\
  :ocvprojectPoints ) `objectPoints`. The use of RANSAC makes the function\n\
  resistant to outliers. The function is parallelized with the TBB\n\
  library."
cv::findFundamentalMat: |
  Calculates a fundamental matrix from the corresponding points in two
  images.
  
  The epipolar geometry is described by the following equation:
  
  $$[p_2; 1]^T F [p_1; 1] = 0$$
  
  where $F$ is a fundamental matrix, $p_1$ and $p_2$ are corresponding
  points in the first and the second images, respectively.
  
  The function calculates the fundamental matrix using one of four methods
  listed above and returns the found fundamental matrix. Normally just one
  matrix is found. But in case of the 7-point algorithm, the function may
  return up to 3 solutions ( $9 \times 3$ matrix that stores all 3
  matrices sequentially).
  
  The calculated fundamental matrix may be passed further to
  :ocvcomputeCorrespondEpilines that finds the epipolar lines
  corresponding to the specified points. It can also be passed to
  :ocvstereoRectifyUncalibrated to compute the rectification
  transformation. :
  
  ```c++
  // Example. Estimation of fundamental matrix using the RANSAC algorithm
  int point_count = 100;
  vector<Point2f> points1(point_count);
  vector<Point2f> points2(point_count);
  
  // initialize the points here ... */
  for( int i = 0; i < point_count; i++ )
  {
      points1[i] = ...;
      points2[i] = ...;
  }
  
  Mat fundamental_matrix =
   findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);
  ```

cv::findEssentialMat: |-
  Calculates an essential matrix from the corresponding points in two
  images.
  
  This function estimates essential matrix based on the five-point
  algorithm solver in [Nister03]_. [SteweniusCFS]_ is also a related.
  The epipolar geometry is described by the following equation:
  
  $$[p_2; 1]^T K^T E K [p_1; 1] = 0 \$$$$K =
  \begin{bmatrix}
  f & 0 & x_{pp}  \
  0 & f & y_{pp}  \
  0 & 0 & 1
  \end{bmatrix}$$
  
  where $E$ is an essential matrix, $p_1$ and $p_2$ are corresponding
  points in the first and the second images, respectively. The result of
  this function may be passed further to `decomposeEssentialMat()` or
  `recoverPose()` to recover the relative pose between cameras.
cv::decomposeEssentialMat: |-
  Decompose an essential matrix to possible rotations and translation.
  
  This function decompose an essential matrix `E` using svd decomposition
  [HartleyZ00]_. Generally 4 possible poses exists for a given `E`. They
  are $[R_1, t]$, $[R_1, -t]$, $[R_2, t]$, $[R_2, -t]$.
cv::recoverPose: |
  Recover relative camera rotation and translation from an estimated
  essential matrix and the corresponding points in two images, using
  cheirality check. Returns the number of inliers which pass the check.
  
  This function decomposes an essential matrix using
  `decomposeEssentialMat()` and then verifies possible pose hypotheses by
  doing cheirality check. The cheirality check basically means that the
  triangulated 3D points should have positive depth. Some details can be
  found from [Nister03]_.
  
  This function can be used to process output `E` and `mask` from
  `findEssentialMat()`. In this scenario, `points1` and `points2` are the
  same input for `findEssentialMat()`. :
  
  ```c++
  // Example. Estimation of fundamental matrix using the RANSAC algorithm
  int point_count = 100;
  vector<Point2f> points1(point_count);
  vector<Point2f> points2(point_count);
  
  // initialize the points here ... */
  for( int i = 0; i < point_count; i++ )
  {
      points1[i] = ...;
      points2[i] = ...;
  }
  
  double focal = 1.0;
  cv::Point2d pp(0.0, 0.0);
  Mat E, R, t, mask;
  
  E = findEssentialMat(points1, points2, focal, pp, CV_RANSAC, 0.999, 1.0, mask);
  recoverPose(E, points1, points2, R, t, focal, pp, mask);
  ```

cv::findHomography: |-
  Finds a perspective transformation between two planes.
  
  The functions find and return the perspective transformation $H$ between
  the source and the destination planes:
  
  $$s_i  \vecthree{x'_i}{y'_i}{1} \sim H  \vecthree{x_i}{y_i}{1}$$
  
  so that the back-projection error
  
  $$\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2$$
  
  is minimized. If the parameter `method` is set to the default value 0,
  the function uses all the point pairs to compute an initial homography
  estimate with a simple least-squares scheme.
  
  However, if not all of the point pairs ( $srcPoints_i$,$dstPoints_i$ )
  fit the rigid perspective transformation (that is, there are some
  outliers), this initial estimate will be poor. In this case, you can use
  one of the two robust methods. Both methods, `RANSAC` and `LMeDS` , try
  many different random subsets of the corresponding point pairs (of four
  pairs each), estimate the homography matrix using this subset and a
  simple least-square algorithm, and then compute the quality/goodness of
  the computed homography (which is the number of inliers for RANSAC or
  the median re-projection error for LMeDs). The best subset is then used
  to produce the initial estimate of the homography matrix and the mask of
  inliers/outliers.
  
  Regardless of the method, robust or not, the computed homography matrix
  is refined further (using inliers only in case of a robust method) with
  the Levenberg-Marquardt method to reduce the re-projection error even
  more.
  
  The method `RANSAC` can handle practically any ratio of outliers but it
  needs a threshold to distinguish inliers from outliers. The method
  `LMeDS` does not need any threshold but it works correctly only when
  there are more than 50% of inliers. Finally, if there are no outliers
  and the noise is rather small, use the default method (`method=0`).
  
  The function is used to find initial intrinsic and extrinsic matrices.
  Homography matrix is determined up to a scale. Thus, it is normalized so
  that $h_{33}=1$ .
cv::estimateAffine3D: |-
  Computes an optimal affine transformation between two 3D point sets.
  
  The function estimates an optimal 3D affine transformation between two
  3D point sets using the RANSAC algorithm.
cv::filterSpeckles: Filters off small noise blobs (speckles) in the disparity map
cv::getOptimalNewCameraMatrix: "Returns the new camera matrix based on the free scaling parameter.\n\n\
  The function computes and returns the optimal new camera matrix based on\n\
  the free scaling parameter. By varying this parameter, you may retrieve\n\
  only sensible pixels `alpha=0` , keep all the original image pixels if\n\
  there is valuable information in the corners `alpha=1` , or get\n\
  something in between. When `alpha>0` , the undistortion result is likely\n\
  to have some black pixels corresponding to \"virtual\" pixels outside of\n\
  the captured distorted image. The original camera matrix, distortion\n\
  coefficients, the computed new camera matrix, and `newImageSize` should\n\
  be passed to :ocvinitUndistortRectifyMap to produce the maps for\n\
  :ocvremap ."
cv::initCameraMatrix2D: |-
  Finds an initial camera matrix from 3D-2D point correspondences.
  
  The function estimates and returns an initial camera matrix for the
  camera calibration process. Currently, the function only supports planar
  calibration patterns, which are patterns where each object point has
  z-coordinate =0.
cv::matMulDeriv: "Computes partial derivatives of the matrix product for each multiplied\n\
  matrix.\n\n\
  The function computes partial derivatives of the elements of the matrix\n\
  product $A*B$ with regard to the elements of each of the two input\n\
  matrices. The function is used to compute the Jacobian matrices in\n\
  :ocvstereoCalibrate but can also be used in any other similar\n\
  optimization function."
cv::projectPoints: "Projects 3D points to an image plane.\n\n\
  The function computes projections of 3D points to the image plane given\n\
  intrinsic and extrinsic camera parameters. Optionally, the function\n\
  computes Jacobians - matrices of partial derivatives of image points\n\
  coordinates (as functions of all the input parameters) with respect to\n\
  the particular parameters, intrinsic and/or extrinsic. The Jacobians are\n\
  used during the global optimization in :ocvcalibrateCamera,\n\
  :ocvsolvePnP, and :ocvstereoCalibrate . The function itself can also be\n\
  used to compute a re-projection error given the current intrinsic and\n\
  extrinsic parameters.\n\n\
  **note**\n\n\
  By setting `rvec=tvec=(0,0,0)` or by setting `cameraMatrix` to a 3x3\n\
  identity matrix, or by passing zero distortion coefficients, you can\n\
  get various useful partial cases of the function. This means that you\n\
  can compute the distorted coordinates for a sparse set of points or\n\
  apply a perspective transformation (and also compute the derivatives)\n\
  in the ideal zero-distortion setup.\n"
cv::reprojectImageTo3D: |-
  Reprojects a disparity image to 3D space.
  
  The function transforms a single-channel disparity map to a 3-channel
  image representing a 3D surface. That is, for each pixel `(x,y)` andthe
  corresponding disparity `d=disparity(x,y)` , it computes:
  
  $$\begin{array}{l} [X \; Y \; Z \; W]^T =  \texttt{Q} *[x \; y \; \texttt{disparity} (x,y) \; 1]^T  \ \texttt{_3dImage} (x,y) = (X/W, \; Y/W, \; Z/W) \end{array}$$
  
  The matrix `Q` can be an arbitrary $4 \times 4$ matrix (for example, the
  one computed by :ocvstereoRectify). To reproject a sparse set of points
  {(x,y,d),...} to 3D space, use :ocvperspectiveTransform .
cv::RQDecomp3x3: |-
  Computes an RQ decomposition of 3x3 matrices.
  
  The function computes a RQ decomposition using the given rotations. This
  function is used in :ocvdecomposeProjectionMatrix to decompose the left
  3x3 submatrix of a projection matrix into a camera and a rotation
  matrix.
  
  It optionally returns three rotation matrices, one for each axis, and
  the three Euler angles in degrees (as the return value) that could be
  used in OpenGL. Note, there is always more than one sequence of
  rotations about the three principle axes that results in the same
  orientation of an object, eg. see [Slabaugh]_. Returned tree rotation
  matrices and corresponding three Euler angules are only one of the
  possible solutions.
cv::Rodrigues: |-
  Converts a rotation matrix to a rotation vector or vice versa.
  
  $$\begin{array}{l} \theta \leftarrow norm(r) \ r  \leftarrow r/ \theta \ R =  \cos{\theta} I + (1- \cos{\theta} ) r r^T +  \sin{\theta} \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \end{array}$$
  
  Inverse transformation can be also done easily, since
  
  $$\sin ( \theta ) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \frac{R - R^T}{2}$$
  
  A rotation vector is a convenient and most compact representation of a
  rotation matrix (since any rotation matrix has just 3 degrees of
  freedom). The representation is used in the global 3D geometry
  optimization procedures like :ocvcalibrateCamera, :ocvstereoCalibrate,
  or :ocvsolvePnP .
cv::StereoMatcher: The base class for stereo correspondence algorithms.
cv::StereoMatcher::compute: Computes disparity map for the specified stereo pair
cv::StereoBM: |-
  Class for computing stereo correspondence using the block matching
  algorithm, introduced and contributed to OpenCV by K. Konolige.
cv::createStereoBM: |-
  Creates StereoBM object
  
  The function create `StereoBM` object. You can then call
  `StereoBM::compute()` to compute disparity for a specific stereo pair.
cv::StereoSGBM: |
  The class implements the modified H. Hirschmuller algorithm [HH08]_
  that differs from the original one as follows:
  
    By default, the algorithm is single-pass, which means that you
  consider only 5 directions instead of 8. Set
  `mode=StereoSGBM::MODE_HH` in `createStereoSGBM` to run the full
  variant of the algorithm but beware that it may consume a lot of
  memory.
  
  
    The algorithm matches blocks, not individual pixels. Though,
  setting `blockSize=1` reduces the blocks to single pixels.
  
  
    Mutual information cost function is not implemented. Instead, a
  simpler Birchfield-Tomasi sub-pixel metric from [BT98]_ is used.
  Though, the color images are supported as well.
  
  
    Some pre- and post- processing steps from K. Konolige algorithm
  `StereoBM` are included, for example: pre-filtering
  (`StereoBM::PREFILTER_XSOBEL` type) and post-filtering (uniqueness
  check, quadratic interpolation and speckle filtering).

cv::createStereoSGBM: "\n\n\
  Creates StereoSGBM object\n\n\
  The first constructor initializes `StereoSGBM` with all the default\n\
  parameters. So, you only have to set `StereoSGBM::numDisparities` at\n\
  minimum. The second constructor enables you to set each parameter to a\n\
  custom value."
cv::stereoCalibrate: |-
  Calibrates the stereo camera.
  
  The function estimates transformation between two cameras making a
  stereo pair. If you have a stereo camera where the relative position and
  orientation of two cameras is fixed, and if you computed poses of an
  object relative to the first camera and to the second camera, (R1, T1)
  and (R2, T2), respectively (this can be done with :ocvsolvePnP ), then
  those poses definitely relate to each other. This means that, given (
  $R_1$,$T_1$ ), it should be possible to compute ( $R_2$,$T_2$ ). You
  only need to know the position and orientation of the second camera
  relative to the first camera. This is what the described function does.
  It computes ( $R$,$T$ ) so that:
  
  $$R_2=R*R_1
  T_2=R*T_1 + T,$$
  
  Optionally, it computes the essential matrix E:
  
  $$E= \vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} *R$$
  
  where $T_i$ are components of the translation vector $T$ :
  $T=[T_0, T_1, T_2]^T$ . And the function can also compute the
  fundamental matrix F:
  
  $$F = cameraMatrix2^{-T} E cameraMatrix1^{-1}$$
  
  Besides the stereo-related information, the function can also perform a
  full calibration of each of two cameras. However, due to the high
  dimensionality of the parameter space and noise in the input data, the
  function can diverge from the correct solution. If the intrinsic
  parameters can be estimated with high accuracy for each of the cameras
  individually (for example, using :ocvcalibrateCamera ), you are
  recommended to do so and then pass `CV_CALIB_FIX_INTRINSIC` flag to the
  function along with the computed intrinsic parameters. Otherwise, if all
  the parameters are estimated at once, it makes sense to restrict some
  parameters, for example, pass `CV_CALIB_SAME_FOCAL_LENGTH` and
  `CV_CALIB_ZERO_TANGENT_DIST` flags, which is usually a reasonable
  assumption.
  
  Similarly to :ocvcalibrateCamera , the function minimizes the total
  re-projection error for all the points in all the available views from
  both cameras. The function returns the final value of the re-projection
  error.
cv::stereoRectify: |-
  Computes rectification transforms for each head of a calibrated stereo
  camera.
  
  The function computes the rotation matrices for each camera that
  (virtually) make both camera image planes the same plane. Consequently,
  this makes all the epipolar lines parallel and thus simplifies the dense
  stereo correspondence problem. The function takes the matrices computed
  by :ocvstereoCalibrate as input. As output, it provides two rotation
  matrices and also two projection matrices in the new coordinates. The
  function distinguishes the following two cases:
  
  #.
  :   **Horizontal stereo**: the first and the second camera views are
      shifted relative to each other mainly along the x axis (with
      possible small vertical shift). In the rectified images, the
      corresponding epipolar lines in the left and right cameras are
      horizontal and have the same y-coordinate. P1 and P2 look like:
  
  ```c++
  $$\texttt{P1} = \begin{bmatrix} f & 0 & cx_1 & 0 \\ 0 & f & cy & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}$$
  
  $$\texttt{P2} = \begin{bmatrix} f & 0 & cx_2 & T_x*f \\ 0 & f & cy & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} ,$$
  
  where $T_x$ is a horizontal shift between the cameras and
  $cx_1=cx_2$ if `CV_CALIB_ZERO_DISPARITY` is set.
  ```
  
  
  #.
  :   **Vertical stereo**: the first and the second camera views are
      shifted relative to each other mainly in vertical direction (and
      probably a bit in the horizontal direction too). The epipolar lines
      in the rectified images are vertical and have the same x-coordinate.
      P1 and P2 look like:
  
  ```c++
  $$\texttt{P1} = \begin{bmatrix} f & 0 & cx & 0 \\ 0 & f & cy_1 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}$$
  
  $$\texttt{P2} = \begin{bmatrix} f & 0 & cx & 0 \\ 0 & f & cy_2 & T_y*f \\ 0 & 0 & 1 & 0 \end{bmatrix} ,$$
  
  where $T_y$ is a vertical shift between the cameras and $cy_1=cy_2$
  if `CALIB_ZERO_DISPARITY` is set.
  ```
  
  
  As you can see, the first three columns of `P1` and `P2` will
  effectively be the new "rectified" camera matrices. The matrices,
  together with `R1` and `R2` , can then be passed to
  :ocvinitUndistortRectifyMap to initialize the rectification map for each
  camera.
  
  See below the screenshot from the `stereo_calib.cpp` sample. Some red
  horizontal lines pass through the corresponding image regions. This
  means that the images are well rectified, which is what most stereo
  correspondence algorithms rely on. The green rectangles are `roi1` and
  `roi2` . You see that their interiors are all valid pixels.
  
  ![image](pics/stereo_undistort.jpg)
cv::stereoRectifyUncalibrated: |
  Computes a rectification transform for an uncalibrated stereo camera.
  
  The function computes the rectification transformations without knowing
  intrinsic parameters of the cameras and their relative position in the
  space, which explains the suffix "uncalibrated". Another related
  difference from :ocvstereoRectify is that the function outputs not the
  rectification transformations in the object (3D) space, but the planar
  perspective transformations encoded by the homography matrices `H1` and
  `H2` . The function implements the algorithm [Hartley99]_.
  
  **note**
  
  While the algorithm does not need to know the intrinsic parameters of
  the cameras, it heavily depends on the epipolar geometry. Therefore,
  if the camera lenses have a significant distortion, it would be better
  to correct it before computing the fundamental matrix and calling this
  function. For example, distortion coefficients can be estimated for
  each head of stereo camera separately by using :ocvcalibrateCamera .
  Then, the images can be corrected using :ocvundistort , or just the
  point coordinates can be corrected with :ocvundistortPoints .

cv::triangulatePoints: |-
  Reconstructs points by triangulation.
  
  The function reconstructs 3-dimensional points (in homogeneous
  coordinates) by using their observations with a stereo camera.
  Projections matrices can be obtained from :ocvstereoRectify.
cv::Haar Feature-based Cascade Classifier for Object Detection: |
  The object detector described below has been initially proposed by Paul
  Viola [Viola01]_ and improved by Rainer Lienhart [Lienhart02]_.
  
  First, a classifier (namely a *cascade of boosted classifiers working
  with haar-like features*) is trained with a few hundred sample views of
  a particular object (i.e., a face or a car), called positive examples,
  that are scaled to the same size (say, 20x20), and negative examples -
  arbitrary images of the same size.
  
  After a classifier is trained, it can be applied to a region of interest
  (of the same size as used during the training) in an input image. The
  classifier outputs a "1" if the region is likely to show the object
  (i.e., face/car), and "0" otherwise. To search for the object in the
  whole image one can move the search window across the image and check
  every location using the classifier. The classifier is designed so that
  it can be easily "resized" in order to be able to find the objects of
  interest at different sizes, which is more efficient than resizing the
  image itself. So, to find an object of an unknown size in the image the
  scan procedure should be done several times at different scales.
  
  The word "cascade" in the classifier name means that the resultant
  classifier consists of several simpler classifiers (*stages*) that are
  applied subsequently to a region of interest until at some stage the
  candidate is rejected or all the stages are passed. The word "boosted"
  means that the classifiers at every stage of the cascade are complex
  themselves and they are built out of basic classifiers using one of four
  different `boosting` techniques (weighted voting). Currently Discrete
  Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported.
  The basic classifiers are decision-tree classifiers with at least 2
  leaves. Haar-like features are the input to the basic classifiers, and
  are calculated as described below. The current algorithm uses the
  following Haar-like features:
  
  ![image](pics/haarfeatures.png)
  
  The feature used in a particular classifier is specified by its shape
  (1a, 2b etc.), position within the region of interest and the scale
  (this scale is not the same as the scale used at the detection stage,
  though these two scales are multiplied). For example, in the case of the
  third line feature (2c) the response is calculated as the difference
  between the sum of image pixels under the rectangle covering the whole
  feature (including the two white stripes and the black stripe in the
  middle) and the sum of the image pixels under the black stripe
  multiplied by 3 in order to compensate for the differences in the size
  of areas. The sums of pixel values over a rectangular regions are
  calculated rapidly using integral images (see below and the :ocvintegral
  description).
  
  To see the object detector at work, have a look at the facedetect demo:
  <http://code.opencv.org/projects/opencv/repository/revisions/master/entry/samples/cpp/dbt_face_detection.cpp>
  
  The following reference is for the detection part only. There is a
  separate application called `opencv_traincascade` that can train a
  cascade of boosted classifiers from a set of samples.
  
  **note**
  
  In the new C++ interface it is also possible to use LBP (local binary
  pattern) features in addition to Haar-like features.

cv::FeatureEvaluator: |
  Base class for computing feature values in cascade classifiers. :
  
  ```c++
  class CV_EXPORTS FeatureEvaluator
  {
  public:
      enum { HAAR = 0, LBP = 1 }; // supported feature types
      virtual ~FeatureEvaluator(); // destructor
      virtual bool read(const FileNode& node);
      virtual Ptr<FeatureEvaluator> clone() const;
      virtual int getFeatureType() const;
  
      virtual bool setImage(const Mat& img, Size origWinSize);
      virtual bool setWindow(Point p);
  
      virtual double calcOrd(int featureIdx) const;
      virtual int calcCat(int featureIdx) const;
  
      static Ptr<FeatureEvaluator> create(int type);
  };
  ```

cv::FeatureEvaluator::read: Reads parameters of features from the `FileStorage` node.
cv::FeatureEvaluator::clone: Returns a full copy of the feature evaluator.
cv::FeatureEvaluator::getFeatureType: Returns the feature type (`HAAR` or `LBP` for now).
cv::FeatureEvaluator::setImage: |-
  Assigns an image to feature evaluator.
  
  The method assigns an image, where the features will be computed, to the
  feature evaluator.
cv::FeatureEvaluator::setWindow: |-
  Assigns a window in the current image where the features will be
  computed.
cv::FeatureEvaluator::calcOrd: |-
  Computes the value of an ordered (numerical) feature.
  
  The function returns the computed value of an ordered feature.
cv::FeatureEvaluator::calcCat: |-
  Computes the value of a categorical feature.
  
  The function returns the computed label of a categorical feature, which
  is the value from [0,... (number of categories - 1)].
cv::FeatureEvaluator::create: Constructs the feature evaluator.
cv::CascadeClassifier: Cascade classifier class for object detection.
cv::CascadeClassifier::CascadeClassifier: Loads a classifier from a file.
cv::CascadeClassifier::empty: Checks whether the classifier has been loaded.
cv::CascadeClassifier::load: Loads a classifier from a file.
cv::CascadeClassifier::read: |
  Reads a classifier from a FileStorage node.
  
  **note**
  
  The file may contain a new cascade classifier (trained traincascade
  application) only.

cv::CascadeClassifier::detectMultiScale: |-
  Detects objects of different sizes in the input image. The detected
  objects are returned as a list of rectangles.
  
  The function is parallelized with the TBB library.
cv::CascadeClassifier::setImage: "Sets an image for detection.\n\n\
  The function is automatically called by\n\
  :ocvCascadeClassifier::detectMultiScale at every image scale. But if you\n\
  want to test various locations manually using\n\
  :ocvCascadeClassifier::runAt, you need to call the function before, so\n\
  that the integral images are computed.\n\n\
  **note**\n\n\
  in the old API you need to supply integral images (that can be\n\
  obtained using :ocvIntegral) instead of the original image.\n"
cv::CascadeClassifier::runAt: |-
  Runs the detector at the specified point.
  
  The function returns 1 if the cascade classifier detects an object in
  the given location. Otherwise, it returns negated index of the stage at
  which the candidate has been rejected.
  
  Use :ocvCascadeClassifier::setImage to set the image for the detector to
  work with.
cv::groupRectangles: |-
  Groups the object candidate rectangles.
  
  The function is a wrapper for the generic function :ocvpartition . It
  clusters all the input rectangles using the rectangle equivalence
  criteria that combines rectangles with similar sizes and similar
  locations. The similarity is defined by `eps`. When `eps=0` , no
  clustering is done at all. If $\texttt{eps}\rightarrow +\inf$ , all the
  rectangles are put in one cluster. Then, the small clusters containing
  less than or equal to `groupThreshold` rectangles are rejected. In each
  other cluster, the average rectangle is computed and put into the output
  rectangle list.
cv::kmeans: |-
  Finds centers of clusters and groups input samples around the clusters.
  
  The function `kmeans` implements a k-means algorithm that finds the
  centers of `cluster_count` clusters and groups the input samples around
  the clusters. As an output, $\texttt{labels}_i$ contains a 0-based
  cluster index for the sample stored in the $i^{th}$ row of the `samples`
  matrix.
  
  The function returns the compactness measure that is computed as
  
  $$\sum _i  | \texttt{samples} _i -  \texttt{centers} _{ \texttt{labels} _i} | ^2$$
  
  after every attempt. The best (minimum) value is chosen and the
  corresponding labels and the compactness value are returned by the
  function. Basically, you can use only the core of the function, set the
  number of attempts to 1, initialize labels each time using a custom
  algorithm, pass them with the ( `flags` = `KMEANS_USE_INITIAL_LABELS` )
  flag, and then choose the best (most-compact) clustering.
cv::partition: |-
  Splits an element set into equivalency classes.
  
  The generic function `partition` implements an $O(N^2)$ algorithm for
  splitting a set of $N$ elements into one or more equivalency classes, as
  described in <http://en.wikipedia.org/wiki/Disjoint-set_data_structure>
  . The function returns the number of equivalency classes.
cv::gpu::cvtColor: |-
  Converts an image from one color space to another.
  
  3-channel color spaces (like `HSV`, `XYZ`, and so on) can be stored in a
  4-channel image for better performance.
cv::gpu::swapChannels: |-
  Exchanges the color channels of an image in-place.
  
  The methods support arbitrary permutations of the original channels,
  including replication.
cv::gpu::alphaComp: |-
  Composites two images using alpha opacity values contained in each
  image.
cv::applyColorMap: |-
  Applies a GNU Octave/MATLAB equivalent colormap on a given image.
  
  Currently the following GNU Octave/MATLAB equivalent colormaps are
  implemented:
  
  ~~~~ {.sourceCode .cpp}
  enum
  {
      COLORMAP_AUTUMN = 0,
      COLORMAP_BONE = 1,
      COLORMAP_JET = 2,
      COLORMAP_WINTER = 3,
      COLORMAP_RAINBOW = 4,
      COLORMAP_OCEAN = 5,
      COLORMAP_SUMMER = 6,
      COLORMAP_SPRING = 7,
      COLORMAP_COOL = 8,
      COLORMAP_HSV = 9,
      COLORMAP_PINK = 10,
      COLORMAP_HOT = 11
  }
  ~~~~
cv::Description: "The human perception isn't built for observing fine changes in grayscale\n\
  images. Human eyes are more sensitive to observing changes between\n\
  colors, so you often need to recolor your grayscale images to get a clue\n\
  about them. OpenCV now comes with various colormaps to enhance the\n\
  visualization in your computer vision application.\n\n\
  In OpenCV 2.4 you only need :ocvapplyColorMap to apply a colormap on a\n\
  given image. The following sample code reads the path to an image from\n\
  command line, applies a Jet colormap on it and shows the result:\n\n\
  ~~~~ {.sourceCode .cpp}\n\n\
  I'll go a bit more into detail explaining :ocvFaceRecognizer, because it\n\
  doesn't look like a powerful interface at first sight. But: Every\n\
  :ocvFaceRecognizer is an :ocvAlgorithm, so you can easily get/set all\n\
  model internals (if allowed by the implementation). :ocvAlgorithm is a\n\
  relatively new OpenCV concept, which is available since the 2.4 release.\n\
  I suggest you take a look at its description.\n\n\
  :ocvAlgorithm provides the following features for all derived classes:\n\n  So called \xE2\x80\x9Cvirtual constructor\xE2\x80\x9D. That is, each Algorithm derivative\n\
  is registered at program start and you can get the list of\n\
  registered algorithms and create instance of a particular algorithm\n\
  by its name (see :ocvAlgorithm::create). If you plan to add your own\n\
  algorithms, it is good practice to add a unique prefix to your\n\
  algorithms to distinguish them from other algorithms.\n\n\n  Setting/Retrieving algorithm parameters by name. If you used video\n\
  capturing functionality from OpenCV highgui module, you are probably\n\
  familar with :ocvcvSetCaptureProperty, :ocvcvGetCaptureProperty,\n\
  :ocvVideoCapture::set and :ocvVideoCapture::get. :ocvAlgorithm\n\
  provides similar method where instead of integer id's you specify\n\
  the parameter names as text Strings. See :ocvAlgorithm::set and\n\
  :ocvAlgorithm::get for details.\n\n\n  Reading and writing parameters from/to XML or YAML files. Every\n\
  Algorithm derivative can store all its parameters and then read them\n\
  back. There is no need to re-implement it each time.\n\n\n\
  Moreover every :ocvFaceRecognizer supports the:\n\n  **Training** of a :ocvFaceRecognizer with :ocvFaceRecognizer::train\n\
  on a given set of images (your face database!).\n\n\n  **Prediction** of a given sample image, that means a face. The image\n\
  is given as a :ocvMat.\n\n\n  **Loading/Saving** the model state from/to a given XML or YAML.\n\n\n\
  Class which allows the [Gipsa](http://www.gipsa-lab.inpg.fr)\n\
  (preliminary work) / [Listic](http://www.listic.univ-savoie.fr) (code\n\
  maintainer and user) labs retina model to be used. This class allows\n\
  human retina spatio-temporal image processing to be applied on still\n\
  images, images sequences and video sequences. Briefly, here are the main\n\
  human retina model properties:\n\n  spectral whithening (mid-frequency details enhancement)\n\n\n  high frequency spatio-temporal noise reduction (temporal noise and\n\
  high frequency spatial noise are minimized)\n\n\n  low frequency luminance reduction (luminance range compression) :\n\
  high luminance regions do not hide details in darker regions anymore\n\n\n  local logarithmic luminance compression allows details to be\n\
  enhanced even in low light conditions\n\n\n\
  Use : this model can be used basically for spatio-temporal video effects\n\
  but also in the aim of :\n\n  performing texture analysis with enhanced signal to noise ratio and\n\
  enhanced details robust against input images luminance ranges (check\n\
  out the parvocellular retina channel output, by using the provided\n\
  **getParvo** methods)\n\n\n  performing motion analysis also taking benefit of the previously\n\
  cited properties (check out the magnocellular retina channel output,\n\
  by using the provided **getMagno** methods)\n"
cv::include <opencv2/highgui.hpp>: |-
  using namespace cv;
  
  int main(int argc, const char *argv[]) {
      // Get the path to the image, if it was given
      // if no arguments were given.
      String filename;
      if (argc > 1) {
          filename = String(argv[1]);
      }
      // The following lines show how to apply a colormap on a given image
      // and show it with cv::imshow example with an image. An exception is
      // thrown if the path to the image is invalid.
      if(!filename.empty()) {
          Mat img0 = imread(filename);
          // Throw an exception, if the image can't be read:
          if(img0.empty()) {
              CV_Error(CV_StsBadArg, "Sample image is empty. Please adjust your path, so it points to a valid input image!");
          }
          // Holds the colormap version of the image:
          Mat cm_img0;
          // Apply the colormap:
          applyColorMap(img0, cm_img0, COLORMAP_JET);
          // Show the result:
          imshow("cm_img0", cm_img0);
          waitKey(0);
      }
  
  ```c++
  return 0;
  ```
  
  
  }
  ~~~~
  
  And here are the color scales for each of the available colormaps:
  
    Class                  Scale
  
    COLORMAP_AUTUMN       ![image](img/colormaps/colorscale_autumn.jpg)
    COLORMAP_BONE         ![image](img/colormaps/colorscale_bone.jpg)
    COLORMAP_COOL         ![image](img/colormaps/colorscale_cool.jpg)
    COLORMAP_HOT          ![image](img/colormaps/colorscale_hot.jpg)
    COLORMAP_HSV          ![image](img/colormaps/colorscale_hsv.jpg)
    COLORMAP_JET          ![image](img/colormaps/colorscale_jet.jpg)
    COLORMAP_OCEAN        ![image](img/colormaps/colorscale_ocean.jpg)
    COLORMAP_PINK         ![image](img/colormaps/colorscale_pink.jpg)
    COLORMAP_RAINBOW      ![image](img/colormaps/colorscale_rainbow.jpg)
    COLORMAP_SPRING       ![image](img/colormaps/colorscale_spring.jpg)
    COLORMAP_SUMMER       ![image](img/colormaps/colorscale_summer.jpg)
    COLORMAP_WINTER       ![image](img/colormaps/colorscale_winter.jpg)
cv::CommandLineParser: |
  The CommandLineParser class is designed for command line arguments
  parsing
  
  The sample below demonstrates how to use CommandLineParser:
  
  ```c++
  CommandLineParser parser(argc, argv, keys);
  parser.about("Application name v1.0.0");
  
  if (parser.has("help"))
  {
      parser.printMessage();
      return 0;
  }
  
  int N = parser.get<int>("N");
  double fps = parser.get<double>("fps");
  String path = parser.get<String>("path");
  
  use_time_stamp = parser.has("timestamp");
  
  String img1 = parser.get<String>(0);
  String img2 = parser.get<String>(1);
  
  int repeat = parser.get<int>(2);
  
  if (!parser.check())
  {
      parser.printErrors();
      return 0;
  }
  ```
  
  
  Syntax:
  
  ```c++
  const String keys =
      "{help h usage ? |      | print this message   }"
      "{@image1        |      | image1 for compare   }"
      "{@image2        |      | image2 for compare   }"
      "{@repeat        |1     | number               }"
      "{path           |.     | path to file         }"
      "{fps            | -1.0 | fps for output video }"
      "{N count        |100   | count of objects     }"
      "{ts timestamp   |      | use time stamp       }"
      ;
  ```
  
  
  Use:
  
  ```c++
  # ./app -N=200 1.png 2.jpg 19 -ts
  
  # ./app -fps=aaa
  ERRORS:
  Exception: can not convert: [aaa] to [double]
  ```

cv::Common Interfaces of Descriptor Extractors: "Extractors of keypoint descriptors in OpenCV have wrappers with a common\n\
  interface that enables you to easily switch between different algorithms\n\
  solving the same problem. This section is devoted to computing\n\
  descriptors represented as vectors in a multidimensional space. All\n\
  objects that implement the `vector` descriptor extractors inherit the\n\
  :ocvDescriptorExtractor interface."
cv::CalonderDescriptorExtractor: |
  Wrapping class for computing descriptors by using the
  :ocvRTreeClassifier class. :
  
  ```c++
  template<typename T>
  class CalonderDescriptorExtractor : public DescriptorExtractor
  {
  public:
      CalonderDescriptorExtractor( const String& classifierFile );
  
      virtual void read( const FileNode &fn );
      virtual void write( FileStorage &fs ) const;
      virtual int descriptorSize() const;
      virtual int descriptorType() const;
  protected:
      ...
  }
  ```

cv::Common Interfaces of Descriptor Matchers: "Matchers of keypoint descriptors in OpenCV have wrappers with a common\n\
  interface that enables you to easily switch between different algorithms\n\
  solving the same problem. This section is devoted to matching\n\
  descriptors that are represented as vectors in a multidimensional space.\n\
  All objects that implement `vector` descriptor matchers inherit the\n\
  :ocvDescriptorMatcher interface."
cv::DescriptorMatcher: |
  Abstract base class for matching keypoint descriptors. It has two groups
  of match methods: for matching descriptors of an image with another
  image or with an image set. :
  
  ```c++
  class DescriptorMatcher
  {
  public:
      virtual ~DescriptorMatcher();
  
      virtual void add( const vector<Mat>& descriptors );
  
      const vector<Mat>& getTrainDescriptors() const;
      virtual void clear();
      bool empty() const;
      virtual bool isMaskSupported() const = 0;
  
      virtual void train();
  
      /*
       * Group of methods to match descriptors from an image pair.
       */
      void match( const Mat& queryDescriptors, const Mat& trainDescriptors,
                  vector<DMatch>& matches, const Mat& mask=Mat() ) const;
      void knnMatch( const Mat& queryDescriptors, const Mat& trainDescriptors,
                     vector<vector<DMatch> >& matches, int k,
                     const Mat& mask=Mat(), bool compactResult=false ) const;
      void radiusMatch( const Mat& queryDescriptors, const Mat& trainDescriptors,
                        vector<vector<DMatch> >& matches, float maxDistance,
                        const Mat& mask=Mat(), bool compactResult=false ) const;
      /*
       * Group of methods to match descriptors from one image to an image set.
       */
      void match( const Mat& queryDescriptors, vector<DMatch>& matches,
                  const vector<Mat>& masks=vector<Mat>() );
      void knnMatch( const Mat& queryDescriptors, vector<vector<DMatch> >& matches,
                     int k, const vector<Mat>& masks=vector<Mat>(),
                     bool compactResult=false );
      void radiusMatch( const Mat& queryDescriptors, vector<vector<DMatch> >& matches,
                        float maxDistance, const vector<Mat>& masks=vector<Mat>(),
                        bool compactResult=false );
  
      virtual void read( const FileNode& );
      virtual void write( FileStorage& ) const;
  
      virtual Ptr<DescriptorMatcher> clone( bool emptyTrainData=false ) const = 0;
  
      static Ptr<DescriptorMatcher> create( const String& descriptorMatcherType );
  
  protected:
      vector<Mat> trainDescCollection;
      ...
  };
  ```

cv::DescriptorMatcher::add: |-
  Adds descriptors to train a descriptor collection. If the collection
  `trainDescCollectionis` is not empty, the new descriptors are added to
  existing train descriptors.
cv::DescriptorMatcher::getTrainDescriptors: |-
  Returns a constant link to the train descriptor collection
  `trainDescCollection` .
cv::DescriptorMatcher::clear: Clears the train descriptor collection.
cv::DescriptorMatcher::empty: Returns true if there are no train descriptors in the collection.
cv::DescriptorMatcher::isMaskSupported: |-
  Returns true if the descriptor matcher supports masking permissible
  matches.
cv::DescriptorMatcher::train: |-
  Trains a descriptor matcher
  
  Trains a descriptor matcher (for example, the flann index). In all
  methods to match, the method `train()` is run every time before
  matching. Some descriptor matchers (for example, `BruteForceMatcher`)
  have an empty implementation of this method. Other matchers really train
  their inner structures (for example, `FlannBasedMatcher` trains
  `flann::Index` ).
cv::DescriptorMatcher::match: |-
  Finds the best match for each descriptor from a query set.
  
  In the first variant of this method, the train descriptors are passed as
  an input argument. In the second variant of the method, train
  descriptors collection that was set by `DescriptorMatcher::add` is used.
  Optional mask (or masks) can be passed to specify which query and
  training descriptors can be matched. Namely, `queryDescriptors[i]` can
  be matched with `trainDescriptors[j]` only if `mask.at<uchar>(i,j)` is
  non-zero.
cv::DescriptorMatcher::knnMatch: |-
  Finds the k best matches for each descriptor from a query set.
  
  These extended variants of :ocvDescriptorMatcher::match methods find
  several best matches for each query descriptor. The matches are returned
  in the distance increasing order. See :ocvDescriptorMatcher::match for
  the details about query and train descriptors.
cv::DescriptorMatcher::radiusMatch: |-
  For each query descriptor, finds the training descriptors not farther
  than the specified distance.
  
  For each query descriptor, the methods find such training descriptors
  that the distance between the query descriptor and the training
  descriptor is equal or smaller than `maxDistance`. Found matches are
  returned in the distance increasing order.
cv::DescriptorMatcher::clone: Clones the matcher.
cv::DescriptorMatcher::create: |-
  Creates a descriptor matcher of a given type with the default parameters
  (using default constructor).
cv::BFMatcher: |-
  Brute-force descriptor matcher. For each descriptor in the first set,
  this matcher finds the closest descriptor in the second set by trying
  each one. This descriptor matcher supports masking permissible matches
  of descriptor sets.
cv::BFMatcher::BFMatcher: Brute-force matcher constructor.
cv::FlannBasedMatcher: |
  Flann-based descriptor matcher. This matcher trains :ocvflann::Index_
  on a train descriptor collection and calls its nearest search methods to
  find the best matches. So, this matcher may be faster when matching a
  large train collection than the brute force matcher. `FlannBasedMatcher`
  does not support masking permissible matches of descriptor sets because
  `flann::Index` does not support this. :
  
  ```c++
  class FlannBasedMatcher : public DescriptorMatcher
  {
  public:
      FlannBasedMatcher(
        const Ptr<flann::IndexParams>& indexParams=new flann::KDTreeIndexParams(),
        const Ptr<flann::SearchParams>& searchParams=new flann::SearchParams() );
  
      virtual void add( const vector<Mat>& descriptors );
      virtual void clear();
  
      virtual void train();
      virtual bool isMaskSupported() const;
  
      virtual Ptr<DescriptorMatcher> clone( bool emptyTrainData=false ) const;
  protected:
      ...
  };
  ```

cv::Common Interfaces of Feature Detectors: "Feature detectors in OpenCV have wrappers with a common interface that\n\
  enables you to easily switch between different algorithms solving the\n\
  same problem. All objects that implement keypoint detectors inherit the\n\
  :ocvFeatureDetector interface."
cv::FeatureDetector: |
  Abstract base class for 2D image feature detectors. :
  
  ```c++
  class CV_EXPORTS FeatureDetector
  {
  public:
      virtual ~FeatureDetector();
  
      void detect( const Mat& image, vector<KeyPoint>& keypoints,
                   const Mat& mask=Mat() ) const;
  
      void detect( const vector<Mat>& images,
                   vector<vector<KeyPoint> >& keypoints,
                   const vector<Mat>& masks=vector<Mat>() ) const;
  
      virtual void read(const FileNode&);
      virtual void write(FileStorage&) const;
  
      static Ptr<FeatureDetector> create( const String& detectorType );
  
  protected:
  ...
  };
  ```

cv::FeatureDetector::detect: |-
  Detects keypoints in an image (first variant) or image set (second
  variant).
cv::FeatureDetector::create: |-
  Creates a feature detector by its name.
  
  The following detector types are supported:
    `"FAST"` -- :ocvFastFeatureDetector
  
    `"STAR"` -- :ocvStarFeatureDetector
  
    `"SIFT"` -- :ocvSIFT (nonfree module)
  
    `"SURF"` -- :ocvSURF (nonfree module)
  
    `"ORB"` -- :ocvORB
  
    `"BRISK"` -- :ocvBRISK
  
    `"MSER"` -- :ocvMSER
  
    `"GFTT"` -- :ocvGoodFeaturesToTrackDetector
  
    `"HARRIS"` -- :ocvGoodFeaturesToTrackDetector with Harris detector
  enabled
  
    `"Dense"` -- :ocvDenseFeatureDetector
  
    `"SimpleBlob"` -- :ocvSimpleBlobDetector
  
  
  Also a combined format is supported: feature detector adapter name (
  `"Grid"` --:ocvGridAdaptedFeatureDetector, `"Pyramid"`
  --:ocvPyramidAdaptedFeatureDetector ) + feature detector name (see
  above), for example: `"GridFAST"`, `"PyramidSTAR"` .
cv::FastFeatureDetector: |
  Wrapping class for feature detection using the :ocvFAST method. :
  
  ```c++
  class FastFeatureDetector : public FeatureDetector
  {
  public:
      FastFeatureDetector( int threshold=1, bool nonmaxSuppression=true, type=FastFeatureDetector::TYPE_9_16 );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::GoodFeaturesToTrackDetector: |
  Wrapping class for feature detection using the :ocvgoodFeaturesToTrack
  function. :
  
  ```c++
  class GoodFeaturesToTrackDetector : public FeatureDetector
  {
  public:
      class Params
      {
      public:
          Params( int maxCorners=1000, double qualityLevel=0.01,
                  double minDistance=1., int blockSize=3,
                  bool useHarrisDetector=false, double k=0.04 );
          void read( const FileNode& fn );
          void write( FileStorage& fs ) const;
  
          int maxCorners;
          double qualityLevel;
          double minDistance;
          int blockSize;
          bool useHarrisDetector;
          double k;
      };
  
      GoodFeaturesToTrackDetector( const GoodFeaturesToTrackDetector::Params& params=
                                              GoodFeaturesToTrackDetector::Params() );
      GoodFeaturesToTrackDetector( int maxCorners, double qualityLevel,
                                   double minDistance, int blockSize=3,
                                   bool useHarrisDetector=false, double k=0.04 );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::MserFeatureDetector: |
  Wrapping class for feature detection using the :ocvMSER class. :
  
  ```c++
  class MserFeatureDetector : public FeatureDetector
  {
  public:
      MserFeatureDetector( CvMSERParams params=cvMSERParams() );
      MserFeatureDetector( int delta, int minArea, int maxArea,
                           double maxVariation, double minDiversity,
                           int maxEvolution, double areaThreshold,
                           double minMargin, int edgeBlurSize );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::StarFeatureDetector: |
  The class implements the keypoint detector introduced by K. Konolige,
  synonym of `StarDetector`. :
  
  ```c++
  class StarFeatureDetector : public FeatureDetector
  {
  public:
      StarFeatureDetector( int maxSize=16, int responseThreshold=30,
                           int lineThresholdProjected = 10,
                           int lineThresholdBinarized=8, int suppressNonmaxSize=5 );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::DenseFeatureDetector: |
  Class for generation of image features which are distributed densely and
  regularly over the image. :
  
  ```c++
  class DenseFeatureDetector : public FeatureDetector
  {
  public:
          DenseFeatureDetector( float initFeatureScale=1.f, int featureScaleLevels=1,
                        float featureScaleMul=0.1f,
                        int initXyStep=6, int initImgBound=0,
                        bool varyXyStepWithScale=true,
                        bool varyImgBoundWithScale=false );
  protected:
  ...
  ```
  
  
  };
  
  
  The detector generates several levels (in the amount of
  `featureScaleLevels`) of features. Features of each level are located in
  the nodes of a regular grid over the image (excluding the image boundary
  of given size). The level parameters (a feature scale, a node size, a
  size of boundary) are multiplied by `featureScaleMul` with level index
  growing depending on input flags, viz.:
  
    Feature scale is multiplied always.
  
  
    The grid node size is multiplied if `varyXyStepWithScale` is `true`.
  
  
    Size of image boundary is multiplied if `varyImgBoundWithScale` is
  `true`.

cv::SimpleBlobDetector: |
  Class for extracting blobs from an image. :
  
  ```c++
  class SimpleBlobDetector : public FeatureDetector
  {
  public:
  struct Params
  {
      Params();
      float thresholdStep;
      float minThreshold;
      float maxThreshold;
      size_t minRepeatability;
      float minDistBetweenBlobs;
  
      bool filterByColor;
      uchar blobColor;
  
      bool filterByArea;
      float minArea, maxArea;
  
      bool filterByCircularity;
      float minCircularity, maxCircularity;
  
      bool filterByInertia;
      float minInertiaRatio, maxInertiaRatio;
  
      bool filterByConvexity;
      float minConvexity, maxConvexity;
  };
  
  SimpleBlobDetector(const SimpleBlobDetector::Params &parameters = SimpleBlobDetector::Params());
  
  protected:
      ...
  };
  ```
  
  
  The class implements a simple algorithm for extracting blobs from an
  image:
  
   Convert the source image to binary images by applying thresholding
  with several thresholds from `minThreshold` (inclusive) to
  `maxThreshold` (exclusive) with distance `thresholdStep` between
  neighboring thresholds.
  
  
   Extract connected components from every binary image by
  :ocvfindContours and calculate their centers.
  
  
   Group centers from several binary images by their coordinates. Close
  centers form one group that corresponds to one blob, which is
  controlled by the `minDistBetweenBlobs` parameter.
  
  
   From the groups, estimate final centers of blobs and their radiuses
  and return as locations and sizes of keypoints.
  
  
  This class performs several filtrations of returned blobs. You should
  set `filterBy*` to true/false to turn on/off corresponding filtration.
  Available filtrations:
  
    **By color**. This filter compares the intensity of a binary image
  at the center of a blob to `blobColor`. If they differ, the blob
  is filtered out. Use `blobColor = 0` to extract dark blobs and
  `blobColor = 255` to extract light blobs.
  
  
    **By area**. Extracted blobs have an area between `minArea`
  (inclusive) and `maxArea` (exclusive).
  
  
    **By circularity**. Extracted blobs have circularity
  ($\frac{4*\pi*Area}{perimeter * perimeter}$) between
  `minCircularity` (inclusive) and `maxCircularity` (exclusive).
  
  
    **By ratio of the minimum inertia to maximum inertia**. Extracted
  blobs have this ratio between `minInertiaRatio` (inclusive) and
  `maxInertiaRatio` (exclusive).
  
  
    **By convexity**. Extracted blobs have convexity (area / area of
  blob convex hull) between `minConvexity` (inclusive) and
  `maxConvexity` (exclusive).
  
  
  Default values of parameters are tuned to extract dark circular blobs.

cv::GridAdaptedFeatureDetector: |
  Class adapting a detector to partition the source image into a grid and
  detect points in each cell. :
  
  ```c++
  class GridAdaptedFeatureDetector : public FeatureDetector
  {
  public:
      /*
       * detector            Detector that will be adapted.
       * maxTotalKeypoints   Maximum count of keypoints detected on the image.
       *                     Only the strongest keypoints will be kept.
       * gridRows            Grid row count.
       * gridCols            Grid column count.
       */
      GridAdaptedFeatureDetector( const Ptr<FeatureDetector>& detector,
                                  int maxTotalKeypoints, int gridRows=4,
                                  int gridCols=4 );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::PyramidAdaptedFeatureDetector: |
  Class adapting a detector to detect points over multiple levels of a
  Gaussian pyramid. Consider using this class for detectors that are not
  inherently scaled. :
  
  ```c++
  class PyramidAdaptedFeatureDetector : public FeatureDetector
  {
  public:
      PyramidAdaptedFeatureDetector( const Ptr<FeatureDetector>& detector,
                                     int levels=2 );
      virtual void read( const FileNode& fn );
      virtual void write( FileStorage& fs ) const;
  protected:
      ...
  };
  ```

cv::DynamicAdaptedFeatureDetector: |
  Adaptively adjusting detector that iteratively detects features until
  the desired number is found. :
  
  ```c++
  class DynamicAdaptedFeatureDetector: public FeatureDetector
  {
  public:
      DynamicAdaptedFeatureDetector( const Ptr<AdjusterAdapter>& adjuster,
          int min_features=400, int max_features=500, int max_iters=5 );
      ...
  };
  ```
  
  
  If the detector is persisted, it "remembers" the parameters used for the
  last detection. In this case, the detector may be used for consistent
  numbers of keypoints in a set of temporally related images, such as
  video streams or panorama series.
  
  `DynamicAdaptedFeatureDetector` uses another detector, such as FAST or
  SURF, to do the dirty work, with the help of `AdjusterAdapter` . If the
  detected number of features is not large enough, `AdjusterAdapter`
  adjusts the detection parameters so that the next detection results in a
  bigger or smaller number of features. This is repeated until either the
  number of desired features are found or the parameters are maxed out.
  
  Adapters can be easily implemented for any detector via the
  `AdjusterAdapter` interface.
  
  Beware that this is not thread-safe since the adjustment of parameters
  requires modification of the feature detector class instance.
  
  Example of creating `DynamicAdaptedFeatureDetector` : :
  
  ```c++
  //sample usage:
  //will create a detector that attempts to find
  //100 - 110 FAST Keypoints, and will at most run
  //FAST feature detection 10 times until that
  //number of keypoints are found
  Ptr<FeatureDetector> detector(new DynamicAdaptedFeatureDetector (100, 110, 10,
                                new FastAdjuster(20,true)));
  ```

cv::DynamicAdaptedFeatureDetector::DynamicAdaptedFeatureDetector: The constructor
cv::AdjusterAdapter: |-
  Class providing an interface for adjusting parameters of a feature
  detector. This interface is used by :ocvDynamicAdaptedFeatureDetector .
  It is a wrapper for :ocvFeatureDetector that enables adjusting
  parameters after feature detection. :
  
  ```c++
  class AdjusterAdapter: public FeatureDetector
  {
  public:
     virtual ~AdjusterAdapter() {}
     virtual void tooFew(int min, int n_detected) = 0;
     virtual void tooMany(int max, int n_detected) = 0;
     virtual bool good() const = 0;
     virtual Ptr<AdjusterAdapter> clone() const = 0;
     static Ptr<AdjusterAdapter> create( const String& detectorType );
  };
  ```
  
  
  See :ocvFastAdjuster, :ocvStarAdjuster, and :ocvSurfAdjuster for
  concrete implementations.
cv::AdjusterAdapter::tooFew: |
  Adjusts the detector parameters to detect more features.
  
  Example: :
  
  ```c++
  void FastAdjuster::tooFew(int min, int n_detected)
  {
          thresh_--;
  }
  ```

cv::AdjusterAdapter::tooMany: |
  Adjusts the detector parameters to detect less features.
  
  Example: :
  
  ```c++
  void FastAdjuster::tooMany(int min, int n_detected)
  {
          thresh_++;
  }
  ```

cv::AdjusterAdapter::good: |
  Returns false if the detector parameters cannot be adjusted any more.
  
  Example: :
  
  ```c++
  bool FastAdjuster::good() const
  {
          return (thresh_ > 1) && (thresh_ < 200);
  }
  ```

cv::AdjusterAdapter::create: Creates an adjuster adapter by name
cv::FastAdjuster: |
  :ocvAdjusterAdapter for :ocvFastFeatureDetector. This class decreases or
  increases the threshold value by 1. :
  
  ```c++
  class FastAdjuster FastAdjuster: public AdjusterAdapter
  {
  public:
          FastAdjuster(int init_thresh = 20, bool nonmax = true);
          ...
  };
  ```

cv::StarAdjuster: |
  :ocvAdjusterAdapter for :ocvStarFeatureDetector. This class adjusts the
  `responseThreshhold` of `StarFeatureDetector`. :
  
  ```c++
  class StarAdjuster: public AdjusterAdapter
  {
          StarAdjuster(double initial_thresh = 30.0);
          ...
  };
  ```

cv::SurfAdjuster: |
  :ocvAdjusterAdapter for `SurfFeatureDetector`. :
  
  ```c++
  class CV_EXPORTS SurfAdjuster: public AdjusterAdapter
  {
  public:
      SurfAdjuster( double initial_thresh=400.f, double min_thresh=2, double max_thresh=1000 );
  
      virtual void tooFew(int minv, int n_detected);
      virtual void tooMany(int maxv, int n_detected);
      virtual bool good() const;
  
      virtual Ptr<AdjusterAdapter> clone() const;
  
      ...
  };
  ```

cv::OneWayDescriptorBase: |
  Class encapsulates functionality for training/loading a set of one way
  descriptors and finding the nearest closest descriptor to an input
  feature. :
  
  ```c++
  class CV_EXPORTS OneWayDescriptorBase
  {
  public:
  
      // creates an instance of OneWayDescriptor from a set of training files
      // - patch_size: size of the input (large) patch
      // - pose_count: the number of poses to generate for each descriptor
      // - train_path: path to training files
      // - pca_config: the name of the file that contains PCA for small patches (2 times smaller
      // than patch_size each dimension
      // - pca_hr_config: the name of the file that contains PCA for large patches (of patch_size size)
      // - pca_desc_config: the name of the file that contains descriptors of PCA components
      OneWayDescriptorBase(CvSize patch_size, int pose_count, const char* train_path = 0, const char* pca_config = 0,
                          const char* pca_hr_config = 0, const char* pca_desc_config = 0, int pyr_levels = 1,
                          int pca_dim_high = 100, int pca_dim_low = 100);
  
      OneWayDescriptorBase(CvSize patch_size, int pose_count, const String &pca_filename, const String &train_path = String(), const String &images_list = String(),
                          float _scale_min = 0.7f, float _scale_max=1.5f, float _scale_step=1.2f, int pyr_levels = 1,
                          int pca_dim_high = 100, int pca_dim_low = 100);
  
  
      virtual ~OneWayDescriptorBase();
      void clear ();
  
  
      // Allocate: allocates memory for a given number of descriptors
      void Allocate(int train_feature_count);
  
      // AllocatePCADescriptors: allocates memory for pca descriptors
      void AllocatePCADescriptors();
  
      // returns patch size
      CvSize GetPatchSize() const {return m_patch_size;};
      // returns the number of poses for each descriptor
      int GetPoseCount() const {return m_pose_count;};
  
      // returns the number of pyramid levels
      int GetPyrLevels() const {return m_pyr_levels;};
  
      // returns the number of descriptors
      int GetDescriptorCount() const {return m_train_feature_count;};
  
      // CreateDescriptorsFromImage: creates descriptors for each of the input features
      // - src: input image
      // - features: input features
      // - pyr_levels: the number of pyramid levels
      void CreateDescriptorsFromImage(IplImage* src, const vector<KeyPoint>& features);
  
      // CreatePCADescriptors: generates descriptors for PCA components, needed for fast generation of feature descriptors
      void CreatePCADescriptors();
  
      // returns a feature descriptor by feature index
      const OneWayDescriptor* GetDescriptor(int desc_idx) const {return &m_descriptors[desc_idx];};
  
      // FindDescriptor: finds the closest descriptor
      // - patch: input image patch
      // - desc_idx: output index of the closest descriptor to the input patch
      // - pose_idx: output index of the closest pose of the closest descriptor to the input patch
      // - distance: distance from the input patch to the closest feature pose
      // - _scales: scales of the input patch for each descriptor
      // - scale_ranges: input scales variation (float[2])
      void FindDescriptor(IplImage* patch, int& desc_idx, int& pose_idx, float& distance, float* _scale = 0, float* scale_ranges = 0) const;
  
      // - patch: input image patch
      // - n: number of the closest indexes
      // - desc_idxs: output indexes of the closest descriptor to the input patch (n)
      // - pose_idx: output indexes of the closest pose of the closest descriptor to the input patch (n)
      // - distances: distance from the input patch to the closest feature pose (n)
      // - _scales: scales of the input patch
      // - scale_ranges: input scales variation (float[2])
      void FindDescriptor(IplImage* patch, int n, vector<int>& desc_idxs, vector<int>& pose_idxs,
                          vector<float>& distances, vector<float>& _scales, float* scale_ranges = 0) const;
  
      // FindDescriptor: finds the closest descriptor
      // - src: input image
      // - pt: center of the feature
      // - desc_idx: output index of the closest descriptor to the input patch
      // - pose_idx: output index of the closest pose of the closest descriptor to the input patch
      // - distance: distance from the input patch to the closest feature pose
      void FindDescriptor(IplImage* src, cv::Point2f pt, int& desc_idx, int& pose_idx, float& distance) const;
  
      // InitializePoses: generates random poses
      void InitializePoses();
  
      // InitializeTransformsFromPoses: generates 2x3 affine matrices from poses (initializes m_transforms)
      void InitializeTransformsFromPoses();
  
      // InitializePoseTransforms: subsequently calls InitializePoses and InitializeTransformsFromPoses
      void InitializePoseTransforms();
  
      // InitializeDescriptor: initializes a descriptor
      // - desc_idx: descriptor index
      // - train_image: image patch (ROI is supported)
      // - feature_label: feature textual label
      void InitializeDescriptor(int desc_idx, IplImage* train_image, const char* feature_label);
  
      void InitializeDescriptor(int desc_idx, IplImage* train_image, const KeyPoint& keypoint, const char* feature_label);
  
      // InitializeDescriptors: load features from an image and create descriptors for each of them
      void InitializeDescriptors(IplImage* train_image, const vector<KeyPoint>& features,
                                const char* feature_label = "", int desc_start_idx = 0);
  
      // Write: writes this object to a file storage
      // - fs: output filestorage
      void Write (FileStorage &fs) const;
  
      // Read: reads OneWayDescriptorBase object from a file node
      // - fn: input file node
      void Read (const FileNode &fn);
  
      // LoadPCADescriptors: loads PCA descriptors from a file
      // - filename: input filename
      int LoadPCADescriptors(const char* filename);
  
      // LoadPCADescriptors: loads PCA descriptors from a file node
      // - fn: input file node
      int LoadPCADescriptors(const FileNode &fn);
  
      // SavePCADescriptors: saves PCA descriptors to a file
      // - filename: output filename
      void SavePCADescriptors(const char* filename);
  
      // SavePCADescriptors: saves PCA descriptors to a file storage
      // - fs: output file storage
      void SavePCADescriptors(CvFileStorage* fs) const;
  
      // GeneratePCA: calculate and save PCA components and descriptors
      // - img_path: path to training PCA images directory
      // - images_list: filename with filenames of training PCA images
      void GeneratePCA(const char* img_path, const char* images_list, int pose_count=500);
  
      // SetPCAHigh: sets the high resolution pca matrices (copied to internal structures)
      void SetPCAHigh(CvMat* avg, CvMat* eigenvectors);
  
      // SetPCALow: sets the low resolution pca matrices (copied to internal structures)
      void SetPCALow(CvMat* avg, CvMat* eigenvectors);
  
      int GetLowPCA(CvMat** avg, CvMat** eigenvectors)
      {
          *avg = m_pca_avg;
          *eigenvectors = m_pca_eigenvectors;
          return m_pca_dim_low;
      };
  
      int GetPCADimLow() const {return m_pca_dim_low;};
      int GetPCADimHigh() const {return m_pca_dim_high;};
  
      void ConvertDescriptorsArrayToTree(); // Converting pca_descriptors array to KD tree
  
      // GetPCAFilename: get default PCA filename
      static String GetPCAFilename () { return "pca.yml"; }
  
      virtual bool empty() const { return m_train_feature_count <= 0 ? true : false; }
  
  protected:
      ...
  };
  ```

cv::OneWayDescriptorMatcher: |
  Wrapping class for computing, matching, and classifying descriptors
  using the :ocvOneWayDescriptorBase class. :
  
  ```c++
  class OneWayDescriptorMatcher : public GenericDescriptorMatcher
  {
  public:
      class Params
      {
      public:
          static const int POSE_COUNT = 500;
          static const int PATCH_WIDTH = 24;
          static const int PATCH_HEIGHT = 24;
          static float GET_MIN_SCALE() { return 0.7f; }
          static float GET_MAX_SCALE() { return 1.5f; }
          static float GET_STEP_SCALE() { return 1.2f; }
  
          Params( int poseCount = POSE_COUNT,
                  Size patchSize = Size(PATCH_WIDTH, PATCH_HEIGHT),
                  String pcaFilename = String(),
                  String trainPath = String(), String trainImagesList = String(),
                  float minScale = GET_MIN_SCALE(), float maxScale = GET_MAX_SCALE(),
                  float stepScale = GET_STEP_SCALE() );
  
          int poseCount;
          Size patchSize;
          String pcaFilename;
          String trainPath;
          String trainImagesList;
  
          float minScale, maxScale, stepScale;
      };
  
      OneWayDescriptorMatcher( const Params& params=Params() );
      virtual ~OneWayDescriptorMatcher();
  
      void initialize( const Params& params, const Ptr<OneWayDescriptorBase>& base=Ptr<OneWayDescriptorBase>() );
  
      // Clears keypoints stored in collection and OneWayDescriptorBase
      virtual void clear();
  
      virtual void train();
  
      virtual bool isMaskSupported();
  
      virtual void read( const FileNode &fn );
      virtual void write( FileStorage& fs ) const;
  
      virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const;
  protected:
      ...
  };
  ```

cv::FernClassifier: |
  ```c++
  class CV_EXPORTS FernClassifier
  {
  public:
      FernClassifier();
      FernClassifier(const FileNode& node);
      FernClassifier(const vector<vector<Point2f> >& points,
                    const vector<Mat>& refimgs,
                    const vector<vector<int> >& labels=vector<vector<int> >(),
                    int _nclasses=0, int _patchSize=PATCH_SIZE,
                    int _signatureSize=DEFAULT_SIGNATURE_SIZE,
                    int _nstructs=DEFAULT_STRUCTS,
                    int _structSize=DEFAULT_STRUCT_SIZE,
                    int _nviews=DEFAULT_VIEWS,
                    int _compressionMethod=COMPRESSION_NONE,
                    const PatchGenerator& patchGenerator=PatchGenerator());
      virtual ~FernClassifier();
      virtual void read(const FileNode& n);
      virtual void write(FileStorage& fs, const String& name=String()) const;
      virtual void trainFromSingleView(const Mat& image,
                                      const vector<KeyPoint>& keypoints,
                                      int _patchSize=PATCH_SIZE,
                                      int _signatureSize=DEFAULT_SIGNATURE_SIZE,
                                      int _nstructs=DEFAULT_STRUCTS,
                                      int _structSize=DEFAULT_STRUCT_SIZE,
                                      int _nviews=DEFAULT_VIEWS,
                                      int _compressionMethod=COMPRESSION_NONE,
                                      const PatchGenerator& patchGenerator=PatchGenerator());
      virtual void train(const vector<vector<Point2f> >& points,
                        const vector<Mat>& refimgs,
                        const vector<vector<int> >& labels=vector<vector<int> >(),
                        int _nclasses=0, int _patchSize=PATCH_SIZE,
                        int _signatureSize=DEFAULT_SIGNATURE_SIZE,
                        int _nstructs=DEFAULT_STRUCTS,
                        int _structSize=DEFAULT_STRUCT_SIZE,
                        int _nviews=DEFAULT_VIEWS,
                        int _compressionMethod=COMPRESSION_NONE,
                        const PatchGenerator& patchGenerator=PatchGenerator());
      virtual int operator()(const Mat& img, Point2f kpt, vector<float>& signature) const;
      virtual int operator()(const Mat& patch, vector<float>& signature) const;
      virtual void clear();
      virtual bool empty() const;
      void setVerbose(bool verbose);
  
      int getClassCount() const;
      int getStructCount() const;
      int getStructSize() const;
      int getSignatureSize() const;
      int getCompressionMethod() const;
      Size getPatchSize() const;
  
      struct Feature
      {
          uchar x1, y1, x2, y2;
          Feature() : x1(0), y1(0), x2(0), y2(0) {}
          Feature(int _x1, int _y1, int _x2, int _y2)
          : x1((uchar)_x1), y1((uchar)_y1), x2((uchar)_x2), y2((uchar)_y2)
          {}
          template<typename _Tp> bool operator ()(const Mat_<_Tp>& patch) const
          { return patch(y1,x1) > patch(y2, x2); }
      };
  
      enum
      {
          PATCH_SIZE = 31,
          DEFAULT_STRUCTS = 50,
          DEFAULT_STRUCT_SIZE = 9,
          DEFAULT_VIEWS = 5000,
          DEFAULT_SIGNATURE_SIZE = 176,
          COMPRESSION_NONE = 0,
          COMPRESSION_RANDOM_PROJ = 1,
          COMPRESSION_PCA = 2,
          DEFAULT_COMPRESSION_METHOD = COMPRESSION_NONE
      };
  
  protected:
      ...
  };
  ```

cv::FernDescriptorMatcher: |
  Wrapping class for computing, matching, and classifying descriptors
  using the :ocvFernClassifier class. :
  
  ```c++
  class FernDescriptorMatcher : public GenericDescriptorMatcher
  {
  public:
      class Params
      {
      public:
          Params( int nclasses=0,
                  int patchSize=FernClassifier::PATCH_SIZE,
                  int signatureSize=FernClassifier::DEFAULT_SIGNATURE_SIZE,
                  int nstructs=FernClassifier::DEFAULT_STRUCTS,
                  int structSize=FernClassifier::DEFAULT_STRUCT_SIZE,
                  int nviews=FernClassifier::DEFAULT_VIEWS,
                  int compressionMethod=FernClassifier::COMPRESSION_NONE,
                  const PatchGenerator& patchGenerator=PatchGenerator() );
  
          Params( const String& filename );
  
          int nclasses;
          int patchSize;
          int signatureSize;
          int nstructs;
          int structSize;
          int nviews;
          int compressionMethod;
          PatchGenerator patchGenerator;
  
          String filename;
      };
  
      FernDescriptorMatcher( const Params& params=Params() );
      virtual ~FernDescriptorMatcher();
  
      virtual void clear();
  
      virtual void train();
  
      virtual bool isMaskSupported();
  
      virtual void read( const FileNode &fn );
      virtual void write( FileStorage& fs ) const;
  
      virtual Ptr<GenericDescriptorMatcher> clone( bool emptyTrainData=false ) const;
  
  protected:
          ...
  };
  ```

cv::contrib. Contributed/Experimental Stuff: |-
  The module contains some recently added functionality that has not been
  stabilized, or functionality that is considered optional.
cv::gpu::merge: Makes a multi-channel matrix out of several single-channel matrices.
cv::gpu::split: Copies each plane of a multi-channel matrix into an array.
cv::gpu::transpose: Transposes a matrix.
cv::gpu::flip: Flips a 2D matrix around vertical, horizontal, or both axes.
cv::gpu::LookUpTable: |
  Base class for transform using lookup table. :
  
  ```c++
  class CV_EXPORTS LookUpTable : public Algorithm
  {
  public:
      virtual void transform(InputArray src, OutputArray dst, Stream& stream = Stream::Null()) = 0;
  };
  ```

cv::gpu::LookUpTable::transform: |-
  Transforms the source matrix into the destination matrix using the given
  look-up table: `dst(I) = lut(src(I))` .
cv::gpu::createLookUpTable: Creates implementation for :ocvgpu::LookUpTable .
cv::gpu::copyMakeBorder: Forms a border around an image.
cv::Data Structures: |-
  OpenCV C++ 1-D or 2-D dense array class :
  
  ```c++
  class CV_EXPORTS oclMat
  {
  public:
      //! default constructor
      oclMat();
      //! constructs oclMatrix of the specified size and type (_type is CV_8UC1, CV_64FC3, CV_32SC(12) etc.)
      oclMat(int rows, int cols, int type);
      oclMat(Size size, int type);
      //! constucts oclMatrix and fills it with the specified value _s.
      oclMat(int rows, int cols, int type, const Scalar &s);
      oclMat(Size size, int type, const Scalar &s);
      //! copy constructor
      oclMat(const oclMat &m);
  
      //! constructor for oclMatrix headers pointing to user-allocated data
      oclMat(int rows, int cols, int type, void *data, size_t step = Mat::AUTO_STEP);
      oclMat(Size size, int type, void *data, size_t step = Mat::AUTO_STEP);
  
      //! creates a matrix header for a part of the bigger matrix
      oclMat(const oclMat &m, const Range &rowRange, const Range &colRange);
      oclMat(const oclMat &m, const Rect &roi);
  
      //! builds oclMat from Mat. Perfom blocking upload to device.
      explicit oclMat (const Mat &m);
  
      //! destructor - calls release()
      ~oclMat();
  
      //! assignment operators
      oclMat &operator = (const oclMat &m);
      //! assignment operator. Perfom blocking upload to device.
      oclMat &operator = (const Mat &m);
  
  
      //! pefroms blocking upload data to oclMat.
      void upload(const cv::Mat &m);
  
  
      //! downloads data from device to host memory. Blocking calls.
      operator Mat() const;
      void download(cv::Mat &m) const;
  
  
      //! returns a new oclMatrix header for the specified row
      oclMat row(int y) const;
      //! returns a new oclMatrix header for the specified column
      oclMat col(int x) const;
      //! ... for the specified row span
      oclMat rowRange(int startrow, int endrow) const;
      oclMat rowRange(const Range &r) const;
      //! ... for the specified column span
      oclMat colRange(int startcol, int endcol) const;
      oclMat colRange(const Range &r) const;
  
      //! returns deep copy of the oclMatrix, i.e. the data is copied
      oclMat clone() const;
      //! copies the oclMatrix content to "m".
      // It calls m.create(this->size(), this->type()).
      // It supports any data type
      void copyTo( oclMat &m ) const;
      //! copies those oclMatrix elements to "m" that are marked with non-zero mask elements.
      //It supports 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4
      void copyTo( oclMat &m, const oclMat &mask ) const;
      //! converts oclMatrix to another datatype with optional scalng. See cvConvertScale.
      //It supports 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4
      void convertTo( oclMat &m, int rtype, double alpha = 1, double beta = 0 ) const;
  
      void assignTo( oclMat &m, int type = -1 ) const;
  
      //! sets every oclMatrix element to s
      //It supports 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4
      oclMat &operator = (const Scalar &s);
      //! sets some of the oclMatrix elements to s, according to the mask
      //It supports 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4
      oclMat &setTo(const Scalar &s, const oclMat &mask = oclMat());
      //! creates alternative oclMatrix header for the same data, with different
      // number of channels and/or different number of rows. see cvReshape.
      oclMat reshape(int cn, int rows = 0) const;
  
      //! allocates new oclMatrix data unless the oclMatrix already has specified size and type.
      // previous data is unreferenced if needed.
      void create(int rows, int cols, int type);
      void create(Size size, int type);
      //! decreases reference counter;
      // deallocate the data when reference counter reaches 0.
      void release();
  
      //! swaps with other smart pointer
      void swap(oclMat &mat);
  
      //! locates oclMatrix header within a parent oclMatrix. See below
      void locateROI( Size &wholeSize, Point &ofs ) const;
      //! moves/resizes the current oclMatrix ROI inside the parent oclMatrix.
      oclMat &adjustROI( int dtop, int dbottom, int dleft, int dright );
      //! extracts a rectangular sub-oclMatrix
      // (this is a generalized form of row, rowRange etc.)
      oclMat operator()( Range rowRange, Range colRange ) const;
      oclMat operator()( const Rect &roi ) const;
  
      //! returns true if the oclMatrix data is continuous
      // (i.e. when there are no gaps between successive rows).
      // similar to CV_IS_oclMat_CONT(cvoclMat->type)
      bool isContinuous() const;
      //! returns element size in bytes,
      // similar to CV_ELEM_SIZE(cvMat->type)
      size_t elemSize() const;
      //! returns the size of element channel in bytes.
      size_t elemSize1() const;
      //! returns element type, similar to CV_MAT_TYPE(cvMat->type)
      int type() const;
      //! returns element type, i.e. 8UC3 returns 8UC4 because in ocl
      //! 3 channels element actually use 4 channel space
      int ocltype() const;
      //! returns element type, similar to CV_MAT_DEPTH(cvMat->type)
      int depth() const;
      //! returns element type, similar to CV_MAT_CN(cvMat->type)
      int channels() const;
      //! returns element type, return 4 for 3 channels element,
      //!becuase 3 channels element actually use 4 channel space
      int oclchannels() const;
      //! returns step/elemSize1()
      size_t step1() const;
      //! returns oclMatrix size:
      // width == number of columns, height == number of rows
      Size size() const;
      //! returns true if oclMatrix data is NULL
      bool empty() const;
  
      //! returns pointer to y-th row
      uchar *ptr(int y = 0);
      const uchar *ptr(int y = 0) const;
  
      //! template version of the above method
      template<typename _Tp> _Tp *ptr(int y = 0);
      template<typename _Tp> const _Tp *ptr(int y = 0) const;
  
      //! matrix transposition
      oclMat t() const;
  
      /*! includes several bit-fields:
        - the magic signature
        - continuity flag
        - depth
        - number of channels
        */
      int flags;
      //! the number of rows and columns
      int rows, cols;
      //! a distance between successive rows in bytes; includes the gap if any
      size_t step;
      //! pointer to the data(OCL memory object)
      uchar *data;
  
      //! pointer to the reference counter;
      // when oclMatrix points to user-allocated data, the pointer is NULL
      int *refcount;
  
      //! helper fields used in locateROI and adjustROI
      //datastart and dataend are not used in current version
      uchar *datastart;
      uchar *dataend;
  
      //! OpenCL context associated with the oclMat object.
      Context *clCxt;
      //add offset for handle ROI, calculated in byte
      int offset;
      //add wholerows and wholecols for the whole matrix, datastart and dataend are no longer used
      int wholerows;
      int wholecols;
  };
  ```
  
  
  Basically speaking, the oclMat is the mirror of Mat with the extension
  of ocl feature, the members have the same meaning and useage of Mat
  except following:
  
  datastart and dataend are replaced with wholerows and wholecols
  
  add clCxt for oclMat
  
  Only basic flags are supported in oclMat(i.e. depth number of channels)
  
  All the 3-channel matrix(i.e. RGB image) are represented by 4-channel
  matrix in oclMat. It means 3-channel image have 4-channel space with the
  last channel unused. We provide a transparent interface to handle the
  difference between OpenCV Mat and oclMat.
  
  For example: If a oclMat has 3 channels, channels() returns 3 and
  oclchannels() returns 4 Decision Trees ==============
  
  The ML classes discussed in this section implement Classification and
  Regression Tree algorithms described in [Breiman84]_.
  
  The class :ocvCvDTree represents a single decision tree that may be used
  alone or as a base class in tree ensembles (see Boosting and
  Random Trees ).
  
  A decision tree is a binary tree (tree where each non-leaf node has two
  child nodes). It can be used either for classification or for
  regression. For classification, each tree leaf is marked with a class
  label; multiple leaves may have the same label. For regression, a
  constant is also assigned to each tree leaf, so the approximation
  function is piecewise constant.
cv::Predicting with Decision Trees: |-
  To reach a leaf node and to obtain a response for the input feature
  vector, the prediction procedure starts with the root node. From each
  non-leaf node the procedure goes to the left (selects the left child
  node as the next observed node) or to the right based on the value of a
  certain variable whose index is stored in the observed node. The
  following variables are possible:
  
  *
  :   **Ordered variables.** The variable value is compared with a
      threshold that is also stored in the node. If the value is less than
      the threshold, the procedure goes to the left. Otherwise, it goes to
      the right. For example, if the weight is less than 1 kilogram, the
      procedure goes to the left, else to the right.
  
  *
  :   **Categorical variables.** A discrete variable value is tested to
      see whether it belongs to a certain subset of values (also stored in
      the node) from a limited set of values the variable could take. If
      it does, the procedure goes to the left. Otherwise, it goes to the
      right. For example, if the color is green or red, go to the left,
      else to the right.
  
  So, in each node, a pair of entities (`variable_index` ,
  `decision_rule (threshold/subset)` ) is used. This pair is called a
  *split* (split on the variable `variable_index` ). Once a leaf node is
  reached, the value assigned to this node is used as the output of the
  prediction procedure.
  
  Sometimes, certain features of the input vector are missed (for example,
  in the darkness it is difficult to determine the object color), and the
  prediction procedure may get stuck in the certain node (in the mentioned
  example, if the node is split by color). To avoid such situations,
  decision trees use so-called *surrogate splits*. That is, in addition to
  the best "primary" split, every tree node may also be split to one or
  more other variables with nearly the same results.
cv::Training Decision Trees: |-
  The tree is built recursively, starting from the root node. All training
  data (feature vectors and responses) is used to split the root node. In
  each node the optimum decision rule (the best "primary" split) is found
  based on some criteria. In machine learning, `gini` "purity" criteria
  are used for classification, and sum of squared errors is used for
  regression. Then, if necessary, the surrogate splits are found. They
  resemble the results of the primary split on the training data. All the
  data is divided using the primary and the surrogate splits (like it is
  done in the prediction procedure) between the left and the right child
  node. Then, the procedure recursively splits both left and right nodes.
  At each node the recursive procedure may stop (that is, stop splitting
  the node further) in one of the following cases:
  
    Depth of the constructed tree branch has reached the specified
  maximum value.
  
  
    Number of training samples in the node is less than the specified
  threshold when it is not statistically representative to split the
  node further.
  
  
    All the samples in the node belong to the same class or, in case of
  regression, the variation is too small.
  
  
    The best found split does not give any noticeable improvement
  compared to a random choice.
  
  
  When the tree is built, it may be pruned using a cross-validation
  procedure, if necessary. That is, some branches of the tree that may
  lead to the model overfitting are cut off. Normally, this procedure is
  only applied to standalone decision trees. Usually tree ensembles build
  trees that are small enough and use their own protection schemes against
  overfitting.
cv::Variable Importance: |-
  Besides the prediction that is an obvious use of decision trees, the
  tree can be also used for various data analyses. One of the key
  properties of the constructed decision tree algorithms is an ability to
  compute the importance (relative decisive power) of each variable. For
  example, in a spam filter that uses a set of words occurred in the
  message as a feature vector, the variable importance rating can be used
  to determine the most "spam-indicating" words and thus help keep the
  dictionary size reasonable.
  
  Importance of each variable is computed over all the splits on this
  variable in the tree, primary and surrogate ones. Thus, to compute
  variable importance correctly, the surrogate splits must be enabled in
  the training parameters, even if there is no missing data.
cv::CvDTreeNode: |-
  Other numerous fields of `CvDTreeNode` are used internally at the
  training stage.
cv::CvDTreeParams: |-
  The structure contains all the decision tree training parameters. You
  can initialize it by default constructor and then override any
  parameters directly before training, or the structure may be fully
  initialized using the advanced variant of the constructor.
cv::CvDTreeParams::CvDTreeParams: |
  The constructors.
  
  The default constructor initializes all the parameters with the default
  values tuned for the standalone classification tree:
  
  ```c++
  CvDTreeParams() : max_categories(10), max_depth(INT_MAX), min_sample_count(10),
      cv_folds(10), use_surrogates(true), use_1se_rule(true),
      truncate_pruned_tree(true), regression_accuracy(0.01f), priors(0)
  {}
  ```

cv::CvDTreeTrainData: |-
  Decision tree training data and shared data for tree ensembles. The
  structure is mostly used internally for storing both standalone trees
  and tree ensembles efficiently. Basically, it contains the following
  types of information:
  
   Training parameters, an instance of :ocvCvDTreeParams.
  
  
   Training data preprocessed to find the best splits more efficiently.
  For tree ensembles, this preprocessed data is reused by all trees.
  Additionally, the training data characteristics shared by all trees
  in the ensemble are stored here: variable types, the number of
  classes, a class label compression map, and so on.
  
  
   Buffers, memory storages for tree nodes, splits, and other elements
  of the constructed trees.
  
  
  There are two ways of using this structure. In simple cases (for
  example, a standalone tree or the ready-to-use "black box" tree ensemble
  from machine learning, like Random Trees or Boosting ), there is no need
  to care or even to know about the structure. You just construct the
  needed statistical model, train it, and use it. The `CvDTreeTrainData`
  structure is constructed and used internally. However, for custom tree
  algorithms or another sophisticated cases, the structure may be
  constructed and used explicitly. The scheme is the following:
  
  #.
  :   The structure is initialized using the default constructor, followed
      by `set_data`, or it is built using the full form of constructor.
      The parameter `_shared` must be set to `true`.
  
  #.
  :   One or more trees are trained using this data (see the special form
      of the method :ocvCvDTree::train).
  
  #.
  :   The structure is released as soon as all the trees using it are
      released.
cv::CvDTree: |-
  The class implements a decision tree as described in the beginning of
  this section.
cv::CvDTree::train: |-
  Trains a decision tree.
  
  There are four `train` methods in :ocvCvDTree:
  
    The **first two** methods follow the generic :ocvCvStatModel::train
  conventions. It is the most complete form. Both data layouts
  (`tflag=CV_ROW_SAMPLE` and `tflag=CV_COL_SAMPLE`) are supported, as
  well as sample and variable subsets, missing measurements, arbitrary
  combinations of input and output variable types, and so on. The last
  parameter contains all of the necessary training parameters (see the
  :ocvCvDTreeParams description).
  
  
    The **third** method uses :ocvCvMLData to pass training data to a
  decision tree.
  
  
    The **last** method `train` is mostly used for building tree
  ensembles. It takes the pre-constructed :ocvCvDTreeTrainData
  instance and an optional subset of the training set. The indices in
  `subsampleIdx` are counted relatively to the `_sample_idx` , passed
  to the `CvDTreeTrainData` constructor. For example, if
  `_sample_idx=[1, 5, 7, 100]` , then `subsampleIdx=[0,3]` means that
  the samples `[1, 100]` of the original training set are used.
  
  
  The function is parallelized with the TBB library.
cv::CvDTree::predict: "Returns the leaf node of a decision tree corresponding to the input\n\
  vector.\n\n\
  The method traverses the decision tree and returns the reached leaf node\n\
  as output. The prediction result, either the class label or the\n\
  estimated function value, may be retrieved as the `value` field of the\n\
  :ocvCvDTreeNode structure, for example:\n\
  `dtree->predict(sample,mask)->value`."
cv::CvDTree::calc_error: |-
  Returns error of the decision tree.
  
  The method calculates error of the decision tree. In case of
  classification it is the percentage of incorrectly classified samples
  and in case of regression it is the mean of squared errors on samples.
cv::CvDTree::getVarImportance: Returns the variable importance array.
cv::CvDTree::get_root: Returns the root of the decision tree.
cv::CvDTree::get_pruned_tree_idx: |-
  Returns the `CvDTree::pruned_tree_idx` parameter.
  
  The parameter `DTree::pruned_tree_idx` is used to prune a decision tree.
  See the `CvDTreeNode::Tn` parameter.
cv::CvDTree::get_data: |-
  Returns used train data of the decision tree.
  
  Example: building a tree for classifying mushrooms. See the
  `mushroom.cpp` sample that demonstrates how to build and use the
  decision tree.
cv::fastNlMeansDenoising: |-
  Perform image denoising using Non-local Means Denoising algorithm
  <http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/> with
  several computational optimizations. Noise expected to be a gaussian
  white noise
  
  This function expected to be applied to grayscale images. For colored
  images look at `fastNlMeansDenoisingColored`. Advanced usage of this
  functions can be manual denoising of colored image in different
  colorspaces. Such approach is used in `fastNlMeansDenoisingColored` by
  converting image to CIELAB colorspace and then separately denoise L and
  AB components with different h parameter.
cv::fastNlMeansDenoisingColored: |-
  Modification of `fastNlMeansDenoising` function for colored images
  
  The function converts image to CIELAB colorspace and then separately
  denoise L and AB components with given h parameters using
  `fastNlMeansDenoising` function.
cv::fastNlMeansDenoisingMulti: |-
  Modification of `fastNlMeansDenoising` function for images sequence
  where consequtive images have been captured in small period of time. For
  example video. This version of the function is for grayscale images or
  for manual manipulation with colorspaces. For more details see
  <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394>
cv::fastNlMeansDenoisingColoredMulti: |-
  Modification of `fastNlMeansDenoisingMulti` function for colored images
  sequences
  
  The function converts images to CIELAB colorspace and then separately
  denoise L and AB components with given h parameters using
  `fastNlMeansDenoisingMulti` function.
cv::gpu::nonLocalMeans: |-
  Performs pure non local means denoising without any simplification, and
  thus it is not fast.
cv::gpu::FastNonLocalMeansDenoising: |-
  The class implements fast approximate Non Local Means Denoising
  algorithm.
cv::gpu::FastNonLocalMeansDenoising::simpleMethod(): |-
  Perform image denoising using Non-local Means Denoising algorithm
  <http://www.ipol.im/pub/algo/bcm_non_local_means_denoising> with several
  computational optimizations. Noise expected to be a gaussian white noise
  
  This function expected to be applied to grayscale images. For colored
  images look at `FastNonLocalMeansDenoising::labMethod`.
cv::gpu::FastNonLocalMeansDenoising::labMethod(): |-
  Modification of `FastNonLocalMeansDenoising::simpleMethod` for color
  images
  
  The function converts image to CIELAB colorspace and then separately
  denoise L and AB components with given h parameters using
  `FastNonLocalMeansDenoising::simpleMethod` function.
cv::drawMatches: |-
  Draws the found matches of keypoints from two images.
  
  This function draws matches of keypoints from two images in the output
  image. Match is a line connecting two keypoints (circles). The structure
  `DrawMatchesFlags` is defined as follows:
  
  ~~~~ {.sourceCode .cpp}
  struct DrawMatchesFlags
  {
      enum
      {
          DEFAULT = 0, // Output image matrix will be created (Mat::create),
                       // i.e. existing memory of output image may be reused.
                       // Two source images, matches, and single keypoints
                       // will be drawn.
                       // For each keypoint, only the center point will be
                       // drawn (without a circle around the keypoint with the
                       // keypoint size and orientation).
          DRAW_OVER_OUTIMG = 1, // Output image matrix will not be
                         // created (using Mat::create). Matches will be drawn
                         // on existing content of output image.
          NOT_DRAW_SINGLE_POINTS = 2, // Single keypoints will not be drawn.
          DRAW_RICH_KEYPOINTS = 4 // For each keypoint, the circle around
                         // keypoint with keypoint size and orientation will
                         // be drawn.
      };
  };
  ~~~~
cv::drawKeypoints: Draws keypoints.
cv::Drawing Functions: |
  Drawing functions work with matrices/images of arbitrary depth. The
  boundaries of the shapes can be rendered with antialiasing (implemented
  only for 8-bit images for now). All the functions include the parameter
  `color` that uses an RGB value (that may be constructed with the
  `Scalar` constructor ) for color images and brightness for grayscale
  images. For color images, the channel ordering is normally *Blue, Green,
  Red*. This is what :ocvimshow, :ocvimread, and :ocvimwrite expect. So,
  if you form a color using the `Scalar` constructor, it should look like:
  
  $$\texttt{Scalar} (blue _ component, green _ component, red _ component[, alpha _ component])$$
  
  If you are using your own image rendering and I/O functions, you can use
  any channel ordering. The drawing functions process each channel
  independently and do not depend on the channel order or even on the used
  color space. The whole image can be converted from BGR to RGB or to a
  different color space using :ocvcvtColor .
  
  If a drawn figure is partially or completely outside the image, the
  drawing functions clip it. Also, many drawing functions can handle pixel
  coordinates specified with sub-pixel accuracy. This means that the
  coordinates can be passed as fixed-point numbers encoded as integers.
  The number of fractional bits is specified by the `shift` parameter and
  the real point coordinates are calculated as
  $\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})$
  . This feature is especially effective when rendering antialiased
  shapes.
  
  **note**
  
  The functions do not support alpha-transparency when the target image
  is 4-channel. In this case, the `color[3]` is simply copied to the
  repainted pixels. Thus, if you want to paint semi-transparent shapes,
  you can paint them in a separate buffer and then blend it with the
  main image.

cv::circle: |-
  Draws a circle.
  
  The function `circle` draws a simple or filled circle with a given
  center and radius.
cv::clipLine: |-
  Clips the line against the image rectangle.
  
  The functions `clipLine` calculate a part of the line segment that is
  entirely within the specified rectangle. They return `false` if the line
  segment is completely outside the rectangle. Otherwise, they return
  `true` .
cv::ellipse: |-
  Draws a simple or thick elliptic arc or fills an ellipse sector.
  
  The functions `ellipse` with less parameters draw an ellipse outline, a
  filled ellipse, an elliptic arc, or a filled ellipse sector. A
  piecewise-linear curve is used to approximate the elliptic arc boundary.
  If you need more control of the ellipse rendering, you can retrieve the
  curve using :ocvellipse2Poly and then render it with :ocvpolylines or
  fill it with :ocvfillPoly . If you use the first variant of the function
  and want to draw the whole ellipse, not an arc, pass `startAngle=0` and
  `endAngle=360` . The figure below explains the meaning of the
  parameters.
  
  **Figure 1. Parameters of Elliptic Arc**
  
  ![image](pics/ellipse.png)
cv::ellipse2Poly: |-
  Approximates an elliptic arc with a polyline.
  
  The function `ellipse2Poly` computes the vertices of a polyline that
  approximates the specified elliptic arc. It is used by :ocvellipse .
cv::fillConvexPoly: |-
  Fills a convex polygon.
  
  The function `fillConvexPoly` draws a filled convex polygon. This
  function is much faster than the function `fillPoly` . It can fill not
  only convex polygons but any monotonic polygon without
  self-intersections, that is, a polygon whose contour intersects every
  horizontal line (scan line) twice at the most (though, its top-most
  and/or the bottom edge could be horizontal).
cv::fillPoly: |-
  Fills the area bounded by one or more polygons.
  
  The function `fillPoly` fills an area bounded by several polygonal
  contours. The function can fill complex areas, for example, areas with
  holes, contours with self-intersections (some of their parts), and so
  forth.
cv::getTextSize: |
  Calculates the width and height of a text string.
  
  The function `getTextSize` calculates and returns the size of a box that
  contains the specified text. That is, the following code renders some
  text, the tight box surrounding it, and the baseline: :
  
  ```c++
  String text = "Funny text inside the box";
  int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;
  double fontScale = 2;
  int thickness = 3;
  
  Mat img(600, 800, CV_8UC3, Scalar::all(0));
  
  int baseline=0;
  Size textSize = getTextSize(text, fontFace,
                              fontScale, thickness, &baseline);
  baseline += thickness;
  
  // center the text
  Point textOrg((img.cols - textSize.width)/2,
                (img.rows + textSize.height)/2);
  
  // draw the box
  rectangle(img, textOrg + Point(0, baseline),
            textOrg + Point(textSize.width, -textSize.height),
            Scalar(0,0,255));
  // ... and the baseline first
  line(img, textOrg + Point(0, thickness),
       textOrg + Point(textSize.width, thickness),
       Scalar(0, 0, 255));
  
  // then put the text itself
  putText(img, text, textOrg, fontFace, fontScale,
          Scalar::all(255), thickness, 8);
  ```

cv::InitFont: |-
  Initializes font structure (OpenCV 1.x API).
  
  The function initializes the font structure that can be passed to text
  rendering functions.
cv::line: |-
  Draws a line segment connecting two points.
  
  The function `line` draws the line segment between `pt1` and `pt2`
  points in the image. The line is clipped by the image boundaries. For
  non-antialiased lines with integer coordinates, the 8-connected or
  4-connected Bresenham algorithm is used. Thick lines are drawn with
  rounding endings. Antialiased lines are drawn using Gaussian filtering.
cv::LineIterator: |
  Class for iterating pixels on a raster line. :
  
  ```c++
  class LineIterator
  {
  public:
      // creates iterators for the line connecting pt1 and pt2
      // the line will be clipped on the image boundaries
      // the line is 8-connected or 4-connected
      // If leftToRight=true, then the iteration is always done
      // from the left-most point to the right most,
      // not to depend on the ordering of pt1 and pt2 parameters
      LineIterator(const Mat& img, Point pt1, Point pt2,
                   int connectivity=8, bool leftToRight=false);
      // returns pointer to the current line pixel
      uchar* operator *();
      // move the iterator to the next pixel
      LineIterator& operator ++();
      LineIterator operator ++(int);
      Point pos() const;
  
      // internal state of the iterator
      uchar* ptr;
      int err, count;
      int minusDelta, plusDelta;
      int minusStep, plusStep;
  };
  ```
  
  
  The class `LineIterator` is used to get each pixel of a raster line. It
  can be treated as versatile implementation of the Bresenham algorithm
  where you can stop at each pixel and do some extra processing, for
  example, grab pixel values along the line or draw a line with an effect
  (for example, with XOR operation).
  
  The number of pixels along the line is stored in `LineIterator::count` .
  The method `LineIterator::pos` returns the current position in the image
  :
  
  ```c++
  // grabs pixels along the line (pt1, pt2)
  // from 8-bit 3-channel image to the buffer
  LineIterator it(img, pt1, pt2, 8);
  LineIterator it2 = it;
  vector<Vec3b> buf(it.count);
  
  for(int i = 0; i < it.count; i++, ++it)
      buf[i] = *(const Vec3b)*it;
  
  // alternative way of iterating through the line
  for(int i = 0; i < it2.count; i++, ++it2)
  {
      Vec3b val = img.at<Vec3b>(it2.pos());
      CV_Assert(buf[i] == val);
  }
  ```

cv::rectangle: |-
  Draws a simple, thick, or filled up-right rectangle.
  
  The function `rectangle` draws a rectangle outline or a filled rectangle
  whose two opposite corners are `pt1` and `pt2`, or `r.tl()` and
  `r.br()-Point(1,1)`.
cv::polylines: |-
  Draws several polygonal curves.
  
  The function `polylines` draws one or more polygonal curves.
cv::drawContours: |
  Draws contours outlines or filled contours.
  
  The function draws contour outlines in the image if
  $\texttt{thickness} \ge 0$ or fills the area bounded by the contours if
  $\texttt{thickness}<0$ . The example below shows how to retrieve
  connected components from the binary image and label them: :
  
  ```c++
  #include "opencv2/imgproc.hpp"
  #include "opencv2/highgui.hpp"
  
  using namespace cv;
  using namespace std;
  
  int main( int argc, char** argv )
  {
      Mat src;
      // the first command-line parameter must be a filename of the binary
      // (black-n-white) image
      if( argc != 2 || !(src=imread(argv[1], 0)).data)
          return -1;
  
      Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);
  
      src = src > 1;
      namedWindow( "Source", 1 );
      imshow( "Source", src );
  
      vector<vector<Point> > contours;
      vector<Vec4i> hierarchy;
  
      findContours( src, contours, hierarchy,
          RETR_CCOMP, CHAIN_APPROX_SIMPLE );
  
      // iterate through all the top-level contours,
      // draw each connected component with its own random color
      int idx = 0;
      for( ; idx >= 0; idx = hierarchy[idx][0] )
      {
          Scalar color( rand()&255, rand()&255, rand()&255 );
          drawContours( dst, contours, idx, color, FILLED, 8, hierarchy );
      }
  
      namedWindow( "Components", 1 );
      imshow( "Components", dst );
      waitKey(0);
  }
  ```

cv::putText: |-
  Draws a text string.
  
  The function `putText` renders the specified text string in the image.
  Symbols that cannot be rendered using the specified font are replaced by
  question marks. See :ocvgetTextSize for a text rendering code example.
cv::Dynamic Structures: |-
  The section describes OpenCV 1.x API for creating growable sequences and
  other dynamic data structures allocated in `CvMemStorage`. If you use
  the new C++, Python, Java etc interface, you will unlikely need this
  functionality. Use `std::vector` or other high-level data structures.
cv::CvMemStorage: "Memory storage is a low-level structure used to store dynamically\n\
  growing data structures such as sequences, contours, graphs,\n\
  subdivisions, etc. It is organized as a list of memory blocks of equal\n\
  size -`bottom` field is the beginning of the list of blocks and `top` is\n\
  the currently used block, but not necessarily the last block of the\n\
  list. All blocks between `bottom` and `top`, not including the latter,\n\
  are considered fully occupied; all blocks between `top` and the last\n\
  block, not including `top`, are considered free and `top` itself is\n\
  partly occupied - `free_space` contains the number of free bytes left in\n\
  the end of `top`.\n\n\
  A new memory buffer that may be allocated explicitly by\n\
  :ocvMemStorageAlloc function or implicitly by higher-level functions,\n\
  such as :ocvSeqPush, :ocvGraphAddEdge etc.\n\n\
  The buffer is put in the end of already allocated space in the `top`\n\
  memory block, if there is enough free space. After allocation,\n\
  `free_space` is decreased by the size of the allocated buffer plus some\n\
  padding to keep the proper alignment. When the allocated buffer does not\n\
  fit into the available portion of `top`, the next storage block from the\n\
  list is taken as `top` and `free_space` is reset to the whole block size\n\
  prior to the allocation.\n\n\
  If there are no more free blocks, a new block is allocated (or borrowed\n\
  from the parent, see :ocvCreateChildMemStorage) and added to the end of\n\
  list. Thus, the storage behaves as a stack with `bottom` indicating\n\
  bottom of the stack and the pair (`top`, `free_space`) indicating top of\n\
  the stack. The stack top may be saved via :ocvSaveMemStoragePos,\n\
  restored via :ocvRestoreMemStoragePos, or reset via :ocvClearMemStorage."
cv::CvMemBlock: |-
  The structure :ocvCvMemBlock represents a single block of memory
  storage. The actual data in the memory blocks follows the header.
cv::CvMemStoragePos: "The structure stores the position in the memory storage. It is used by\n\
  :ocvSaveMemStoragePos and :ocvRestoreMemStoragePos."
cv::CvSeq: |-
  The structure `CvSeq` is a base for all of OpenCV dynamic data
  structures. There are two types of sequences - dense and sparse. The
  base type for dense sequences is :ocvCvSeq and such sequences are used
  to represent growable 1d arrays - vectors, stacks, queues, and deques.
  They have no gaps in the middle - if an element is removed from the
  middle or inserted into the middle of the sequence, the elements from
  the closer end are shifted. Sparse sequences have :ocvCvSet as a base
  class and they are discussed later in more detail. They are sequences of
  nodes; each may be either occupied or free as indicated by the node
  flag. Such sequences are used for unordered data structures such as sets
  of elements, graphs, hash tables and so forth.
cv::CvSlice: |-
  There are helper functions to construct the slice and to compute its
  length:
  
  ```c++
  #define CV_WHOLE_SEQ_END_INDEX 0x3fffffff
  #define CV_WHOLE_SEQ  cvSlice(0, CV_WHOLE_SEQ_END_INDEX)
  ```
  
  
  Calculates the sequence slice length.
  
  Some of functions that operate on sequences take a `CvSlice slice`
  parameter that is often set to the whole sequence (CV_WHOLE_SEQ) by
  default. Either of the `start_index` and `end_index` may be negative or
  exceed the sequence length. If they are equal, the slice is considered
  empty (i.e., contains no elements). Because sequences are treated as
  circular structures, the slice may select a few elements in the end of a
  sequence followed by a few elements at the beginning of the sequence.
  For example, `cvSlice(-2, 3)` in the case of a 10-element sequence will
  select a 5-element slice, containing the pre-last (8th), last (9th), the
  very first (0th), second (1th) and third (2nd) elements. The functions
  normalize the slice argument in the following way:
   :ocvSliceLength is called to determine the length of the slice,
  
   `start_index` of the slice is normalized similarly to the argument
  of :ocvGetSeqElem (i.e., negative indices are allowed). The actual
  slice to process starts at the normalized `start_index` and lasts
  :ocvSliceLength elements (again, assuming the sequence is a
  circular structure).
  
  
  
  If a function does not accept a slice argument, but you want to process
  only a part of the sequence, the sub-sequence may be extracted using the
  :ocvSeqSlice function, or stored into a continuous buffer with
  :ocvCvtSeqToArray (optionally, followed by :ocvMakeSeqHeaderForArray).
cv::CvSet: |-
  The structure `CvSet` is a base for OpenCV 1.x sparse data structures.
  It is derived from :ocvCvSeq and includes an additional member
  `free_elems` - a list of free nodes. Every node of the set, whether free
  or not, is an element of the underlying sequence. While there are no
  restrictions on elements of dense sequences, the set (and derived
  structures) elements must start with an integer field and be able to fit
  CvSetElem structure, because these two fields (an integer followed by a
  pointer) are required for the organization of a node set with the list
  of free nodes. If a node is free, the `flags` field is negative (the
  most-significant bit, or MSB, of the field is set), and the `next_free`
  points to the next free node (the first free node is referenced by the
  `free_elems` field of :ocvCvSet). And if a node is occupied, the `flags`
  field is positive and contains the node index that may be retrieved
  using the (`set_elem->flags & CV_SET_ELEM_IDX_MASK`) expressions, the
  rest of the node content is determined by the user. In particular, the
  occupied nodes are not linked as the free nodes are, so the second field
  can be used for such a link as well as for some different purpose. The
  macro `CV_IS_SET_ELEM(set_elem_ptr)` can be used to determined whether
  the specified node is occupied or not.
  
  Initially the set and the free node list are empty. When a new node is
  requested from the set, it is taken from the list of free nodes, which
  is then updated. If the list appears to be empty, a new sequence block
  is allocated and all the nodes within the block are joined in the list
  of free nodes. Thus, the `total` field of the set is the total number of
  nodes both occupied and free. When an occupied node is released, it is
  added to the list of free nodes. The node released last will be occupied
  first.
  
  `CvSet` is used to represent graphs (:ocvCvGraph), sparse
  multi-dimensional arrays (:ocvCvSparseMat), and planar subdivisions
  (:ocvCvSubdiv2D).
cv::CvSetElem: |-
  The structure is represent single element of :ocvCvSet. It consists of
  two fields: element data pointer and flags.
cv::CvGraph: |-
  The structure `CvGraph` is a base for graphs used in OpenCV 1.x. It
  inherits from :ocvCvSet, that is, it is considered as a set of vertices.
  Besides, it contains another set as a member, a set of graph edges.
  Graphs in OpenCV are represented using adjacency lists format.
cv::CvGraphVtx: |-
  The structure represents single vertex in :ocvCvGraph. It consists of
  two filds: pointer to first edge and flags.
cv::CvGraphEdge: |
  The structure represents edge in :ocvCvGraph. Each edge consists of:
    Two pointers to the starting and ending vertices (vtx[0] and vtx[1]
  respectively);
  
    Two pointers to next edges for the starting and ending vertices,
  where next[0] points to the next edge in the vtx[0] adjacency list
  and next[1] points to the next edge in the vtx[1] adjacency list;
  
    Weight;
  
    Flags.

cv::CvGraphScanner: |-
  The structure `CvGraphScanner` is used for depth-first graph traversal.
  See discussion of the functions below.
cv::CvTreeNodeIterator: |-
  The structure `CvTreeNodeIterator` is used to traverse trees of
  sequences.
cv::ClearGraph: |-
  Clears a graph.
  
  The function removes all vertices and edges from a graph. The function
  has O(1) time complexity.
cv::ClearMemStorage: |-
  Clears memory storage.
  
  The function resets the top (free space boundary) of the storage to the
  very beginning. This function does not deallocate any memory. If the
  storage has a parent, the function returns all blocks to the parent.
cv::ClearSeq: "Clears a sequence.\n\n\
  The function removes all elements from a sequence. The function does not\n\
  return the memory to the storage block, but this memory is reused later\n\
  when new elements are added to the sequence. The function has 'O(1)'\n\
  time complexity.\n\n\
  **note**\n\n\
  It is impossible to deallocate a sequence, i.e. free space in the\n\
  memory storage occupied by the sequence. Instead, call\n\
  :ocvClearMemStorage or :ocvReleaseMemStorage from time to time\n\
  somewhere in a top-level processing loop.\n"
cv::ClearSet: |-
  Clears a set.
  
  The function removes all elements from set. It has O(1) time complexity.
cv::CloneGraph: |-
  Clones a graph.
  
  The function creates a full copy of the specified graph. If the graph
  vertices or edges have pointers to some external data, it can still be
  shared between the copies. The vertex and edge indices in the new graph
  may be different from the original because the function defragments the
  vertex and edge sets.
cv::CloneSeq: |-
  Creates a copy of a sequence.
  
  The function makes a complete copy of the input sequence and returns it.
  
  The call `cvCloneSeq( seq, storage )` is equivalent to
  `cvSeqSlice( seq, CV_WHOLE_SEQ, storage, 1 )`.
cv::CreateChildMemStorage: |-
  Creates child memory storage.
  
  The function creates a child memory storage that is similar to simple
  memory storage except for the differences in the memory
  allocation/deallocation mechanism. When a child storage needs a new
  block to add to the block list, it tries to get this block from the
  parent. The first unoccupied parent block available is taken and
  excluded from the parent block list. If no blocks are available, the
  parent either allocates a block or borrows one from its own parent, if
  any. In other words, the chain, or a more complex structure, of memory
  storages where every storage is a child/parent of another is possible.
  When a child storage is released or even cleared, it returns all blocks
  to the parent. In other aspects, child storage is the same as simple
  storage.
  
  Child storage is useful in the following situation. Imagine that the
  user needs to process dynamic data residing in a given storage area and
  put the result back to that same storage area. With the simplest
  approach, when temporary data is resided in the same storage area as the
  input and output data, the storage area will look as follows after
  processing:
  
  Dynamic data processing without using child storage
  
  ![image](pics/memstorage1.png)
  
  That is, garbage appears in the middle of the storage. However, if one
  creates a child memory storage at the beginning of processing, writes
  temporary data there, and releases the child storage at the end, no
  garbage will appear in the source/destination storage:
  
  Dynamic data processing using a child storage
  
  ![image](pics/memstorage2.png)
cv::CreateGraph: |-
  Creates an empty graph.
  
  The function creates an empty graph and returns a pointer to it.
cv::CreateGraphScanner: |-
  Creates structure for depth-first graph traversal.
  
  The function creates a structure for depth-first graph traversal/search.
  The initialized structure is used in the :ocvNextGraphItem function -
  the incremental traversal procedure.
cv::CreateMemStorage: |-
  Creates memory storage.
  
  The function creates an empty memory storage. See :ocvCvMemStorage
  description.
cv::CreateSeq: |-
  Creates a sequence.
  
  The function creates a sequence and returns the pointer to it. The
  function allocates the sequence header in the storage block as one
  continuous chunk and sets the structure fields `flags` , `elemSize` ,
  `headerSize` , and `storage` to passed values, sets `delta_elems` to the
  default value (that may be reassigned using the :ocvSetSeqBlockSize
  function), and clears other header fields, including the space following
  the first `sizeof(CvSeq)` bytes.
cv::CreateSet: |-
  Creates an empty set.
  
  The function creates an empty set with a specified header size and
  element size, and returns the pointer to the set. This function is just
  a thin layer on top of :ocvCreateSeq.
cv::CvtSeqToArray: |-
  Copies a sequence to one continuous block of memory.
  
  The function copies the entire sequence or subsequence to the specified
  buffer and returns the pointer to the buffer.
cv::EndWriteSeq: "Finishes the process of writing a sequence.\n\n\
  The function finishes the writing process and returns the pointer to the\n\
  written sequence. The function also truncates the last incomplete\n\
  sequence block to return the remaining part of the block to memory\n\
  storage. After that, the sequence can be read and modified safely. See\n\
  :ocvStartWriteSeq and :ocvStartAppendToSeq"
cv::FindGraphEdge: |-
  Finds an edge in a graph.
  
  ```c++
  #define cvGraphFindEdge cvFindGraphEdge
  ```
  
  
  The function finds the graph edge connecting two specified vertices and
  returns a pointer to it or NULL if the edge does not exist.
cv::FindGraphEdgeByPtr: |-
  Finds an edge in a graph by using its pointer.
  
  ```c++
  #define cvGraphFindEdgeByPtr cvFindGraphEdgeByPtr
  ```
  
  
  The function finds the graph edge connecting two specified vertices and
  returns pointer to it or NULL if the edge does not exists.
cv::FlushSeqWriter: |-
  Updates sequence headers from the writer.
  
  The function is intended to enable the user to read sequence elements,
  whenever required, during the writing process, e.g., in order to check
  specific conditions. The function updates the sequence headers to make
  reading from the sequence possible. The writer is not closed, however,
  so that the writing process can be continued at any time. If an
  algorithm requires frequent flushes, consider using :ocvSeqPush instead.
cv::GetGraphVtx: |-
  Finds a graph vertex by using its index.
  
  The function finds the graph vertex by using its index and returns the
  pointer to it or NULL if the vertex does not belong to the graph.
cv::GetSeqElem: |-
  Returns a pointer to a sequence element according to its index.
  
  ```c++
  #define CV_GET_SEQ_ELEM( TYPE, seq, index )  (TYPE*)cvGetSeqElem( (CvSeq*)(seq), (index) )
  ```
  
  
  The function finds the element with the given index in the sequence and
  returns the pointer to it. If the element is not found, the function
  returns 0. The function supports negative indices, where -1 stands for
  the last sequence element, -2 stands for the one before last, etc. If
  the sequence is most likely to consist of a single sequence block or the
  desired element is likely to be located in the first block, then the
  macro `CV_GET_SEQ_ELEM( elemType, seq, index )` should be used, where
  the parameter `elemType` is the type of sequence elements ( :ocvCvPoint
  for example), the parameter `seq` is a sequence, and the parameter
  `index` is the index of the desired element. The macro checks first
  whether the desired element belongs to the first block of the sequence
  and returns it if it does; otherwise the macro calls the main function
  `GetSeqElem` . Negative indices always cause the :ocvGetSeqElem call.
  The function has O(1) time complexity assuming that the number of blocks
  is much smaller than the number of elements.
cv::GetSeqReaderPos: |-
  Returns the current reader position.
  
  The function returns the current reader position (within 0 ...
  `reader->seq->total` - 1).
cv::GetSetElem: |-
  Finds a set element by its index.
  
  The function finds a set element by its index. The function returns the
  pointer to it or 0 if the index is invalid or the corresponding node is
  free. The function supports negative indices as it uses :ocvGetSeqElem
  to locate the node.
cv::GraphAddEdge: |-
  Adds an edge to a graph.
  
  The function connects two specified vertices. The function returns 1 if
  the edge has been added successfully, 0 if the edge connecting the two
  vertices exists already and -1 if either of the vertices was not found,
  the starting and the ending vertex are the same, or there is some other
  critical situation. In the latter case (i.e., when the result is
  negative), the function also reports an error by default.
cv::GraphAddEdgeByPtr: |-
  Adds an edge to a graph by using its pointer.
  
  The function connects two specified vertices. The function returns 1 if
  the edge has been added successfully, 0 if the edge connecting the two
  vertices exists already, and -1 if either of the vertices was not found,
  the starting and the ending vertex are the same or there is some other
  critical situation. In the latter case (i.e., when the result is
  negative), the function also reports an error by default.
cv::GraphAddVtx: |-
  Adds a vertex to a graph.
  
  The function adds a vertex to the graph and returns the vertex index.
cv::GraphEdgeIdx: |-
  Returns the index of a graph edge.
  
  The function returns the index of a graph edge.
cv::GraphRemoveEdge: |-
  Removes an edge from a graph.
  
  The function removes the edge connecting two specified vertices. If the
  vertices are not connected [in that order], the function does nothing.
cv::GraphRemoveEdgeByPtr: |-
  Removes an edge from a graph by using its pointer.
  
  The function removes the edge connecting two specified vertices. If the
  vertices are not connected [in that order], the function does nothing.
cv::GraphRemoveVtx: |-
  Removes a vertex from a graph.
  
  The function removes a vertex from a graph together with all the edges
  incident to it. The function reports an error if the input vertex does
  not belong to the graph. The return value is the number of edges
  deleted, or -1 if the vertex does not belong to the graph.
cv::GraphRemoveVtxByPtr: |-
  Removes a vertex from a graph by using its pointer.
  
  The function removes a vertex from the graph by using its pointer
  together with all the edges incident to it. The function reports an
  error if the vertex does not belong to the graph. The return value is
  the number of edges deleted, or -1 if the vertex does not belong to the
  graph.
cv::GraphVtxDegree: |-
  Counts the number of edges incident to the vertex.
  
  The function returns the number of edges incident to the specified
  vertex, both incoming and outgoing. To count the edges, the following
  code is used:
  
  ```c++
  CvGraphEdge* edge = vertex->first; int count = 0;
  while( edge )
  {
      edge = CV_NEXT_GRAPH_EDGE( edge, vertex );
      count++;
  }
  ```
  
  
  The macro `CV_NEXT_GRAPH_EDGE( edge, vertex )` returns the edge incident
  to `vertex` that follows after `edge` .
cv::GraphVtxDegreeByPtr: |-
  Finds an edge in a graph.
  
  The function returns the number of edges incident to the specified
  vertex, both incoming and outcoming.
cv::GraphVtxIdx: |-
  Returns the index of a graph vertex.
  
  The function returns the index of a graph vertex.
cv::InitTreeNodeIterator: |-
  Initializes the tree node iterator.
  
  The function initializes the tree iterator. The tree is traversed in
  depth-first order.
cv::InsertNodeIntoTree: |-
  Adds a new node to a tree.
  
  The function adds another node into tree. The function does not allocate
  any memory, it can only modify links of the tree nodes.
cv::MakeSeqHeaderForArray: |-
  Constructs a sequence header for an array.
  
  The function initializes a sequence header for an array. The sequence
  header as well as the sequence block are allocated by the user (for
  example, on stack). No data is copied by the function. The resultant
  sequence will consists of a single block and have NULL storage pointer;
  thus, it is possible to read its elements, but the attempts to add
  elements to the sequence will raise an error in most cases.
cv::MemStorageAlloc: |-
  Allocates a memory buffer in a storage block.
  
  The function allocates a memory buffer in a storage block. The buffer
  size must not exceed the storage block size, otherwise a runtime error
  is raised. The buffer address is aligned by
  `CV_STRUCT_ALIGN=sizeof(double)` (for the moment) bytes.
cv::MemStorageAllocString: |-
  Allocates a text string in a storage block.
  
  ```c++
  typedef struct CvString
  {
      int len;
      char* ptr;
  }
  CvString;
  ```
  
  
  The function creates copy of the string in memory storage. It returns
  the structure that contains user-passed or computed length of the string
  and pointer to the copied string.
cv::NextGraphItem: "Executes one or more steps of the graph traversal procedure.\n\n\
  The function traverses through the graph until an event of interest to\n\
  the user (that is, an event, specified in the `mask` in the\n\
  :ocvCreateGraphScanner call) is met or the traversal is completed. In\n\
  the first case, it returns one of the events listed in the description\n\
  of the `mask` parameter above and with the next call it resumes the\n\
  traversal. In the latter case, it returns `CV_GRAPH_OVER` (-1). When the\n\
  event is `CV_GRAPH_VERTEX` , `CV_GRAPH_BACKTRACKING` , or\n\
  `CV_GRAPH_NEW_TREE` , the currently observed vertex is stored in\n\
  `` scanner-:math:`>`vtx `` . And if the event is edge-related, the edge\n\
  itself is stored at `` scanner-:math:`>`edge `` , the previously visited\n\
  vertex - at `` scanner-:math:`>`vtx `` and the other ending vertex of\n\
  the edge - at `` scanner-:math:`>`dst `` ."
cv::NextTreeNode: |-
  Returns the currently observed node and moves the iterator toward the
  next node.
  
  The function returns the currently observed node and then updates the
  iterator - moving it toward the next node. In other words, the function
  behavior is similar to the `*p++` expression on a typical C pointer or
  C++ collection iterator. The function returns NULL if there are no more
  nodes.
cv::PrevTreeNode: |-
  Returns the currently observed node and moves the iterator toward the
  previous node.
  
  The function returns the currently observed node and then updates the
  iterator - moving it toward the previous node. In other words, the
  function behavior is similar to the `*p--` expression on a typical C
  pointer or C++ collection iterator. The function returns NULL if there
  are no more nodes.
cv::ReleaseGraphScanner: |-
  Completes the graph traversal procedure.
  
  The function completes the graph traversal procedure and releases the
  traverser state.
cv::ReleaseMemStorage: |-
  Releases memory storage.
  
  The function deallocates all storage memory blocks or returns them to
  the parent, if any. Then it deallocates the storage header and clears
  the pointer to the storage. All child storage associated with a given
  parent storage block must be released before the parent storage block is
  released.
cv::RestoreMemStoragePos: |-
  Restores memory storage position.
  
  The function restores the position of the storage top from the parameter
  `pos` . This function and the function `cvClearMemStorage` are the only
  methods to release memory occupied in memory blocks. Note again that
  there is no way to free memory in the middle of an occupied portion of a
  storage block.
cv::SaveMemStoragePos: |-
  Saves memory storage position.
  
  The function saves the current position of the storage top to the
  parameter `pos` . The function `cvRestoreMemStoragePos` can further
  retrieve this position.
cv::SeqElemIdx: |-
  Returns the index of a specific sequence element.
  
  The function returns the index of a sequence element or a negative
  number if the element is not found.
cv::SeqInsert: |-
  Inserts an element in the middle of a sequence.
  
  The function shifts the sequence elements from the inserted position to
  the nearest end of the sequence and copies the `element` content there
  if the pointer is not NULL. The function returns a pointer to the
  inserted element.
cv::SeqInsertSlice: |-
  Inserts an array in the middle of a sequence.
  
  The function inserts all `fromArr` array elements at the specified
  position of the sequence. The array `fromArr` can be a matrix or another
  sequence.
cv::SeqInvert: |-
  Reverses the order of sequence elements.
  
  The function reverses the sequence in-place - the first element becomes
  the last one, the last element becomes the first one and so forth.
cv::SeqPop: |-
  Removes an element from the end of a sequence.
  
  The function removes an element from a sequence. The function reports an
  error if the sequence is already empty. The function has O(1)
  complexity.
cv::SeqPopFront: |-
  Removes an element from the beginning of a sequence.
  
  The function removes an element from the beginning of a sequence. The
  function reports an error if the sequence is already empty. The function
  has O(1) complexity.
cv::SeqPopMulti: |-
  Removes several elements from either end of a sequence.
  
  The function removes several elements from either end of the sequence.
  If the number of the elements to be removed exceeds the total number of
  elements in the sequence, the function removes as many elements as
  possible.
cv::SeqPush: |-
  Adds an element to the end of a sequence.
  
  The function adds an element to the end of a sequence and returns a
  pointer to the allocated element. If the input `element` is NULL, the
  function simply allocates a space for one more element.
  
  The following code demonstrates how to create a new sequence using this
  function:
  
  ```c++
  CvMemStorage* storage = cvCreateMemStorage(0);
  CvSeq* seq = cvCreateSeq( CV_32SC1, /* sequence of integer elements */
                            sizeof(CvSeq), /* header size - no extra fields */
                            sizeof(int), /* element size */
                            storage /* the container storage */ );
  int i;
  for( i = 0; i < 100; i++ )
  {
      int* added = (int*)cvSeqPush( seq, &i );
      printf( "
  }
  
  ...
  /* release memory storage in the end */
  cvReleaseMemStorage( &storage );
  ```
  
  
  The function has O(1) complexity, but there is a faster method for
  writing large sequences (see :ocvStartWriteSeq and related functions).
cv::SeqPushFront: |-
  Adds an element to the beginning of a sequence.
  
  The function is similar to :ocvSeqPush but it adds the new element to
  the beginning of the sequence. The function has O(1) complexity.
cv::SeqPushMulti: |-
  Pushes several elements to either end of a sequence.
  
  The function adds several elements to either end of a sequence. The
  elements are added to the sequence in the same order as they are
  arranged in the input array but they can fall into different sequence
  blocks.
cv::SeqRemove: |-
  Removes an element from the middle of a sequence.
  
  The function removes elements with the given index. If the index is out
  of range the function reports an error. An attempt to remove an element
  from an empty sequence is a special case of this situation. The function
  removes an element by shifting the sequence elements between the nearest
  end of the sequence and the `index` -th position, not counting the
  latter.
cv::SeqRemoveSlice: |-
  Removes a sequence slice.
  
  The function removes a slice from the sequence.
cv::SeqSearch: |-
  Searches for an element in a sequence.
  
  ```c++
  /* a < b ? -1 : a > b ? 1 : 0 */
  typedef int (CV_CDECL* CvCmpFunc)(const void* a, const void* b, void* userdata);
  ```
  
  
  The function searches for the element in the sequence. If the sequence
  is sorted, a binary O(log(N)) search is used; otherwise, a simple linear
  search is used. If the element is not found, the function returns a NULL
  pointer and the index is set to the number of sequence elements if a
  linear search is used, or to the smallest index `i, seq(i)>elem` .
cv::SeqSlice: |-
  Makes a separate header for a sequence slice.
  
  The function creates a sequence that represents the specified slice of
  the input sequence. The new sequence either shares the elements with the
  original sequence or has its own copy of the elements. So if one needs
  to process a part of sequence but the processing function does not have
  a slice parameter, the required sub-sequence may be extracted using this
  function.
cv::SeqSort: |
  Sorts sequence element using the specified comparison function.
  
  ```c++
  /* a < b ? -1 : a > b ? 1 : 0 */
  typedef int (CV_CDECL* CvCmpFunc)(const void* a, const void* b, void* userdata);
  ```
  
  
  The function sorts the sequence in-place using the specified criteria.
  Below is an example of using this function:
  
  ```c++
  /* Sort 2d points in top-to-bottom left-to-right order */
  static int cmp_func( const void* _a, const void* _b, void* userdata )
  {
      CvPoint* a = (CvPoint*)_a;
      CvPoint* b = (CvPoint*)_b;
      int y_diff = a->y - b->y;
      int x_diff = a->x - b->x;
      return y_diff ? y_diff : x_diff;
  }
  
  ...
  
  CvMemStorage* storage = cvCreateMemStorage(0);
  CvSeq* seq = cvCreateSeq( CV_32SC2, sizeof(CvSeq), sizeof(CvPoint), storage );
  int i;
  
  for( i = 0; i < 10; i++ )
  {
      CvPoint pt;
      pt.x = rand()
      pt.y = rand()
      cvSeqPush( seq, &pt );
  }
  
  cvSeqSort( seq, cmp_func, 0 /* userdata is not used here */ );
  
  /* print out the sorted sequence */
  for( i = 0; i < seq->total; i++ )
  {
      CvPoint* pt = (CvPoint*)cvSeqElem( seq, i );
      printf( "(
  }
  
  cvReleaseMemStorage( &storage );
  ```

cv::SetAdd: |-
  Occupies a node in the set.
  
  The function allocates a new node, optionally copies input element data
  to it, and returns the pointer and the index to the node. The index
  value is taken from the lower bits of the `flags` field of the node. The
  function has O(1) complexity; however, there exists a faster function
  for allocating set nodes (see :ocvSetNew ).
cv::SetNew: |-
  Adds an element to a set (fast variant).
  
  The function is an inline lightweight variant of :ocvSetAdd . It
  occupies a new node and returns a pointer to it rather than an index.
cv::SetRemove: |-
  Removes an element from a set.
  
  The function removes an element with a specified index from the set. If
  the node at the specified location is not occupied, the function does
  nothing. The function has O(1) complexity; however, :ocvSetRemoveByPtr
  provides a quicker way to remove a set element if it is located already.
cv::SetRemoveByPtr: |-
  Removes a set element based on its pointer.
  
  The function is an inline lightweight variant of :ocvSetRemove that
  requires an element pointer. The function does not check whether the
  node is occupied or not - the user should take care of that.
cv::SetSeqBlockSize: |-
  Sets up sequence block size.
  
  The function affects memory allocation granularity. When the free space
  in the sequence buffers has run out, the function allocates the space
  for `delta_elems` sequence elements. If this block immediately follows
  the one previously allocated, the two blocks are concatenated;
  otherwise, a new sequence block is created. Therefore, the bigger the
  parameter is, the lower the possible sequence fragmentation, but the
  more space in the storage block is wasted. When the sequence is created,
  the parameter `delta_elems` is set to the default value of about 1K. The
  function can be called any time after the sequence is created and
  affects future allocations. The function can modify the passed value of
  the parameter to meet memory storage constraints.
cv::SetSeqReaderPos: |-
  Moves the reader to the specified position.
  
  The function moves the read position to an absolute position or relative
  to the current position.
cv::StartAppendToSeq: "Initializes the process of writing data to a sequence.\n\n\
  The function initializes the process of writing data to a sequence.\n\
  Written elements are added to the end of the sequence by using the\n\
  `CV_WRITE_SEQ_ELEM( written_elem, writer )` macro. Note that during the\n\
  writing process, other operations on the sequence may yield an incorrect\n\
  result or even corrupt the sequence (see description of\n\
  :ocvFlushSeqWriter , which helps to avoid some of these problems)."
cv::StartReadSeq: |
  Initializes the process of sequential reading from a sequence.
  
  The function initializes the reader state. After that, all the sequence
  elements from the first one down to the last one can be read by
  subsequent calls of the macro `CV_READ_SEQ_ELEM( read_elem, reader )` in
  the case of forward reading and by using
  `CV_REV_READ_SEQ_ELEM( read_elem, reader )` in the case of reverse
  reading. Both macros put the sequence element to `read_elem` and move
  the reading pointer toward the next element. A circular structure of
  sequence blocks is used for the reading process, that is, after the last
  element has been read by the macro `CV_READ_SEQ_ELEM` , the first
  element is read when the macro is called again. The same applies to
  `CV_REV_READ_SEQ_ELEM` . There is no function to finish the reading
  process, since it neither changes the sequence nor creates any temporary
  buffers. The reader field `ptr` points to the current element of the
  sequence that is to be read next. The code below demonstrates how to use
  the sequence writer and reader.
  
  ```c++
  CvMemStorage* storage = cvCreateMemStorage(0);
  CvSeq* seq = cvCreateSeq( CV_32SC1, sizeof(CvSeq), sizeof(int), storage );
  CvSeqWriter writer;
  CvSeqReader reader;
  int i;
  
  cvStartAppendToSeq( seq, &writer );
  for( i = 0; i < 10; i++ )
  {
      int val = rand()
      CV_WRITE_SEQ_ELEM( val, writer );
      printf("
  }
  cvEndWriteSeq( &writer );
  
  cvStartReadSeq( seq, &reader, 0 );
  for( i = 0; i < seq->total; i++ )
  {
      int val;
  #if 1
      CV_READ_SEQ_ELEM( val, reader );
      printf("
  #else /* alternative way, that is prefferable if sequence elements are large,
           or their size/type is unknown at compile time */
      printf("
      CV_NEXT_SEQ_ELEM( seq->elem_size, reader );
  #endif
  }
  ...
  
  cvReleaseStorage( &storage );
  ```

cv::StartWriteSeq: |-
  Creates a new sequence and initializes a writer for it.
  
  The function is a combination of :ocvCreateSeq and :ocvStartAppendToSeq
  . The pointer to the created sequence is stored at `writer->seq` and is
  also returned by the :ocvEndWriteSeq function that should be called at
  the end.
cv::TreeToNodeSeq: |-
  Gathers all node pointers to a single sequence.
  
  The function puts pointers of all nodes reachable from `first` into a
  single sequence. The pointers are written sequentially in the
  depth-first order.
cv::gpu::add: Computes a matrix-matrix or matrix-scalar sum.
cv::gpu::subtract: Computes a matrix-matrix or matrix-scalar difference.
cv::gpu::multiply: Computes a matrix-matrix or matrix-scalar per-element product.
cv::gpu::divide: |-
  Computes a matrix-matrix or matrix-scalar division.
  
  This function, in contrast to :ocvdivide, uses a round-down rounding
  mode.
cv::gpu::absdiff: |-
  Computes per-element absolute difference of two matrices (or of a matrix
  and scalar).
cv::gpu::abs: Computes an absolute value of each matrix element.
cv::gpu::sqr: Computes a square value of each matrix element.
cv::gpu::sqrt: Computes a square root of each matrix element.
cv::gpu::exp: Computes an exponent of each matrix element.
cv::gpu::log: Computes a natural logarithm of absolute value of each matrix element.
cv::gpu::pow: |-
  Raises every matrix element to a power.
  
  The function `pow` raises every element of the input matrix to `power` :
  
  $$\texttt{dst} (I) =  \fork{\texttt{src}(I)^power}{if \texttt{power} is integer}{|\texttt{src}(I)|^power}{otherwise}$$
cv::gpu::compare: Compares elements of two matrices (or of a matrix and scalar).
cv::gpu::bitwise_not: Performs a per-element bitwise inversion.
cv::gpu::bitwise_or: |-
  Performs a per-element bitwise disjunction of two matrices (or of matrix
  and scalar).
cv::gpu::bitwise_and: |-
  Performs a per-element bitwise conjunction of two matrices (or of matrix
  and scalar).
cv::gpu::bitwise_xor: |-
  Performs a per-element bitwise `exclusive or` operation of two matrices
  (or of matrix and scalar).
cv::gpu::rshift: Performs pixel by pixel right shift of an image by a constant value.
cv::gpu::lshift: Performs pixel by pixel right left of an image by a constant value.
cv::gpu::min: |-
  Computes the per-element minimum of two matrices (or a matrix and a
  scalar).
cv::gpu::max: |-
  Computes the per-element maximum of two matrices (or a matrix and a
  scalar).
cv::gpu::addWeighted: |-
  Computes the weighted sum of two arrays.
  
  The function `addWeighted` calculates the weighted sum of two arrays as
  follows:
  
  $$\texttt{dst} (I)= \texttt{saturate} ( \texttt{src1} (I)* \texttt{alpha} +  \texttt{src2} (I)* \texttt{beta} +  \texttt{gamma} )$$
  
  where `I` is a multi-dimensional index of array elements. In case of
  multi-channel arrays, each channel is processed independently.
cv::gpu::threshold: Applies a fixed-level threshold to each array element.
cv::gpu::magnitude: Computes magnitudes of complex matrix elements.
cv::gpu::magnitudeSqr: Computes squared magnitudes of complex matrix elements.
cv::gpu::phase: Computes polar angles of complex matrix elements.
cv::gpu::cartToPolar: Converts Cartesian coordinates into polar.
cv::gpu::polarToCart: Converts polar coordinates into Cartesian.
cv::Extremely randomized trees: |
  Extremely randomized trees have been introduced by Pierre Geurts, Damien
  Ernst and Louis Wehenkel in the article "Extremely randomized trees",
  2006
  [<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf>].
  The algorithm of growing Extremely randomized trees is similar to
  Random Trees (Random Forest), but there are two differences:
  
   Extremely randomized trees don't apply the bagging procedure to
  construct a set of the training samples for each tree. The same
  input training set is used to train all trees.
  
  
   Extremely randomized trees pick a node split very extremely (both a
  variable index and variable splitting value are chosen randomly),
  whereas Random Forest finds the best split (optimal one by variable
  index and variable splitting value) among random subset of
  variables.

cv::Expectation Maximization: |-
  The Expectation Maximization(EM) algorithm estimates the parameters of
  the multivariate probability density function in the form of a Gaussian
  mixture distribution with a specified number of mixtures.
  
  Consider the set of the N feature vectors { $x_1, x_2,...,x_{N}$ } from
  a d-dimensional Euclidean space drawn from a Gaussian mixture:
  
  $$p(x;a_k,S_k, \pi _k) =  \sum _{k=1}^{m} \pi _kp_k(x),  \quad \pi _k  \geq 0,  \quad \sum _{k=1}^{m} \pi _k=1,$$
  
  $$p_k(x)= \varphi (x;a_k,S_k)= \frac{1}{(2\pi)^{d/2}\mid{S_k}\mid^{1/2}} exp \left { - \frac{1}{2} (x-a_k)^TS_k^{-1}(x-a_k) \right } ,$$
  
  where $m$ is the number of mixtures, $p_k$ is the normal distribution
  density with the mean $a_k$ and covariance matrix $S_k$, $\pi_k$ is the
  weight of the k-th mixture. Given the number of mixtures $M$ and the
  samples $x_i$, $i=1..N$ the algorithm finds the maximum-likelihood
  estimates (MLE) of all the mixture parameters, that is, $a_k$, $S_k$ and
  $\pi_k$ :
  
  $$L(x, \theta )=logp(x, \theta )= \sum _{i=1}^{N}log \left ( \sum _{k=1}^{m} \pi _kp_k(x) \right ) \to \max _{ \theta \in \Theta },$$
  
  $$\Theta = \left { (a_k,S_k, \pi _k): a_k  \in \mathbbm{R} ^d,S_k=S_k^T>0,S_k  \in \mathbbm{R} ^{d  \times d}, \pi _k \geq 0, \sum _{k=1}^{m} \pi _k=1 \right } .$$
  
  The EM algorithm is an iterative procedure. Each iteration includes two
  steps. At the first step (Expectation step or E-step), you find a
  probability $p_{i,k}$ (denoted $\alpha_{i,k}$ in the formula below) of
  sample `i` to belong to mixture `k` using the currently available
  mixture parameter estimates:
  
  $$\alpha _{ki} =  \frac{\pi_k\varphi(x;a_k,S_k)}{\sum\limits_{j=1}^{m}\pi_j\varphi(x;a_j,S_j)} .$$
  
  At the second step (Maximization step or M-step), the mixture parameter
  estimates are refined using the computed probabilities:
  
  $$\pi _k= \frac{1}{N} \sum _{i=1}^{N} \alpha _{ki},  \quad a_k= \frac{\sum\limits_{i=1}^{N}\alpha_{ki}x_i}{\sum\limits_{i=1}^{N}\alpha_{ki}} ,  \quad S_k= \frac{\sum\limits_{i=1}^{N}\alpha_{ki}(x_i-a_k)(x_i-a_k)^T}{\sum\limits_{i=1}^{N}\alpha_{ki}}$$
  
  Alternatively, the algorithm may start with the M-step when the initial
  values for $p_{i,k}$ can be provided. Another alternative when $p_{i,k}$
  are unknown is to use a simpler clustering algorithm to pre-cluster the
  input samples and thus obtain initial $p_{i,k}$ . Often (including
  machine learning) the :ocvkmeans algorithm is used for that purpose.
  
  One of the main problems of the EM algorithm is a large number of
  parameters to estimate. The majority of the parameters reside in
  covariance matrices, which are $d \times d$ elements each where $d$ is
  the feature space dimensionality. However, in many practical problems,
  the covariance matrices are close to diagonal or even to $\mu_k*I$ ,
  where $I$ is an identity matrix and $\mu_k$ is a mixture-dependent
  "scale" parameter. So, a robust computation scheme could start with
  harder constraints on the covariance matrices and then use the estimated
  parameters as an input for a less constrained optimization problem
  (often a diagonal covariance matrix is already a good enough
  approximation).
  
  **References:**
  
  *
  :   Bilmes98 J. A. Bilmes. *A Gentle Tutorial of the EM Algorithm and
      its Application to Parameter Estimation for Gaussian Mixture and
      Hidden Markov Models*. Technical Report TR-97-021, International
      Computer Science Institute and Computer Science Division, University
      of California at Berkeley, April 1998.
cv::EM: |-
  The class implements the EM algorithm as described in the beginning of
  this section. It is inherited from :ocvAlgorithm.
cv::EM::EM: The constructor of the class
cv::EM::train: |-
  Estimates the Gaussian mixture parameters from a samples set.
  
  Three versions of training method differ in the initialization of
  Gaussian mixture model parameters and start step:
  
    **train** - Starts with Expectation step. Initial values of the
  model parameters will be estimated by the k-means algorithm.
  
  
    **trainE** - Starts with Expectation step. You need to provide
  initial means $a_k$ of mixture components. Optionally you can pass
  initial weights $\pi_k$ and covariance matrices $S_k$ of mixture
  components.
  
  
    **trainM** - Starts with Maximization step. You need to provide
  initial probabilities $p_{i,k}$ to use this option.
  
  
  The methods return `true` if the Gaussian mixture model was trained
  successfully, otherwise it returns `false`.
  
  Unlike many of the ML models, EM is an unsupervised learning algorithm
  and it does not take responses (class labels or function values) as
  input. Instead, it computes the *Maximum Likelihood Estimate* of the
  Gaussian mixture parameters from an input sample set, stores all the
  parameters inside the structure: $p_{i,k}$ in `probs`, $a_k$ in `means`
  , $S_k$ in `covs[k]`, $\pi_k$ in `weights` , and optionally computes the
  output "class label" for each sample:
  $\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N$ (indices of the
  most probable mixture component for each sample).
  
  The trained model can be used further for prediction, just like any
  other classifier. The trained model is similar to the
  :ocvCvNormalBayesClassifier.
cv::EM::predict: |-
  Returns a likelihood logarithm value and an index of the most probable
  mixture component for the given sample.
  
  The method returns a two-element `double` vector. Zero element is a
  likelihood logarithm value for the sample. First element is an index of
  the most probable mixture component for the given sample.
cv::CvEM::isTrained: Returns `true` if the Gaussian mixture model was trained.
cv::EM::read, EM::write: See :ocvAlgorithm::read and :ocvAlgorithm::write.
cv::EM::get, EM::set: |
  See :ocvAlgorithm::get and :ocvAlgorithm::set. The following parameters
  are available:
    `"nclusters"`
  
    `"covMatType"`
  
    `"maxIters"`
  
    `"epsilon"`
  
    `"weights"` *(read-only)*
  
    `"means"` *(read-only)*
  
    `"covs"` *(read-only)*

cv::detail::ExposureCompensator: |
  Base class for all exposure compensators. :
  
  ```c++
  class CV_EXPORTS ExposureCompensator
  {
  public:
      virtual ~ExposureCompensator() {}
  
      enum { NO, GAIN, GAIN_BLOCKS };
      static Ptr<ExposureCompensator> createDefault(int type);
  
      void feed(const std::vector<Point> &corners, const std::vector<Mat> &images,
                const std::vector<Mat> &masks);
      virtual void feed(const std::vector<Point> &corners, const std::vector<Mat> &images,
                        const std::vector<std::pair<Mat,uchar> > &masks) = 0;
      virtual void apply(int index, Point corner, Mat &image, const Mat &mask) = 0;
  };
  ```

cv::detil::ExposureCompensator::apply: Compensate exposure in the specified image.
cv::detail::NoExposureCompensator: |
  Stub exposure compensator which does nothing. :
  
  ```c++
  class CV_EXPORTS NoExposureCompensator : public ExposureCompensator
  {
  public:
      void feed(const std::vector<Point> &/*corners*/, const std::vector<Mat> &/*images*/,
                const std::vector<std::pair<Mat,uchar> > &/*masks*/) {};
      void apply(int /*index*/, Point /*corner*/, Mat &/*image*/, const Mat &/*mask*/) {};
  };
  ```

cv::detail::GainCompensator: |
  Exposure compensator which tries to remove exposure related artifacts by
  adjusting image intensities, see [BL07]_ and [WJ10]_ for details. :
  
  ```c++
  class CV_EXPORTS GainCompensator : public ExposureCompensator
  {
  public:
      void feed(const std::vector<Point> &corners, const std::vector<Mat> &images,
                const std::vector<std::pair<Mat,uchar> > &masks);
      void apply(int index, Point corner, Mat &image, const Mat &mask);
      std::vector<double> gains() const;
  
  private:
      /* hidden */
  };
  ```

cv::detail::BlocksGainCompensator: |
  Exposure compensator which tries to remove exposure related artifacts by
  adjusting image block intensities, see [UES01]_ for details. :
  
  ```c++
  class CV_EXPORTS BlocksGainCompensator : public ExposureCompensator
  {
  public:
      BlocksGainCompensator(int bl_width = 32, int bl_height = 32)
              : bl_width_(bl_width), bl_height_(bl_height) {}
      void feed(const std::vector<Point> &corners, const std::vector<Mat> &images,
                const std::vector<std::pair<Mat,uchar> > &masks);
      void apply(int index, Point corner, Mat &image, const Mat &mask);
  
  private:
      /* hidden */
  };
  ```

cv::FaceRecognizer: |
  All face recognition models in OpenCV are derived from the abstract base
  class :ocvFaceRecognizer, which provides a unified access to all face
  recongition algorithms in OpenCV. :
  
  ```c++
  class FaceRecognizer : public Algorithm
  {
  public:
      //! virtual destructor
      virtual ~FaceRecognizer() {}
  
      // Trains a FaceRecognizer.
      virtual void train(InputArray src, InputArray labels) = 0;
  
      // Updates a FaceRecognizer.
      virtual void update(InputArrayOfArrays src, InputArray labels);
  
      // Gets a prediction from a FaceRecognizer.
      virtual int predict(InputArray src) const = 0;
  
      // Predicts the label and confidence for a given sample.
      virtual void predict(InputArray src, int &label, double &confidence) const = 0;
  
      // Serializes this object to a given filename.
      virtual void save(const String& filename) const;
  
      // Deserializes this object from a given filename.
      virtual void load(const String& filename);
  
      // Serializes this object to a given cv::FileStorage.
      virtual void save(FileStorage& fs) const = 0;
  
      // Deserializes this object from a given cv::FileStorage.
      virtual void load(const FileStorage& fs) = 0;
  };
  ```

cv::Setting the Thresholds: "Sometimes you run into the situation, when you want to apply a threshold\n\
  on the prediction. A common scenario in face recognition is to tell,\n\
  wether a face belongs to the training dataset or if it is unknown. You\n\
  might wonder, why there's no public API in :ocvFaceRecognizer to set the\n\
  threshold for the prediction, but rest assured: It's supported. It just\n\
  means there's no generic way in an abstract class to provide an\n\
  interface for setting/getting the thresholds of *every possible*\n\
  :ocvFaceRecognizer algorithm. The appropriate place to set the\n\
  thresholds is in the constructor of the specific :ocvFaceRecognizer and\n\
  since every :ocvFaceRecognizer is a :ocvAlgorithm (see above), you can\n\
  get/set the thresholds at runtime!\n\n\
  Here is an example of setting a threshold for the Eigenfaces method,\n\
  when creating the model:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // Let's say we want to keep 10 Eigenfaces and have a threshold value of 10.0\n\
  int num_components = 10;\n\
  double threshold = 10.0;\n\
  // Then if you want to have a cv::FaceRecognizer with a confidence threshold,\n\
  // create the concrete implementation with the appropiate parameters:\n\
  Ptr<FaceRecognizer> model = createEigenFaceRecognizer(num_components, threshold);\n\
  ~~~~\n\n\
  Sometimes it's impossible to train the model, just to experiment with\n\
  threshold values. Thanks to :ocvAlgorithm it's possible to set internal\n\
  model thresholds during runtime. Let's see how we would set/get the\n\
  prediction for the Eigenface model, we've created above:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // The following line reads the threshold from the Eigenfaces model:\n\
  double current_threshold = model->getDouble(\"threshold\");\n\
  // And this line sets the threshold to 0.0:\n\
  model->set(\"threshold\", 0.0);\n\
  ~~~~\n\n\
  If you've set the threshold to `0.0` as we did above, then:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  //\n\
  Mat img = imread(\"person1/3.jpg\", CV_LOAD_IMAGE_GRAYSCALE);\n\
  // Get a prediction from the model. Note: We've set a threshold of 0.0 above,\n\
  // since the distance is almost always larger than 0.0, you'll get -1 as\n\
  // label, which indicates, this face is unknown\n\
  int predicted_label = model->predict(img);\n\
  // ...\n\
  ~~~~\n\n\
  is going to yield `-1` as predicted label, which states this face is\n\
  unknown."
cv::Getting the name of a FaceRecognizer: "Since every :ocvFaceRecognizer is a :ocvAlgorithm, you can use\n\
  :ocvAlgorithm::name to get the name of a :ocvFaceRecognizer:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // Create a FaceRecognizer:\n\
  Ptr<FaceRecognizer> model = createEigenFaceRecognizer();\n\
  // And here's how to get its name:\n\
  String name = model->name();\n\
  ~~~~"
cv::FaceRecognizer::train: "Trains a FaceRecognizer with given data and associated labels.\n\n\
  The following source code snippet shows you how to learn a Fisherfaces\n\
  model on a given set of images. The images are read with :ocvimread and\n\
  pushed into a `std::vector<Mat>`. The labels of each image are stored\n\
  within a `std::vector<int>` (you could also use a :ocvMat of type\n\
  CV_32SC1). Think of the label as the subject (the person) this image\n\
  belongs to, so same subjects (persons) should have the same label. For\n\
  the available :ocvFaceRecognizer you don't have to pay any attention to\n\
  the order of the labels, just make sure same persons have the same\n\
  label:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // holds images and labels\n\
  vector<Mat> images;\n\
  vector<int> labels;\n\
  // images for first person\n\
  images.push_back(imread(\"person0/0.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n\
  images.push_back(imread(\"person0/1.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n\
  images.push_back(imread(\"person0/2.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(0);\n\
  // images for second person\n\
  images.push_back(imread(\"person1/0.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n\
  images.push_back(imread(\"person1/1.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n\
  images.push_back(imread(\"person1/2.jpg\", CV_LOAD_IMAGE_GRAYSCALE)); labels.push_back(1);\n\
  ~~~~\n\n\
  Now that you have read some images, we can create a new\n\
  :ocvFaceRecognizer. In this example I'll create a Fisherfaces model and\n\
  decide to keep all of the possible Fisherfaces:\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // Create a new Fisherfaces model and retain all available Fisherfaces,\n\
  // this is the most common usage of this specific FaceRecognizer:\n\
  //\n\
  Ptr<FaceRecognizer> model =  createFisherFaceRecognizer();\n\
  ~~~~\n\n\
  And finally train it on the given dataset (the face images and labels):\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // This is the common interface to train all of the available cv::FaceRecognizer\n\
  // implementations:\n\
  //\n\
  model->train(images, labels);\n\
  ~~~~"
cv::FaceRecognizer::update: "Updates a FaceRecognizer with given data and associated labels.\n\n\
  This method updates a (probably trained) :ocvFaceRecognizer, but only if\n\
  the algorithm supports it. The Local Binary Patterns Histograms (LBPH)\n\
  recognizer (see :ocvcreateLBPHFaceRecognizer) can be updated. For the\n\
  Eigenfaces and Fisherfaces method, this is algorithmically not possible\n\
  and you have to re-estimate the model with :ocvFaceRecognizer::train. In\n\
  any case, a call to train empties the existing model and learns a new\n\
  model, while update does not delete any model data.\n\n\
  ~~~~ {.sourceCode .cpp}\n\
  // Create a new LBPH model (it can be updated) and use the default parameters,\n\
  // this is the most common usage of this specific FaceRecognizer:\n\
  //\n\
  Ptr<FaceRecognizer> model =  createLBPHFaceRecognizer();\n\
  // This is the common interface to train all of the available cv::FaceRecognizer\n\
  // implementations:\n\
  //\n\
  model->train(images, labels);\n\
  // Some containers to hold new image:\n\
  vector<Mat> newImages;\n\
  vector<int> newLabels;\n\
  // You should add some images to the containers:\n\
  //\n\
  // ...\n\
  //\n\
  // Now updating the model is as easy as calling:\n\
  model->update(newImages,newLabels);\n\
  // This will preserve the old model data and extend the existing model\n\
  // with the new features extracted from newImages!\n\
  ~~~~\n\n\
  Calling update on an Eigenfaces model (see\n\
  :ocvcreateEigenFaceRecognizer), which doesn't support updating, will\n\
  throw an error similar to:\n\n\
  ~~~~ {.sourceCode .none}\n\
  OpenCV Error: The function/feature is not implemented (This FaceRecognizer (FaceRecognizer.Eigenfaces) does not support updating, you have to use FaceRecognizer::train to update it.) in update, file /home/philipp/git/opencv/modules/contrib/src/facerec.cpp, line 305\n\
  terminate called after throwing an instance of 'cv::Exception'\n\
  ~~~~\n\n\
  Please note: The :ocvFaceRecognizer does not store your training images,\n\
  because this would be very memory intense and it's not the\n\
  responsibility of te :ocvFaceRecognizer to do so. The caller is\n\
  responsible for maintaining the dataset, he want to work with."
cv::FaceRecognizer::predict: |-
  The suffix `const` means that prediction does not affect the internal
  model state, so the method can be safely called from within different
  threads.
  
  The following example shows how to get a prediction from a trained
  model:
  
  ~~~~ {.sourceCode .cpp}
  using namespace cv;
  // Do your initialization here (create the cv::FaceRecognizer model) ...
  // ...
  // Read in a sample image:
  Mat img = imread("person1/3.jpg", CV_LOAD_IMAGE_GRAYSCALE);
  // And get a prediction from the cv::FaceRecognizer:
  int predicted = model->predict(img);
  ~~~~
  
  Or to get a prediction and the associated confidence (e.g. distance):
  
  ~~~~ {.sourceCode .cpp}
  using namespace cv;
  // Do your initialization here (create the cv::FaceRecognizer model) ...
  // ...
  Mat img = imread("person1/3.jpg", CV_LOAD_IMAGE_GRAYSCALE);
  // Some variables for the predicted label and associated confidence (e.g. distance):
  int predicted_label = -1;
  double predicted_confidence = 0.0;
  // Get the prediction and associated confidence from the model
  model->predict(img, predicted_label, predicted_confidence);
  ~~~~
cv::FaceRecognizer::save: |-
  Saves a :ocvFaceRecognizer and its model state.
  
  Every :ocvFaceRecognizer overwrites
  `FaceRecognizer::save(FileStorage& fs)` to save the internal model
  state. `FaceRecognizer::save(const String& filename)` saves the state of
  a model to the given filename.
  
  The suffix `const` means that prediction does not affect the internal
  model state, so the method can be safely called from within different
  threads.
cv::FaceRecognizer::load: "Loads a :ocvFaceRecognizer and its model state.\n\n\
  Loads a persisted model and state from a given XML or YAML file . Every\n\
  :ocvFaceRecognizer has to overwrite\n\
  `FaceRecognizer::load(FileStorage& fs)` to enable loading the model\n\
  state. `FaceRecognizer::load(FileStorage& fs)` in turn gets called by\n\
  `FaceRecognizer::load(const String& filename)`, to ease saving a model."
"cv::Notes:": "  Training and prediction must be done on grayscale images, use\n\
  :ocvcvtColor to convert between the color spaces.\n\n  **THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND\n\
  TEST IMAGES ARE OF EQUAL SIZE.** (caps-lock, because I got so many\n\
  mails asking for this). You have to make sure your input data has\n\
  the correct shape, else a meaningful exception is thrown. Use\n\
  :ocvresize to resize the images.\n\n  This model does not support updating.\n\n  Training and prediction must be done on grayscale images, use\n\
  :ocvcvtColor to convert between the color spaces.\n\n  **THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND\n\
  TEST IMAGES ARE OF EQUAL SIZE.** (caps-lock, because I got so many\n\
  mails asking for this). You have to make sure your input data has\n\
  the correct shape, else a meaningful exception is thrown. Use\n\
  :ocvresize to resize the images.\n\n  This model does not support updating.\n\n  The Circular Local Binary Patterns (used in training and prediction)\n\
  expect the data given as grayscale images, use :ocvcvtColor to\n\
  convert between the color spaces.\n\n  This model supports updating.\n"
"cv::Model internal data:": "  `num_components` see :ocvcreateEigenFaceRecognizer.\n\n  `threshold` see :ocvcreateEigenFaceRecognizer.\n\n  `eigenvalues` The eigenvalues for this Principal Component Analysis\n\
  (ordered descending).\n\n  `eigenvectors` The eigenvectors for this Principal Component\n\
  Analysis (ordered by their eigenvalue).\n\n  `mean` The sample mean calculated from the training data.\n\n  `projections` The projections of the training data.\n\n  `labels` The threshold applied in the prediction. If the distance to\n\
  the nearest neighbor is larger than the threshold, this method\n\
  returns -1.\n\n  `num_components` see :ocvcreateFisherFaceRecognizer.\n\n  `threshold` see :ocvcreateFisherFaceRecognizer.\n\n  `eigenvalues` The eigenvalues for this Linear Discriminant Analysis\n\
  (ordered descending).\n\n  `eigenvectors` The eigenvectors for this Linear Discriminant\n\
  Analysis (ordered by their eigenvalue).\n\n  `mean` The sample mean calculated from the training data.\n\n  `projections` The projections of the training data.\n\n  `labels` The labels corresponding to the projections.\n\n  `radius` see :ocvcreateLBPHFaceRecognizer.\n\n  `neighbors` see :ocvcreateLBPHFaceRecognizer.\n\n  `grid_x` see :ocvcreateLBPHFaceRecognizer.\n\n  `grid_y` see :ocvcreateLBPHFaceRecognizer.\n\n  `threshold` see :ocvcreateLBPHFaceRecognizer.\n\n  `histograms` Local Binary Patterns Histograms calculated from the\n\
  given training data (empty if none was given).\n\n  `labels` Labels corresponding to the calculated Local Binary\n\
  Patterns Histograms.\n"
cv::Release 0.05: |
  This library is now included in the official OpenCV distribution (from
  2.4 on). The :ocv:class`FaceRecognizer` is now an :ocvAlgorithm, which
  better fits into the overall OpenCV API.
  
  To reduce the confusion on user side and minimize my work, libfacerec
  and OpenCV have been synchronized and are now based on the same
  interfaces and implementation.
  
  The library now has an extensive documentation:
    The API is explained in detail and with a lot of code examples.
  
    The face recognition guide I had written for Python and GNU
  Octave/MATLAB has been adapted to the new OpenCV C++
  `cv::FaceRecognizer`.
  
    A tutorial for gender classification with Fisherfaces.
  
    A tutorial for face recognition in videos (e.g. webcam).

cv::Release highlights: "  There are no single highlights to pick from, this release is a\n\
  highlight itself.\n\n  A whole lot of exceptions with meaningful error messages.\n\n  A tutorial for Windows users:\n\
  [<http://bytefish.de/blog/opencv_visual_studio_and_libfacerec>](http://bytefish.de/blog/opencv_visual_studio_and_libfacerec)\n\n  New Unit Tests (for LBP Histograms) make the library more robust.\n\n  Added more documentation.\n\n  New Unit Tests (for LBP Histograms) make the library more robust.\n\n  Added a documentation and changelog in reStructuredText.\n\n  Colormaps for OpenCV to enhance the visualization.\n\n\n  Face Recognition algorithms implemented:\n  Eigenfaces [TP91]_\n\n  Fisherfaces [BHK97]_\n\n  Local Binary Patterns Histograms [AHP04]_\n\n\n\n  Added persistence facilities to store the models with a common API.\n\n\n  Unit Tests (using [gtest](http://code.google.com/p/googletest/)).\n\n\n  Providing a CMakeLists.txt to enable easy cross-platform building.\n"
cv::Release 0.04: |-
  This version is fully Windows-compatible and works with OpenCV 2.3.1.
  Several bugfixes, but none influenced the recognition rate.
cv::Release 0.03: |-
  Reworked the library to provide separate implementations in cpp files,
  because it's the preferred way of contributing OpenCV libraries. This
  means the library is not header-only anymore. Slight API changes were
  done, please see the documentation for details.
cv::Release 0.02: |-
  Reworked the library to provide separate implementations in cpp files,
  because it's the preferred way of contributing OpenCV libraries. This
  means the library is not header-only anymore. Slight API changes were
  done, please see the documentation for details.
cv::Release 0.01: Initial release as header-only library.
cv::Introduction: |-
  A lot of people interested in face recognition, also want to know how to
  perform image classification tasks like:
    Gender Classification (Gender Detection)
  
    Emotion Classification (Emotion Detection)
  
    Glasses Classification (Glasses Detection)
  
    ...
  
  
  This is has become very, very easy with the new :ocvFaceRecognizer
  class. In this tutorial I'll show you how to perform gender
  classification with OpenCV on a set of face images. You'll also learn
  how to align your images to enhance the recognition results. If you want
  to do emotion classification instead of gender classification, all you
  need to do is to update is your training data and the configuration you
  pass to the demo.
  
  Saving and loading a :ocvFaceRecognizer is very important. Training a
  FaceRecognizer can be a very time-intense task, plus it's often
  impossible to ship the whole face database to the user of your product.
  The task of saving and loading a FaceRecognizer is easy with
  :ocvFaceRecognizer. You only have to call :ocvFaceRecognizer::load for
  loading and :ocvFaceRecognizer::save for saving a :ocvFaceRecognizer.
  
  I'll adapt the Eigenfaces example from the ../facerec_tutorial: Imagine
  we want to learn the Eigenfaces of the [AT&T
  Facedatabase](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html),
  store the model to a YAML file and then load it again.
  
  From the loaded model, we'll get a prediction, show the mean, Eigenfaces
  and the image reconstruction.
  
  [OpenCV (Open Source Computer Vision)](http://opencv.org) is a popular
  computer vision library started by [Intel](http://www.intel.com) in
  1999. The cross-platform library sets its focus on real-time image
  processing and includes patent-free implementations of the latest
  computer vision algorithms. In 2008 [Willow
  Garage](http://www.willowgarage.com) took over support and OpenCV 2.3.1
  now comes with a programming interface to C, C++,
  [Python](http://www.python.org) and [Android](http://www.android.com).
  OpenCV is released under a BSD license so it is used in academic
  projects and commercial products alike.
  
  OpenCV 2.4 now comes with the very new :ocvFaceRecognizer class for face
  recognition, so you can start experimenting with face recognition right
  away. This document is the guide I've wished for, when I was working
  myself into face recognition. It shows you how to perform face
  recognition with :ocvFaceRecognizer in OpenCV (with full source code
  listings) and gives you an introduction into the algorithms behind. I'll
  also show how to create the visualizations you can find in many
  publications, because a lot of people asked for.
  
  The currently available algorithms are:
    Eigenfaces (see :ocvcreateEigenFaceRecognizer)
  
    Fisherfaces (see :ocvcreateFisherFaceRecognizer)
  
    Local Binary Patterns Histograms (see :ocvcreateLBPHFaceRecognizer)
  
  
  You don't need to copy and paste the source code examples from this
  page, because they are available in the `src` folder coming with this
  documentation. If you have built OpenCV with the samples turned on,
  chances are good you have them compiled already! Although it might be
  interesting for very advanced users, I've decided to leave the
  implementation details out as I am afraid they confuse new users.
  
  All code in this document is released under the [BSD
  license](http://www.opensource.org/licenses/bsd-license), so feel free
  to use it for your projects.
  
  Whenever you hear the term *face recognition*, you instantly think of
  surveillance in videos. So performing face recognition in videos (e.g.
  webcam) is one of the most requested features I have got. I have heard
  your cries, so here it is. An application, that shows you how to do face
  recognition in videos! For the face detection part we'll use the awesome
  :ocvCascadeClassifier and we'll use :ocvFaceRecognizer for face
  recognition. This example uses the Fisherfaces method for face
  recognition, because it is robust against large changes in illumination.
  
  Here is what the final application looks like. As you can see I am only
  writing the id of the recognized person above the detected face (by the
  way this id is Arnold Schwarzenegger for my data set):
  
  ![image](../img/tutorial/facerec_video/facerec_video.png%0A%20:align:%20center%0A%20:scale:%2070%)
  
  This demo is a basis for your research and it shows you how to implement
  face recognition in videos. You probably want to extend the application
  and make it more sophisticated: You could combine the id with the name,
  then show the confidence of the prediction, recognize the emotion... and
  and and. But before you send mails, asking what these Haar-Cascade thing
  is or what a CSV is: Make sure you have read the entire tutorial. It's
  all explained in here. If you just want to scroll down to the code,
  please note:
    The available Haar-Cascades for face detection are located in the
  `data` folder of your OpenCV installation! One of the available
  Haar-Cascades for face detection is for example
  `/path/to/opencv/data/haarcascades/haarcascade_frontalface_default.xml`.
  
  
  I encourage you to experiment with the application. Play around with the
  available :ocvFaceRecognizer implementations, try the available cascades
  in OpenCV and see if you can improve your results!
  
  Class which provides the main controls to the Gipsa/Listic labs human
  retina model. This is a non separable spatio-temporal filter modelling
  the two main retina information channels :
  
    foveal vision for detailled color vision : the parvocellular
  pathway.
  
  
    peripheral vision for sensitive transient signals detection (motion
  and events) : the magnocellular pathway.
  
  
  From a general point of view, this filter whitens the image spectrum and
  corrects luminance thanks to local adaptation. An other important
  property is its hability to filter out spatio-temporal noise while
  enhancing details. This model originates from Jeanny Herault work
  [Herault2010]_. It has been involved in Alexandre Benoit phd and his
  current research [Benoit2010]_ (he currently maintains this module
  within OpenCV). It includes the work of other Jeanny's phd student such
  as [Chaix2007]_ and the log polar transformations of Barthelemy Durette
  described in Jeanny's book.
  
  **NOTES :**
  
    For ease of use in computer vision applications, the two retina
  channels are applied homogeneously on all the input images. This
  does not follow the real retina topology but this can still be done
  using the log sampling capabilities proposed within the class.
  
  
    Extend the retina description and code use in the tutorial/contrib
  section for complementary explanations.
  
  
  OpenCV (Open Source Computer Vision Library: <http://opencv.org>) is an
  open-source BSD-licensed library that includes several hundreds of
  computer vision algorithms. The document describes the so-called OpenCV
  2.x API, which is essentially a C++ API, as opposite to the C-based
  OpenCV 1.x API. The latter is described in opencv1x.pdf.
  
  OpenCV has a modular structure, which means that the package includes
  several shared or static libraries. The following modules are available:
    **core** - a compact module defining basic data structures,
  including the dense multi-dimensional array `Mat` and basic
  functions used by all other modules.
  
    **imgproc** - an image processing module that includes linear and
  non-linear image filtering, geometrical image transformations
  (resize, affine and perspective warping, generic table-based
  remapping), color space conversion, histograms, and so on.
  
    **video** - a video analysis module that includes motion
  estimation, background subtraction, and object tracking
  algorithms.
  
    **calib3d** - basic multiple-view geometry algorithms, single and
  stereo camera calibration, object pose estimation, stereo
  correspondence algorithms, and elements of 3D reconstruction.
  
    **features2d** - salient feature detectors, descriptors, and
  descriptor matchers.
  
    **objdetect** - detection of objects and instances of the
  predefined classes (for example, faces, eyes, mugs, people, cars,
  and so on).
  
    **highgui** - an easy-to-use interface to video capturing, image
  and video codecs, as well as simple UI capabilities.
  
    **gpu** - GPU-accelerated algorithms from different OpenCV
  modules.
  
    ... some other helper modules, such as FLANN and Google test
  wrappers, Python bindings, and others.
  
  
  
  The further chapters of the document describe functionality of each
  module. But first, make sure to get familiar with the common API
  concepts used thoroughly in the library.
  
  The video stabilization module contains a set of functions and classes
  that can be used to solve the problem of video stabilization. There are
  a few methods implemented, most of them are descibed in the papers
  [OF06]_ and [G11]_. However, there are some extensions and deviations
  from the orginal paper methods.
cv::Prerequisites: |-
  For gender classification of faces, you'll need some images of male and
  female faces first. I've decided to search faces of celebrities using
  [Google Images](http://www.google.com/images) with the faces filter
  turned on (my god, they have great algorithms at
  [Google](http://www.google.com)!). My database has 8 male and 5 female
  subjects, each with 10 images. Here are the names, if you don't know who
  to search:
    Angelina Jolie
  
    Arnold Schwarzenegger
  
    Brad Pitt
  
    Emma Watson
  
    George Clooney
  
    Jennifer Lopez
  
    Johnny Depp
  
    Justin Timberlake
  
    Katy Perry
  
    Keanu Reeves
  
    Naomi Watts
  
    Patrick Stewart
  
    Tom Cruise
  
  
  Once you have acquired some images, you'll need to read them. In the
  demo application I have decided to read the images from a very simple
  CSV file. Why? Because it's the simplest platform-independent approach I
  can think of. However, if you know a simpler solution please ping me
  about it. Basically all the CSV file needs to contain are lines composed
  of a `filename` followed by a `;` followed by the `label` (as *integer
  number*), making up a line like this:
  
  ~~~~ {.sourceCode .none}
  /path/to/image.ext;0
  ~~~~
  
  Let's dissect the line. `/path/to/image.ext` is the path to an image,
  probably something like this if you are in Windows:
  `C:/faces/person0/image0.jpg`. Then there is the separator `;` and
  finally we assign a label `0` to the image. Think of the label as the
  subject (the person, the gender or whatever comes to your mind). In the
  gender classification scenario, the label is the gender the person has.
  I'll give the label `0` to *male* persons and the label `1` is for
  *female* subjects. So my CSV file looks like this:
  
  ~~~~ {.sourceCode .none}
  /home/philipp/facerec/data/gender/male/keanu_reeves/keanu_reeves_01.jpg;0
  /home/philipp/facerec/data/gender/male/keanu_reeves/keanu_reeves_02.jpg;0
  /home/philipp/facerec/data/gender/male/keanu_reeves/keanu_reeves_03.jpg;0
  ...
  /home/philipp/facerec/data/gender/female/katy_perry/katy_perry_01.jpg;1
  /home/philipp/facerec/data/gender/female/katy_perry/katy_perry_02.jpg;1
  /home/philipp/facerec/data/gender/female/katy_perry/katy_perry_03.jpg;1
  ...
  /home/philipp/facerec/data/gender/male/brad_pitt/brad_pitt_01.jpg;0
  /home/philipp/facerec/data/gender/male/brad_pitt/brad_pitt_02.jpg;0
  /home/philipp/facerec/data/gender/male/brad_pitt/brad_pitt_03.jpg;0
  ...
  /home/philipp/facerec/data/gender/female/emma_watson/emma_watson_08.jpg;1
  /home/philipp/facerec/data/gender/female/emma_watson/emma_watson_02.jpg;1
  /home/philipp/facerec/data/gender/female/emma_watson/emma_watson_03.jpg;1
  ~~~~
  
  All images for this example were chosen to have a frontal face
  perspective. They have been cropped, scaled and rotated to be aligned at
  the eyes, just like this set of George Clooney images:
  
  ![image](../img/tutorial/gender_classification/clooney_set.png%0A%20:align:%20center)
  
  You really don't want to create the CSV file by hand. And you really
  don't want scale, rotate & translate the images manually. I have
  prepared you two Python scripts `create_csv.py` and `crop_face.py`, you
  can find them in the `src` folder coming with this documentation. You'll
  see how to use them in the appendixfgc.
  
  You want to do face recognition, so you need some face images to learn a
  :ocvFaceRecognizer on. I have decided to reuse the images from the
  gender classification example: facerec_gender_classification.
  
  I have the following celebrities in my training data set:
    Angelina Jolie
  
    Arnold Schwarzenegger
  
    Brad Pitt
  
    George Clooney
  
    Johnny Depp
  
    Justin Timberlake
  
    Katy Perry
  
    Keanu Reeves
  
    Patrick Stewart
  
    Tom Cruise
  
  
  In the demo I have decided to read the images from a very simple CSV
  file. Why? Because it's the simplest platform-independent approach I can
  think of. However, if you know a simpler solution please ping me about
  it. Basically all the CSV file needs to contain are lines composed of a
  `filename` followed by a `;` followed by the `label` (as *integer
  number*), making up a line like this:
  
  ~~~~ {.sourceCode .none}
  /path/to/image.ext;0
  ~~~~
  
  Let's dissect the line. `/path/to/image.ext` is the path to an image,
  probably something like this if you are in Windows:
  `C:/faces/person0/image0.jpg`. Then there is the separator `;` and
  finally we assign a label `0` to the image. Think of the label as the
  subject (the person, the gender or whatever comes to your mind). In the
  face recognition scenario, the label is the person this image belongs
  to. In the gender classification scenario, the label is the gender the
  person has. So my CSV file looks like this:
  
  ~~~~ {.sourceCode .none}
  /home/philipp/facerec/data/c/keanu_reeves/keanu_reeves_01.jpg;0
  /home/philipp/facerec/data/c/keanu_reeves/keanu_reeves_02.jpg;0
  /home/philipp/facerec/data/c/keanu_reeves/keanu_reeves_03.jpg;0
  ...
  /home/philipp/facerec/data/c/katy_perry/katy_perry_01.jpg;1
  /home/philipp/facerec/data/c/katy_perry/katy_perry_02.jpg;1
  /home/philipp/facerec/data/c/katy_perry/katy_perry_03.jpg;1
  ...
  /home/philipp/facerec/data/c/brad_pitt/brad_pitt_01.jpg;2
  /home/philipp/facerec/data/c/brad_pitt/brad_pitt_02.jpg;2
  /home/philipp/facerec/data/c/brad_pitt/brad_pitt_03.jpg;2
  ...
  /home/philipp/facerec/data/c1/crop_arnold_schwarzenegger/crop_08.jpg;6
  /home/philipp/facerec/data/c1/crop_arnold_schwarzenegger/crop_05.jpg;6
  /home/philipp/facerec/data/c1/crop_arnold_schwarzenegger/crop_02.jpg;6
  /home/philipp/facerec/data/c1/crop_arnold_schwarzenegger/crop_03.jpg;6
  ~~~~
  
  All images for this example were chosen to have a frontal face
  perspective. They have been cropped, scaled and rotated to be aligned at
  the eyes, just like this set of George Clooney images:
  
  ![image](../img/tutorial/gender_classification/clooney_set.png%0A%20:align:%20center)
cv::Fisherfaces for Gender Classification: |-
  If you want to decide wether a person is *male* or *female*, you have to
  learn the discriminative features of both classes. The Eigenfaces method
  is based on the Principal Component Analysis, which is an unsupervised
  statistical model and not suitable for this task. Please see the Face
  Recognition tutorial for insights into the algorithms. The Fisherfaces
  instead yields a class-specific linear projection, so it is much better
  suited for the gender classification task.
  [<http://www.bytefish.de/blog/gender_classification>](http://www.bytefish.de/blog/gender_classification)
  shows the recognition rate of the Fisherfaces method for gender
  classification.
  
  The Fisherfaces method achieves a 98% recognition rate in a
  subject-independent cross-validation. A subject-independent
  cross-validation means *images of the person under test are never used
  for learning the model*. And could you believe it: you can simply use
  the facerec_fisherfaces demo, that's inlcuded in OpenCV.
cv::Fisherfaces in OpenCV: |-
  The source code for this demo application is also available in the `src`
  folder coming with this documentation:
    src/facerec_fisherfaces.cpp <../src/facerec_fisherfaces.cpp>
  
  
  The source code for this demo application is also available in the `src`
  folder coming with this documentation:
    src/facerec_fisherfaces.cpp <src/facerec_fisherfaces.cpp>
  
  
  For this example I am going to use the Yale Facedatabase A, just because
  the plots are nicer. Each Fisherface has the same length as an original
  image, thus it can be displayed as an image. The demo shows (or saves)
  the first, at most 16 Fisherfaces:
  
  ![image](img/fisherfaces_opencv.png%0A%20:align:%20center)
  
  The Fisherfaces method learns a class-specific transformation matrix, so
  the they do not capture illumination as obviously as the Eigenfaces
  method. The Discriminant Analysis instead finds the facial features to
  discriminate between the persons. It's important to mention, that the
  performance of the Fisherfaces heavily depends on the input data as
  well. Practically said: if you learn the Fisherfaces for
  well-illuminated pictures only and you try to recognize faces in
  bad-illuminated scenes, then method is likely to find the wrong
  components (just because those features may not be predominant on bad
  illuminated images). This is somewhat logical, since the method had no
  chance to learn the illumination.
  
  The Fisherfaces allow a reconstruction of the projected image, just like
  the Eigenfaces did. But since we only identified the features to
  distinguish between subjects, you can't expect a nice reconstruction of
  the original image. For the Fisherfaces method we'll project the sample
  image onto each of the Fisherfaces instead. So you'll have a nice
  visualization, which feature each of the Fisherfaces describes:
  
  ~~~~ {.sourceCode .cpp}
  // Display or save the image reconstruction at some predefined steps:
  for(int num_component = 0; num_component < min(16, W.cols); num_component++) {
      // Slice the Fisherface from the model:
      Mat ev = W.col(num_component);
      Mat projection = subspaceProject(ev, mean, images[0].reshape(1,1));
      Mat reconstruction = subspaceReconstruct(ev, mean, projection);
      // Normalize the result:
      reconstruction = norm_0_255(reconstruction.reshape(1, images[0].rows));
      // Display or save:
      if(argc == 2) {
          imshow(format("fisherface_reconstruction_%d", num_component), reconstruction);
      } else {
          imwrite(format("%s/fisherface_reconstruction_%d.png", output_folder.c_str(), num_component), reconstruction);
      }
  }
  ~~~~
  
  The differences may be subtle for the human eyes, but you should be able
  to see some differences:
  
  ![image](img/fisherface_reconstruction_opencv.png%0A%20:align:%20center)
cv::Running the Demo: |-
  If you are in Windows, then simply start the demo by running (from
  command line):
  
  ~~~~ {.sourceCode .none}
  facerec_fisherfaces.exe C:/path/to/your/csv.ext
  ~~~~
  
  If you are in Linux, then simply start the demo by running:
  
  ~~~~ {.sourceCode .none}
  ./facerec_fisherfaces /path/to/your/csv.ext
  ~~~~
  
  If you don't want to display the images, but save them, then pass the
  desired path to the demo. It works like this in Windows:
  
  ~~~~ {.sourceCode .none}
  facerec_fisherfaces.exe C:/path/to/your/csv.ext C:/path/to/store/results/at
  ~~~~
  
  And in Linux:
  
  ~~~~ {.sourceCode .none}
  ./facerec_fisherfaces /path/to/your/csv.ext /path/to/store/results/at
  ~~~~
  
  You'll need:
    The path to a valid Haar-Cascade for detecting a face with a
  :ocvCascadeClassifier.
  
    The path to a valid CSV File for learning a :ocvFaceRecognizer.
  
    A webcam and its device id (you don't know the device id? Simply
  start from 0 on and see what happens).
  
  
  If you are in Windows, then simply start the demo by running (from
  command line):
  
  ~~~~ {.sourceCode .none}
  facerec_video.exe <C:/path/to/your/haar_cascade.xml> <C:/path/to/your/csv.ext> <video device>
  ~~~~
  
  If you are in Linux, then simply start the demo by running:
  
  ~~~~ {.sourceCode .none}
  ./facerec_video </path/to/your/haar_cascade.xml> </path/to/your/csv.ext> <video device>
  ~~~~
  
  An example. If the haar-cascade is at
  `C:/opencv/data/haarcascades/haarcascade_frontalface_default.xml`, the
  CSV file is at `C:/facerec/data/celebrities.txt` and I have a webcam
  with deviceId `1`, then I would call the demo with:
  
  ~~~~ {.sourceCode .none}
  facerec_video.exe C:/opencv/data/haarcascades/haarcascade_frontalface_default.xml C:/facerec/data/celebrities.txt 1
  ~~~~
  
  That's it.
cv::Results: |-
  If you run the program with your CSV file as parameter, you'll see the
  Fisherface that separates between male and female images. I've decided
  to apply a Jet colormap in this demo, so you can see which features the
  method identifies:
  
  ![image](../img/tutorial/gender_classification/fisherface_0.png)
  
  The demo also shows the average face of the male and female training
  images you have passed:
  
  ![image](../img/tutorial/gender_classification/mean.png)
  
  Moreover it the demo should yield the prediction for the correct gender:
  
  ~~~~ {.sourceCode .none}
  Predicted class = 1 / Actual class = 1.
  ~~~~
  
  And for advanced users I have also shown the Eigenvalue for the
  Fisherface:
  
  ~~~~ {.sourceCode .none}
  Eigenvalue #0 = 152.49493
  ~~~~
  
  And the Fisherfaces reconstruction:
  
  ![image](../img/tutorial/gender_classification/fisherface_reconstruction_0.png)
  
  I hope this gives you an idea how to approach gender classification and
  the other image classification tasks.
  
  `eigenfaces_at.yml` then contains the model state, we'll simply look at
  the first 10 lines with `head eigenfaces_at.yml`:
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/github/libfacerec-build$ head eigenfaces_at.yml
  %YAML:1.0
  num_components: 399
  mean: !!opencv-matrix
     rows: 1
     cols: 10304
     dt: d
     data: [ 8.5558897243107765e+01, 8.5511278195488714e+01,
         8.5854636591478695e+01, 8.5796992481203006e+01,
         8.5952380952380949e+01, 8.6162907268170414e+01,
         8.6082706766917283e+01, 8.5776942355889716e+01,
  ~~~~
  
  And here is the Reconstruction, which is the same as the original:
  
  ![image](../img/eigenface_reconstruction_opencv.png%0A%20:align:%20center)
  
  Enjoy!
cv::Creating the CSV File: |-
  You don't really want to create the CSV file by hand. I have prepared
  you a little Python script `create_csv.py` (you find it at
  `/src/create_csv.py` coming with this tutorial) that automatically
  creates you a CSV file. If you have your images in hierarchie like this
  (`/basepath/<subject>/<image.ext>`):
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data/at$ tree
  .
  |-- s1
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  |-- s2
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ...
  |-- s40
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ~~~~
  
  Then simply call `create_csv.py` with the path to the folder, just like
  this and you could save the output:
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data$ python create_csv.py
  at/s13/2.pgm;0
  at/s13/7.pgm;0
  at/s13/6.pgm;0
  at/s13/9.pgm;0
  at/s13/5.pgm;0
  at/s13/3.pgm;0
  at/s13/4.pgm;0
  at/s13/10.pgm;0
  at/s13/8.pgm;0
  at/s13/1.pgm;0
  at/s17/2.pgm;1
  at/s17/7.pgm;1
  at/s17/6.pgm;1
  at/s17/9.pgm;1
  at/s17/5.pgm;1
  at/s17/3.pgm;1
  [...]
  ~~~~
  
  Here is the script, if you can't find it:
  
  You don't really want to create the CSV file by hand. I have prepared
  you a little Python script `create_csv.py` (you find it at
  `src/create_csv.py` coming with this tutorial) that automatically
  creates you a CSV file. If you have your images in hierarchie like this
  (`/basepath/<subject>/<image.ext>`):
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data/at$ tree
  .
  |-- s1
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  |-- s2
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ...
  |-- s40
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ~~~~
  
  Then simply call create_csv.py with the path to the folder, just like
  this and you could save the output:
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data$ python create_csv.py
  at/s13/2.pgm;0
  at/s13/7.pgm;0
  at/s13/6.pgm;0
  at/s13/9.pgm;0
  at/s13/5.pgm;0
  at/s13/3.pgm;0
  at/s13/4.pgm;0
  at/s13/10.pgm;0
  at/s13/8.pgm;0
  at/s13/1.pgm;0
  at/s17/2.pgm;1
  at/s17/7.pgm;1
  at/s17/6.pgm;1
  at/s17/9.pgm;1
  at/s17/5.pgm;1
  at/s17/3.pgm;1
  [...]
  ~~~~
  
  Please see the appendixft for additional informations.
  
  You don't really want to create the CSV file by hand. I have prepared
  you a little Python script `create_csv.py` (you find it at
  `/src/create_csv.py` coming with this tutorial) that automatically
  creates you a CSV file. If you have your images in hierarchie like this
  (`/basepath/<subject>/<image.ext>`):
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data/at$ tree
  .
  |-- s1
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  |-- s2
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ...
  |-- s40
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ~~~~
  
  Then simply call `create_csv.py` with the path to the folder, just like
  this and you could save the output:
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data$ python create_csv.py
  at/s13/2.pgm;0
  at/s13/7.pgm;0
  at/s13/6.pgm;0
  at/s13/9.pgm;0
  at/s13/5.pgm;0
  at/s13/3.pgm;0
  at/s13/4.pgm;0
  at/s13/10.pgm;0
  at/s13/8.pgm;0
  at/s13/1.pgm;0
  at/s17/2.pgm;1
  at/s17/7.pgm;1
  at/s17/6.pgm;1
  at/s17/9.pgm;1
  at/s17/5.pgm;1
  at/s17/3.pgm;1
  [...]
  ~~~~
  
  Here is the script, if you can't find it:
  
  You don't really want to create the CSV file by hand. I have prepared
  you a little Python script `create_csv.py` (you find it at
  `/src/create_csv.py` coming with this tutorial) that automatically
  creates you a CSV file. If you have your images in hierarchie like this
  (`/basepath/<subject>/<image.ext>`):
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data/at$ tree
  .
  |-- s1
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  |-- s2
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ...
  |-- s40
  |   |-- 1.pgm
  |   |-- ...
  |   |-- 10.pgm
  ~~~~
  
  Then simply call `create_csv.py` with the path to the folder, just like
  this and you could save the output:
  
  ~~~~ {.sourceCode .none}
  philipp@mango:~/facerec/data$ python create_csv.py
  at/s13/2.pgm;0
  at/s13/7.pgm;0
  at/s13/6.pgm;0
  at/s13/9.pgm;0
  at/s13/5.pgm;0
  at/s13/3.pgm;0
  at/s13/4.pgm;0
  at/s13/10.pgm;0
  at/s13/8.pgm;0
  at/s13/1.pgm;0
  at/s17/2.pgm;1
  at/s17/7.pgm;1
  at/s17/6.pgm;1
  at/s17/9.pgm;1
  at/s17/5.pgm;1
  at/s17/3.pgm;1
  [...]
  ~~~~
  
  Here is the script, if you can't find it:
cv::Aligning Face Images: |-
  An accurate alignment of your image data is especially important in
  tasks like emotion detection, were you need as much detail as possible.
  Believe me... You don't want to do this by hand. So I've prepared you a
  tiny Python script. The code is really easy to use. To scale, rotate and
  crop the face image you just need to call *CropFace(image, eye_left,
  eye_right, offset_pct, dest_sz)*, where:
    *eye_left* is the position of the left eye
  
    *eye_right* is the position of the right eye
  
    *offset_pct* is the percent of the image you want to keep next to
  the eyes (horizontal, vertical direction)
  
    *dest_sz* is the size of the output image
  
  
  If you are using the same *offset_pct* and *dest_sz* for your images,
  they are all aligned at the eyes.
  
  Imagine we are given [this photo of Arnold
  Schwarzenegger](http://en.wikipedia.org/wiki/File:Arnold_Schwarzenegger_edit%28ws%29.jpg),
  which is under a Public Domain license. The (x,y)-position of the eyes
  is approximately *(252,364)* for the left and *(420,366)* for the right
  eye. Now you only need to define the horizontal offset, vertical offset
  and the size your scaled, rotated & cropped face should have.
  
  Here are some examples:
  
    Configuration          Cropped, Scaled, Rotated Face
  
    0.1 (10%), 0.1 (10%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_10_10_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_20_20_200_200.jpg)
  
    0.3 (30%), 0.3 (30%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_30_30_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](../img/tutorial/gender_classification/ar
    (70,70)                nie_20_20_70_70.jpg)
  
  An accurate alignment of your image data is especially important in
  tasks like emotion detection, were you need as much detail as possible.
  Believe me... You don't want to do this by hand. So I've prepared you a
  tiny Python script. The code is really easy to use. To scale, rotate and
  crop the face image you just need to call *CropFace(image, eye_left,
  eye_right, offset_pct, dest_sz)*, where:
    *eye_left* is the position of the left eye
  
    *eye_right* is the position of the right eye
  
    *offset_pct* is the percent of the image you want to keep next to
  the eyes (horizontal, vertical direction)
  
    *dest_sz* is the size of the output image
  
  
  If you are using the same *offset_pct* and *dest_sz* for your images,
  they are all aligned at the eyes.
  
  Imagine we are given [this photo of Arnold
  Schwarzenegger](http://en.wikipedia.org/wiki/File:Arnold_Schwarzenegger_edit%28ws%29.jpg),
  which is under a Public Domain license. The (x,y)-position of the eyes
  is approximately *(252,364)* for the left and *(420,366)* for the right
  eye. Now you only need to define the horizontal offset, vertical offset
  and the size your scaled, rotated & cropped face should have.
  
  Here are some examples:
  
    Configuration          Cropped, Scaled, Rotated Face
  
    0.1 (10%), 0.1 (10%),  ![image](./img/tutorial/gender_classification/arn
    (200,200)              ie_10_10_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](./img/tutorial/gender_classification/arn
    (200,200)              ie_20_20_200_200.jpg)
  
    0.3 (30%), 0.3 (30%),  ![image](./img/tutorial/gender_classification/arn
    (200,200)              ie_30_30_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](./img/tutorial/gender_classification/arn
    (70,70)                ie_20_20_70_70.jpg)
  
  An accurate alignment of your image data is especially important in
  tasks like emotion detection, were you need as much detail as possible.
  Believe me... You don't want to do this by hand. So I've prepared you a
  tiny Python script. The code is really easy to use. To scale, rotate and
  crop the face image you just need to call *CropFace(image, eye_left,
  eye_right, offset_pct, dest_sz)*, where:
    *eye_left* is the position of the left eye
  
    *eye_right* is the position of the right eye
  
    *offset_pct* is the percent of the image you want to keep next to
  the eyes (horizontal, vertical direction)
  
    *dest_sz* is the size of the output image
  
  
  If you are using the same *offset_pct* and *dest_sz* for your images,
  they are all aligned at the eyes.
  
  Imagine we are given [this photo of Arnold
  Schwarzenegger](http://en.wikipedia.org/wiki/File:Arnold_Schwarzenegger_edit%28ws%29.jpg),
  which is under a Public Domain license. The (x,y)-position of the eyes
  is approximately *(252,364)* for the left and *(420,366)* for the right
  eye. Now you only need to define the horizontal offset, vertical offset
  and the size your scaled, rotated & cropped face should have.
  
  Here are some examples:
  
    Configuration          Cropped, Scaled, Rotated Face
  
    0.1 (10%), 0.1 (10%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_10_10_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_20_20_200_200.jpg)
  
    0.3 (30%), 0.3 (30%),  ![image](../img/tutorial/gender_classification/ar
    (200,200)              nie_30_30_200_200.jpg)
  
    0.2 (20%), 0.2 (20%),  ![image](../img/tutorial/gender_classification/ar
    (70,70)                nie_20_20_70_70.jpg)
cv::Using FaceRecognizer::save and FaceRecognizer::load: |
  The source code for this demo application is also available in the `src`
  folder coming with this documentation:
    src/facerec_save_load.cpp <../src/facerec_save_load.cpp>

cv::Face Recognition: |-
  Face recognition is an easy task for humans. Experiments in [Tu06]_
  have shown, that even one to three day old babies are able to
  distinguish between known faces. So how hard could it be for a computer?
  It turns out we know little about human recognition to date. Are inner
  features (eyes, nose, mouth) or outer features (head shape, hairline)
  used for a successful face recognition? How do we analyze an image and
  how does the brain encode it? It was shown by [David
  Hubel](http://en.wikipedia.org/wiki/David_H._Hubel) and [Torsten
  Wiesel](http://en.wikipedia.org/wiki/Torsten_Wiesel), that our brain has
  specialized nerve cells responding to specific local features of a
  scene, such as lines, edges, angles or movement. Since we don't see the
  world as scattered pieces, our visual cortex must somehow combine the
  different sources of information into useful patterns. Automatic face
  recognition is all about extracting those meaningful features from an
  image, putting them into a useful representation and performing some
  kind of classification on them.
  
  Face recognition based on the geometric features of a face is probably
  the most intuitive approach to face recognition. One of the first
  automated face recognition systems was described in [Kanade73]_: marker
  points (position of eyes, ears, nose, ...) were used to build a feature
  vector (distance between the points, angle between them, ...). The
  recognition was performed by calculating the euclidean distance between
  feature vectors of a probe and reference image. Such a method is robust
  against changes in illumination by its nature, but has a huge drawback:
  the accurate registration of the marker points is complicated, even with
  state of the art algorithms. Some of the latest work on geometric face
  recognition was carried out in [Bru92]_. A 22-dimensional feature
  vector was used and experiments on large datasets have shown, that
  geometrical features alone my not carry enough information for face
  recognition.
  
  The Eigenfaces method described in [TP91]_ took a holistic approach to
  face recognition: A facial image is a point from a high-dimensional
  image space and a lower-dimensional representation is found, where
  classification becomes easy. The lower-dimensional subspace is found
  with Principal Component Analysis, which identifies the axes with
  maximum variance. While this kind of transformation is optimal from a
  reconstruction standpoint, it doesn't take any class labels into
  account. Imagine a situation where the variance is generated from
  external sources, let it be light. The axes with maximum variance do not
  necessarily contain any discriminative information at all, hence a
  classification becomes impossible. So a class-specific projection with a
  Linear Discriminant Analysis was applied to face recognition in
  [BHK97]_. The basic idea is to minimize the variance within a class,
  while maximizing the variance between the classes at the same time.
  
  Recently various methods for a local feature extraction emerged. To
  avoid the high-dimensionality of the input data only local regions of an
  image are described, the extracted features are (hopefully) more robust
  against partial occlusion, illumation and small sample size. Algorithms
  used for a local feature extraction are Gabor Wavelets ([Wiskott97]_),
  Discrete Cosinus Transform ([Messer06]_) and Local Binary Patterns
  ([AHP04]_). It's still an open research question what's the best way to
  preserve spatial information when applying a local feature extraction,
  because spatial information is potentially useful information.
cv::Face Database: |
  Let's get some data to experiment with first. I don't want to do a toy
  example here. We are doing face recognition, so you'll need some face
  images! You can either create your own dataset or start with one of the
  available face databases,
  [<http://face-rec.org/databases/>](http://face-rec.org/databases) gives
  you an up-to-date overview. Three interesting databases are (parts of
  the description are quoted from
  [<http://face-rec.org>](http://face-rec.org)):
  
    [AT&T
  Facedatabase](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)
  The AT&T Facedatabase, sometimes also referred to as *ORL Database
  of Faces*, contains ten different images of each of 40 distinct
  subjects. For some subjects, the images were taken at different
  times, varying the lighting, facial expressions (open / closed eyes,
  smiling / not smiling) and facial details (glasses / no glasses).
  All the images were taken against a dark homogeneous background with
  the subjects in an upright, frontal position (with tolerance for
  some side movement).
  
  
    [Yale Facedatabase
  A](http://vision.ucsd.edu/content/yale-face-database), also known as
  Yalefaces. The AT&T Facedatabase is good for initial tests, but it's
  a fairly easy database. The Eigenfaces method already has a 97%
  recognition rate on it, so you won't see any great improvements with
  other algorithms. The Yale Facedatabase A (also known as Yalefaces)
  is a more appropriate dataset for initial experiments, because the
  recognition problem is harder. The database consists of 15 people
  (14 male, 1 female) each with 11 grayscale images sized
  $320 \times 243$ pixel. There are changes in the light conditions
  (center light, left light, right light), facial expressions (happy,
  normal, sad, sleepy, surprised, wink) and glasses (glasses,
  no-glasses).
  
  The original images are not cropped and aligned. Please look into
  the appendixft for a Python script, that does the job for you.
  
  
    [Extended Yale Facedatabase
  B](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html) The
  Extended Yale Facedatabase B contains 2414 images of 38 different
  people in its cropped version. The focus of this database is set on
  extracting features that are robust to illumination, the images have
  almost no variation in emotion/occlusion/... . I personally think,
  that this dataset is too large for the experiments I perform in this
  document. You better use the [AT&T
  Facedatabase](http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html)
  for intial testing. A first version of the Yale Facedatabase B was
  used in [BHK97]_ to see how the Eigenfaces and Fisherfaces method
  perform under heavy illumination changes. [Lee05]_ used the same
  setup to take 16128 images of 28 people. The Extended Yale
  Facedatabase B is the merge of the two databases, which is now known
  as Extended Yalefacedatabase B.

cv::Preparing the data: |-
  Once we have acquired some data, we'll need to read it in our program.
  In the demo applications I have decided to read the images from a very
  simple CSV file. Why? Because it's the simplest platform-independent
  approach I can think of. However, if you know a simpler solution please
  ping me about it. Basically all the CSV file needs to contain are lines
  composed of a `filename` followed by a `;` followed by the `label` (as
  *integer number*), making up a line like this:
  
  ~~~~ {.sourceCode .none}
  /path/to/image.ext;0
  ~~~~
  
  Let's dissect the line. `/path/to/image.ext` is the path to an image,
  probably something like this if you are in Windows:
  `C:/faces/person0/image0.jpg`. Then there is the separator `;` and
  finally we assign the label `0` to the image. Think of the label as the
  subject (the person) this image belongs to, so same subjects (persons)
  should have the same label.
  
  Download the AT&T Facedatabase from AT&T Facedatabase and the
  corresponding CSV file from at.txt, which looks like this (file is
  without ... of course):
  
  ~~~~ {.sourceCode .none}
  ./at/s1/1.pgm;0
  ./at/s1/2.pgm;0
  ...
  ./at/s2/1.pgm;1
  ./at/s2/2.pgm;1
  ...
  ./at/s40/1.pgm;39
  ./at/s40/2.pgm;39
  ~~~~
  
  Imagine I have extracted the files to `D:/data/at` and have downloaded
  the CSV file to `D:/data/at.txt`. Then you would simply need to Search &
  Replace `./` with `D:/data/`. You can do that in an editor of your
  choice, every sufficiently advanced editor can do this. Once you have a
  CSV file with valid filenames and labels, you can run any of the demos
  by passing the path to the CSV file as parameter:
  
  ~~~~ {.sourceCode .none}
  facerec_demo.exe D:/data/at.txt
  ~~~~
cv::Eigenfaces: |-
  The problem with the image representation we are given is its high
  dimensionality. Two-dimensional $p \times q$ grayscale images span a
  $m = pq$-dimensional vector space, so an image with $100 \times 100$
  pixels lies in a $10,000$-dimensional image space already. The question
  is: Are all dimensions equally useful for us? We can only make a
  decision if there's any variance in data, so what we are looking for are
  the components that account for most of the information. The Principal
  Component Analysis (PCA) was independently proposed by [Karl
  Pearson](http://en.wikipedia.org/wiki/Karl_Pearson) (1901) and [Harold
  Hotelling](http://en.wikipedia.org/wiki/Harold_Hotelling) (1933) to turn
  a set of possibly correlated variables into a smaller set of
  uncorrelated variables. The idea is, that a high-dimensional dataset is
  often described by correlated variables and therefore only a few
  meaningful dimensions account for most of the information. The PCA
  method finds the directions with the greatest variance in the data,
  called principal components.
cv::Algorithmic Description: |-
  Let $X = { x_{1}, x_{2}, \ldots, x_{n} }$ be a random vector with
  observations $x_i \in R^{d}$.
   Compute the mean $\mu$
  
  
  $$\mu = \frac{1}{n} \sum_{i=1}^{n} x_{i}$$
  
   Compute the the Covariance Matrix S
  
  
  $$S = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - \mu) (x_{i} - \mu)^{T}`$$
  
   Compute the eigenvalues $\lambda_{i}$ and eigenvectors $v_{i}$ of
  $S$
  
  
  $$S v_{i} = \lambda_{i} v_{i}, i=1,2,\ldots,n$$
  
   Order the eigenvectors descending by their eigenvalue. The $k$
  principal components are the eigenvectors corresponding to the $k$
  largest eigenvalues.
  
  
  The $k$ principal components of the observed vector $x$ are then given
  by:
  
  $$y = W^{T} (x - \mu)$$
  
  where $W = (v_{1}, v_{2}, \ldots, v_{k})$.
  
  The reconstruction from the PCA basis is given by:
  
  $$x = W y + \mu$$
  
  where $W = (v_{1}, v_{2}, \ldots, v_{k})$.
  
  The Eigenfaces method then performs face recognition by:
    Projecting all training samples into the PCA subspace.
  
    Projecting the query image into the PCA subspace.
  
    Finding the nearest neighbor between the projected training images
  and the projected query image.
  
  
  Still there's one problem left to solve. Imagine we are given $400$
  images sized $100 \times 100$ pixel. The Principal Component Analysis
  solves the covariance matrix $S = X X^{T}$, where
  ${size}(X) = 10000 \times 400$ in our example. You would end up with a
  $10000 \times 10000$ matrix, roughly $0.8 GB$. Solving this problem
  isn't feasible, so we'll need to apply a trick. From your linear algebra
  lessons you know that a $M \times N$ matrix with $M > N$ can only have
  $N - 1$ non-zero eigenvalues. So it's possible to take the eigenvalue
  decomposition $S = X^{T} X$ of size $N \times N$ instead:
  
  $$X^{T} X v_{i} = \lambda_{i} v{i}$$
  
  and get the original eigenvectors of $S = X X^{T}$ with a left
  multiplication of the data matrix:
  
  $$X X^{T} (X v_{i}) = \lambda_{i} (X v_{i})$$
  
  The resulting eigenvectors are orthogonal, to get orthonormal
  eigenvectors they need to be normalized to unit length. I don't want to
  turn this into a publication, so please look into [Duda01]_ for the
  derivation and proof of the equations.
  
  Let $X$ be a random vector with samples drawn from $c$ classes:
  
  $$:nowrap:$$$$\begin{align*}
      X & = & {X_1,X_2,\ldots,X_c} \
      X_i & = & {x_1, x_2, \ldots, x_n}
  \end{align*}$$
  
  The scatter matrices $S_{B}$ and S_{W} are calculated as:
  
  $$:nowrap:$$$$\begin{align*}
      S_{B} & = & \sum_{i=1}^{c} N_{i} (\mu_i - \mu)(\mu_i - \mu)^{T} \
      S_{W} & = & \sum_{i=1}^{c} \sum_{x_{j} \in X_{i}} (x_j - \mu_i)(x_j - \mu_i)^{T}
  \end{align*}$$
  
  , where $\mu$ is the total mean:
  
  $$\mu = \frac{1}{N} \sum_{i=1}^{N} x_i$$
  
  And $\mu_i$ is the mean of class $i \in {1,\ldots,c}$:
  
  $$\mu_i = \frac{1}{|X_i|} \sum_{x_j \in X_i} x_j$$
  
  Fisher's classic algorithm now looks for a projection $W$, that
  maximizes the class separability criterion:
  
  $$W_{opt} = \operatorname{arg\,max}_{W} \frac{|W^T S_B W|}{|W^T S_W W|}$$
  
  Following [BHK97]_, a solution for this optimization problem is given
  by solving the General Eigenvalue Problem:
  
  $$:nowrap:$$$$\begin{align*}
      S_{B} v_{i} & = & \lambda_{i} S_w v_{i} \nonumber \
      S_{W}^{-1} S_{B} v_{i} & = & \lambda_{i} v_{i}
  \end{align*}$$
  
  There's one problem left to solve: The rank of $S_{W}$ is at most
  $(N-c)$, with $N$ samples and $c$ classes. In pattern recognition
  problems the number of samples $N$ is almost always samller than the
  dimension of the input data (the number of pixels), so the scatter
  matrix $S_{W}$ becomes singular (see [RJ91]_). In [BHK97]_ this was
  solved by performing a Principal Component Analysis on the data and
  projecting the samples into the $(N-c)$-dimensional space. A Linear
  Discriminant Analysis was then performed on the reduced data, because
  $S_{W}$ isn't singular anymore.
  
  The optimization problem can then be rewritten as:
  
  $$:nowrap:$$$$\begin{align*}
      W_{pca} & = & \operatorname{arg\,max}_{W} |W^T S_T W| \
      W_{fld} & = & \operatorname{arg\,max}_{W} \frac{|W^T W_{pca}^T S_{B} W_{pca} W|}{|W^T W_{pca}^T S_{W} W_{pca} W|}
  \end{align*}$$
  
  The transformation matrix $W$, that projects a sample into the
  $(c-1)$-dimensional space is then given by:
  
  $$W = W_{fld}^{T} W_{pca}^{T}$$
  
  A more formal description of the LBP operator can be given as:
  
  $$LBP(x_c, y_c) = \sum_{p=0}^{P-1} 2^p s(i_p - i_c)$$
  
  , with $(x_c, y_c)$ as central pixel with intensity $i_c$; and $i_n$
  being the intensity of the the neighbor pixel. $s$ is the sign function
  defined as:
  
  $$:nowrap:$$$$\begin{equation}
  s(x) =
  \begin{cases}
  1 & \text{if $x \geq 0$}\
  0 & \text{else}
  \end{cases}
  \end{equation}$$
  
  This description enables you to capture very fine grained details in
  images. In fact the authors were able to compete with state of the art
  results for texture classification. Soon after the operator was
  published it was noted, that a fixed neighborhood fails to encode
  details differing in scale. So the operator was extended to use a
  variable neighborhood in [AHP04]_. The idea is to align an abritrary
  number of neighbors on a circle with a variable radius, which enables to
  capture the following neighborhoods:
  
  ![image](img/lbp/patterns.png%0A%20:scale:%2080%%0A%20:align:%20center)
  
  For a given Point $(x_c,y_c)$ the position of the neighbor
  $(x_p,y_p), p \in P$ can be calculated by:
  
  $$:nowrap:$$$$\begin{align*}
  x_{p} & = & x_c + R \cos({\frac{2\pi p}{P}})\
  y_{p} & = & y_c - R \sin({\frac{2\pi p}{P}})
  \end{align*}$$
  
  Where $R$ is the radius of the circle and $P$ is the number of sample
  points.
  
  The operator is an extension to the original LBP codes, so it's
  sometimes called *Extended LBP* (also referred to as *Circular LBP*) .
  If a points coordinate on the circle doesn't correspond to image
  coordinates, the point get's interpolated. Computer science has a bunch
  of clever interpolation schemes, the OpenCV implementation does a
  bilinear interpolation:
  
  $$:nowrap:$$$$\begin{align*}
  f(x,y) \approx \begin{bmatrix}
      1-x & x \end{bmatrix} \begin{bmatrix}
      f(0,0) & f(0,1) \
      f(1,0) & f(1,1) \end{bmatrix} \begin{bmatrix}
      1-y \
      y \end{bmatrix}.
  \end{align*}$$
  
  By definition the LBP operator is robust against monotonic gray scale
  transformations. We can easily verify this by looking at the LBP image
  of an artificially modified image (so you see what an LBP image looks
  like!):
  
  ![image](img/lbp/lbp_yale.jpg%0A%20:scale:%2060%%0A%20:align:%20center)
  
  So what's left to do is how to incorporate the spatial information in
  the face recognition model. The representation proposed by Ahonen et. al
  [AHP04]_ is to divide the LBP image into $m$ local regions and extract
  a histogram from each. The spatially enhanced feature vector is then
  obtained by concatenating the local histograms (**not merging them**).
  These histograms are called *Local Binary Patterns Histograms*.
cv::Eigenfaces in OpenCV: |-
  For the first source code example, I'll go through it with you. I am
  first giving you the whole source code listing, and after this we'll
  look at the most important lines in detail. Please note: every source
  code listing is commented in detail, so you should have no problems
  following it.
  
  The source code for this demo application is also available in the `src`
  folder coming with this documentation:
    src/facerec_eigenfaces.cpp <src/facerec_eigenfaces.cpp>
  
  
  I've used the jet colormap, so you can see how the grayscale values are
  distributed within the specific Eigenfaces. You can see, that the
  Eigenfaces do not only encode facial features, but also the illumination
  in the images (see the left light in Eigenface #4, right light in
  Eigenfaces #5):
  
  ![image](img/eigenfaces_opencv.png%0A%20:align:%20center)
  
  We've already seen, that we can reconstruct a face from its lower
  dimensional approximation. So let's see how many Eigenfaces are needed
  for a good reconstruction. I'll do a subplot with $10,30,\ldots,310$
  Eigenfaces:
  
  ~~~~ {.sourceCode .cpp}
  // Display or save the image reconstruction at some predefined steps:
  for(int num_components = 10; num_components < 300; num_components+=15) {
      // slice the eigenvectors from the model
      Mat evs = Mat(W, Range::all(), Range(0, num_components));
      Mat projection = subspaceProject(evs, mean, images[0].reshape(1,1));
      Mat reconstruction = subspaceReconstruct(evs, mean, projection);
      // Normalize the result:
      reconstruction = norm_0_255(reconstruction.reshape(1, images[0].rows));
      // Display or save:
      if(argc == 2) {
          imshow(format("eigenface_reconstruction_%d", num_components), reconstruction);
      } else {
          imwrite(format("%s/eigenface_reconstruction_%d.png", output_folder.c_str(), num_components), reconstruction);
      }
  }
  ~~~~
  
  10 Eigenvectors are obviously not sufficient for a good image
  reconstruction, 50 Eigenvectors may already be sufficient to encode
  important facial features. You'll get a good reconstruction with
  approximately 300 Eigenvectors for the AT&T Facedatabase. There are rule
  of thumbs how many Eigenfaces you should choose for a successful face
  recognition, but it heavily depends on the input data. [Zhao03]_ is the
  perfect point to start researching for this:
  
  ![image](img/eigenface_reconstruction_opencv.png%0A%20:align:%20center)
cv::Fisherfaces: |-
  The Principal Component Analysis (PCA), which is the core of the
  Eigenfaces method, finds a linear combination of features that maximizes
  the total variance in data. While this is clearly a powerful way to
  represent data, it doesn't consider any classes and so a lot of
  discriminative information *may* be lost when throwing components away.
  Imagine a situation where the variance in your data is generated by an
  external source, let it be the light. The components identified by a PCA
  do not necessarily contain any discriminative information at all, so the
  projected samples are smeared together and a classification becomes
  impossible (see
  [<http://www.bytefish.de/wiki/pca_lda_with_gnu_octave>](http://www.bytefish.de/wiki/pca_lda_with_gnu_octave)
  for an example).
  
  The Linear Discriminant Analysis performs a class-specific
  dimensionality reduction and was invented by the great statistician [Sir
  R. A. Fisher](http://en.wikipedia.org/wiki/Ronald_Fisher). He
  successfully used it for classifying flowers in his 1936 paper *The use
  of multiple measurements in taxonomic problems* [Fisher36]_. In order
  to find the combination of features that separates best between classes
  the Linear Discriminant Analysis maximizes the ratio of between-classes
  to within-classes scatter, instead of maximizing the overall scatter.
  The idea is simple: same classes should cluster tightly together, while
  different classes are as far away as possible from each other in the
  lower-dimensional representation. This was also recognized by
  [Belhumeur](http://www.cs.columbia.edu/~belhumeur/),
  [Hespanha](http://www.ece.ucsb.edu/~hespanha/) and
  [Kriegman](http://cseweb.ucsd.edu/~kriegman/) and so they applied a
  Discriminant Analysis to face recognition in [BHK97]_.
cv::Local Binary Patterns Histograms: |-
  Eigenfaces and Fisherfaces take a somewhat holistic approach to face
  recognition. You treat your data as a vector somewhere in a
  high-dimensional image space. We all know high-dimensionality is bad, so
  a lower-dimensional subspace is identified, where (probably) useful
  information is preserved. The Eigenfaces approach maximizes the total
  scatter, which can lead to problems if the variance is generated by an
  external source, because components with a maximum variance over all
  classes aren't necessarily useful for classification (see
  [<http://www.bytefish.de/wiki/pca_lda_with_gnu_octave>](http://www.bytefish.de/wiki/pca_lda_with_gnu_octave)).
  So to preserve some discriminative information we applied a Linear
  Discriminant Analysis and optimized as described in the Fisherfaces
  method. The Fisherfaces method worked great... at least for the
  constrained scenario we've assumed in our model.
  
  Now real life isn't perfect. You simply can't guarantee perfect light
  settings in your images or 10 different images of a person. So what if
  there's only one image for each person? Our covariance estimates for the
  subspace *may* be horribly wrong, so will the recognition. Remember the
  Eigenfaces method had a 96% recognition rate on the AT&T Facedatabase?
  How many images do we actually need to get such useful estimates? Here
  are the Rank-1 recognition rates of the Eigenfaces and Fisherfaces
  method on the AT&T Facedatabase, which is a fairly easy image database:
  
  ![image](img/at_database_small_sample_size.png%0A%20:scale:%2060%%0A%20:align:%20center)
  
  So in order to get good recognition rates you'll need at least 8(+-1)
  images for each person and the Fisherfaces method doesn't really help
  here. The above experiment is a 10-fold cross validated result carried
  out with the facerec framework at:
  [<https://github.com/bytefish/facerec>](https://github.com/bytefish/facerec).
  This is not a publication, so I won't back these figures with a deep
  mathematical analysis. Please have a look into [KM01]_ for a detailed
  analysis of both methods, when it comes to small training datasets.
  
  So some research concentrated on extracting local features from images.
  The idea is to not look at the whole image as a high-dimensional vector,
  but describe only local features of an object. The features you extract
  this way will have a low-dimensionality implicitly. A fine idea! But
  you'll soon observe the image representation we are given doesn't only
  suffer from illumination variations. Think of things like scale,
  translation or rotation in images - your local description has to be at
  least a bit robust against those things. Just like :ocvSIFT, the Local
  Binary Patterns methodology has its roots in 2D texture analysis. The
  basic idea of Local Binary Patterns is to summarize the local structure
  in an image by comparing each pixel with its neighborhood. Take a pixel
  as center and threshold its neighbors against. If the intensity of the
  center pixel is greater-equal its neighbor, then denote it with 1 and 0
  if not. You'll end up with a binary number for each pixel, just like
  11001111. So with 8 surrounding pixels you'll end up with 2^8 possible
  combinations, called *Local Binary Patterns* or sometimes referred to as
  *LBP codes*. The first LBP operator described in literature actually
  used a fixed 3 x 3 neighborhood just like this:
  
  ![image](img/lbp/lbp.png%0A%20:scale:%2080%%0A%20:align:%20center)
cv::Local Binary Patterns Histograms in OpenCV: |
  The source code for this demo application is also available in the `src`
  folder coming with this documentation:
    src/facerec_lbph.cpp <src/facerec_lbph.cpp>

cv::Conclusion: |-
  You've learned how to use the new :ocvFaceRecognizer in real
  applications. After reading the document you also know how the
  algorithms work, so now it's time for you to experiment with the
  available algorithms. Use them, improve them and let the OpenCV
  community participate!
cv::Credits: |-
  This document wouldn't be possible without the kind permission to use
  the face images of the *AT&T Database of Faces* and the *Yale
  Facedatabase A/B*.
cv::The Database of Faces: |-
  ** Important: when using these images, please give credit to "AT&T
  Laboratories, Cambridge."**
  
  The Database of Faces, formerly *The ORL Database of Faces*, contains a
  set of face images taken between April 1992 and April 1994. The database
  was used in the context of a face recognition project carried out in
  collaboration with the Speech, Vision and Robotics Group of the
  Cambridge University Engineering Department.
  
  There are ten different images of each of 40 distinct subjects. For some
  subjects, the images were taken at different times, varying the
  lighting, facial expressions (open / closed eyes, smiling / not smiling)
  and facial details (glasses / no glasses). All the images were taken
  against a dark homogeneous background with the subjects in an upright,
  frontal position (with tolerance for some side movement).
  
  The files are in PGM format. The size of each image is 92x112 pixels,
  with 256 grey levels per pixel. The images are organised in 40
  directories (one for each subject), which have names of the form sX,
  where X indicates the subject number (between 1 and 40). In each of
  these directories, there are ten different images of that subject, which
  have names of the form Y.pgm, where Y is the image number for that
  subject (between 1 and 10).
  
  A copy of the database can be retrieved from:
  [<http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip>](http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip).
cv::Yale Facedatabase A: |-
  *With the permission of the authors I am allowed to show a small number
  of images (say subject 1 and all the variations) and all images such as
  Fisherfaces and Eigenfaces from either Yale Facedatabase A or the Yale
  Facedatabase B.*
  
  The Yale Face Database A (size 6.4MB) contains 165 grayscale images in
  GIF format of 15 individuals. There are 11 images per subject, one per
  different facial expression or configuration: center-light, w/glasses,
  happy, left-light, w/no glasses, normal, right-light, sad, sleepy,
  surprised, and wink. (Source:
  [<http://cvc.yale.edu/projects/yalefaces/yalefaces.html>](http://cvc.yale.edu/projects/yalefaces/yalefaces.html))
cv::Yale Facedatabase B: |-
  *With the permission of the authors I am allowed to show a small number
  of images (say subject 1 and all the variations) and all images such as
  Fisherfaces and Eigenfaces from either Yale Facedatabase A or the Yale
  Facedatabase B.*
  
  The extended Yale Face Database B contains 16128 images of 28 human
  subjects under 9 poses and 64 illumination conditions. The data format
  of this database is the same as the Yale Face Database B. Please refer
  to the homepage of the Yale Face Database B (or one copy of this page)
  for more detailed information of the data format.
  
  You are free to use the extended Yale Face Database B for research
  purposes. All publications which use this database should acknowledge
  the use of "the Exteded Yale Face Database B" and reference Athinodoros
  Georghiades, Peter Belhumeur, and David Kriegman's paper, "From Few to
  Many: Illumination Cone Models for Face Recognition under Variable
  Lighting and Pose", PAMI, 2001,
  [[bibtex]](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/athosref.html).
  
  The extended database as opposed to the original Yale Face Database B
  with 10 subjects was first reported by Kuang-Chih Lee, Jeffrey Ho, and
  David Kriegman in "Acquiring Linear Subspaces for Face Recognition under
  Variable Lighting, PAMI, May, 2005
  [[pdf]](http://vision.ucsd.edu/~leekc/papers/9pltsIEEE.pdf)." All test
  image data used in the experiments are manually aligned, cropped, and
  then re-sized to 168x192 images. If you publish your experimental
  results with the cropped images, please reference the PAMI2005 paper as
  well. (Source:
  [<http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html>](http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html))
cv::Face Recongition from Videos: |-
  The source code for the demo is available in the `src` folder coming
  with this documentation:
    src/facerec_video.cpp <../src/facerec_video.cpp>
  
  
  This demo uses the :ocvCascadeClassifier:
cv::Fast Marching Method: |-
  The Fast Marching Method [T04]_ is used in of the video stabilization
  routines to do motion and color inpainting. The method is implemented is
  a flexible way and it's made public for other users.
cv::videostab::FastMarchingMethod: |
  Describes the Fast Marching Method implementation.
  
  ```c++
  class CV_EXPORTS FastMarchingMethod
  {
  public:
      FastMarchingMethod();
  
      template <typename Inpaint>
      Inpaint run(const Mat &mask, Inpaint inpaint);
  
      Mat distanceMap() const;
  };
  ```

cv::videostab::FastMarchingMethod::FastMarchingMethod: Constructor.
cv::videostab::FastMarchingMethod::run: Template method that runs the Fast Marching Method.
cv::SIFT: |-
  Class for extracting keypoints and computing descriptors using the Scale
  Invariant Feature Transform (SIFT) algorithm by D. Lowe [Lowe04]_.
cv::SIFT::SIFT: The SIFT constructors.
cv::SIFT::operator (): Extract features and computes their descriptors using SIFT algorithm
cv::SURF::SURF: The SURF extractor constructors.
cv::SURF::operator(): |-
  Detects keypoints and computes SURF descriptors for them.
  
  The function is parallelized with the TBB library.
  
  If you are using the C version, make sure you call
  `cv::initModule_nonfree()` from `nonfree/nonfree.hpp`.
cv::gpu::SURF_GPU: |-
  Class used for extracting Speeded Up Robust Features (SURF) from an
  image. :
  
  ```c++
  class SURF_GPU
  {
  public:
      enum KeypointLayout
      {
          X_ROW = 0,
          Y_ROW,
          LAPLACIAN_ROW,
          OCTAVE_ROW,
          SIZE_ROW,
          ANGLE_ROW,
          HESSIAN_ROW,
          ROWS_COUNT
      };
  
      //! the default constructor
      SURF_GPU();
      //! the full constructor taking all the necessary parameters
      explicit SURF_GPU(double _hessianThreshold, int _nOctaves=4,
           int _nOctaveLayers=2, bool _extended=false, float _keypointsRatio=0.01f);
  
      //! returns the descriptor size in float's (64 or 128)
      int descriptorSize() const;
  
      //! upload host keypoints to device memory
      void uploadKeypoints(const vector<KeyPoint>& keypoints,
          GpuMat& keypointsGPU);
      //! download keypoints from device to host memory
      void downloadKeypoints(const GpuMat& keypointsGPU,
          vector<KeyPoint>& keypoints);
  
      //! download descriptors from device to host memory
      void downloadDescriptors(const GpuMat& descriptorsGPU,
          vector<float>& descriptors);
  
      void operator()(const GpuMat& img, const GpuMat& mask,
          GpuMat& keypoints);
  
      void operator()(const GpuMat& img, const GpuMat& mask,
          GpuMat& keypoints, GpuMat& descriptors,
          bool useProvidedKeypoints = false,
          bool calcOrientation = true);
  
      void operator()(const GpuMat& img, const GpuMat& mask,
          std::vector<KeyPoint>& keypoints);
  
      void operator()(const GpuMat& img, const GpuMat& mask,
          std::vector<KeyPoint>& keypoints, GpuMat& descriptors,
          bool useProvidedKeypoints = false,
          bool calcOrientation = true);
  
      void operator()(const GpuMat& img, const GpuMat& mask,
          std::vector<KeyPoint>& keypoints,
          std::vector<float>& descriptors,
          bool useProvidedKeypoints = false,
          bool calcOrientation = true);
  
      void releaseMemory();
  
      // SURF parameters
      double hessianThreshold;
      int nOctaves;
      int nOctaveLayers;
      bool extended;
      bool upright;
  
      //! max keypoints = keypointsRatio * img.size().area()
      float keypointsRatio;
  
      GpuMat sum, mask1, maskSum, intBuffer;
  
      GpuMat det, trace;
  
      GpuMat maxPosBuffer;
  };
  ```
  
  
  The class `SURF_GPU` implements Speeded Up Robust Features descriptor.
  There is a fast multi-scale Hessian keypoint detector that can be used
  to find the keypoints (which is the default option). But the descriptors
  can also be computed for the user-specified keypoints. Only 8-bit
  grayscale images are supported.
  
  The class `SURF_GPU` can store results in the GPU and CPU memory. It
  provides functions to convert results between CPU and GPU version (
  `uploadKeypoints`, `downloadKeypoints`, `downloadDescriptors` ). The
  format of CPU results is the same as `SURF` results. GPU results are
  stored in `GpuMat`. The `keypoints` matrix is
  $\texttt{nFeatures} \times 7$ matrix with the `CV_32FC1` type.
    `keypoints.ptr<float>(X_ROW)[i]` contains x coordinate of the i-th
  feature.
  
    `keypoints.ptr<float>(Y_ROW)[i]` contains y coordinate of the i-th
  feature.
  
    `keypoints.ptr<float>(LAPLACIAN_ROW)[i]` contains the laplacian sign
  of the i-th feature.
  
    `keypoints.ptr<float>(OCTAVE_ROW)[i]` contains the octave of the
  i-th feature.
  
    `keypoints.ptr<float>(SIZE_ROW)[i]` contains the size of the i-th
  feature.
  
    `keypoints.ptr<float>(ANGLE_ROW)[i]` contain orientation of the i-th
  feature.
  
    `keypoints.ptr<float>(HESSIAN_ROW)[i]` contains the response of the
  i-th feature.
  
  
  The `descriptors` matrix is
  $\texttt{nFeatures} \times \texttt{descriptorSize}$ matrix with the
  `CV_32FC1` type.
  
  The class `SURF_GPU` uses some buffers and provides access to it. All
  buffers can be safely released between function calls.
cv::ocl::SURF_OCL: |-
  Class used for extracting Speeded Up Robust Features (SURF) from an
  image. :
  
  ```c++
  class SURF_OCL
  {
  public:
      enum KeypointLayout
      {
          X_ROW = 0,
          Y_ROW,
          LAPLACIAN_ROW,
          OCTAVE_ROW,
          SIZE_ROW,
          ANGLE_ROW,
          HESSIAN_ROW,
          ROWS_COUNT
      };
  
      //! the default constructor
      SURF_OCL();
      //! the full constructor taking all the necessary parameters
      explicit SURF_OCL(double _hessianThreshold, int _nOctaves=4,
           int _nOctaveLayers=2, bool _extended=false, float _keypointsRatio=0.01f, bool _upright = false);
  
      //! returns the descriptor size in float's (64 or 128)
      int descriptorSize() const;
  
      //! upload host keypoints to device memory
      void uploadKeypoints(const vector<KeyPoint>& keypoints,
          oclMat& keypointsocl);
      //! download keypoints from device to host memory
      void downloadKeypoints(const oclMat& keypointsocl,
          vector<KeyPoint>& keypoints);
  
      //! download descriptors from device to host memory
      void downloadDescriptors(const oclMat& descriptorsocl,
          vector<float>& descriptors);
  
      void operator()(const oclMat& img, const oclMat& mask,
          oclMat& keypoints);
  
      void operator()(const oclMat& img, const oclMat& mask,
          oclMat& keypoints, oclMat& descriptors,
          bool useProvidedKeypoints = false);
  
      void operator()(const oclMat& img, const oclMat& mask,
          std::vector<KeyPoint>& keypoints);
  
      void operator()(const oclMat& img, const oclMat& mask,
          std::vector<KeyPoint>& keypoints, oclMat& descriptors,
          bool useProvidedKeypoints = false);
  
      void operator()(const oclMat& img, const oclMat& mask,
          std::vector<KeyPoint>& keypoints,
          std::vector<float>& descriptors,
          bool useProvidedKeypoints = false);
  
      void releaseMemory();
  
      // SURF parameters
      double hessianThreshold;
      int nOctaves;
      int nOctaveLayers;
      bool extended;
      bool upright;
  
      //! max keypoints = min(keypointsRatio * img.size().area(), 65535)
      float keypointsRatio;
  
      oclMat sum, mask1, maskSum, intBuffer;
  
      oclMat det, trace;
  
      oclMat maxPosBuffer;
  };
  ```
  
  
  The class `SURF_OCL` implements Speeded Up Robust Features descriptor.
  There is a fast multi-scale Hessian keypoint detector that can be used
  to find the keypoints (which is the default option). But the descriptors
  can also be computed for the user-specified keypoints. Only 8-bit
  grayscale images are supported.
  
  The class `SURF_OCL` can store results in the GPU and CPU memory. It
  provides functions to convert results between CPU and GPU version (
  `uploadKeypoints`, `downloadKeypoints`, `downloadDescriptors` ). The
  format of CPU results is the same as `SURF` results. GPU results are
  stored in `oclMat`. The `keypoints` matrix is
  $\texttt{nFeatures} \times 7$ matrix with the `CV_32FC1` type.
    `keypoints.ptr<float>(X_ROW)[i]` contains x coordinate of the i-th
  feature.
  
    `keypoints.ptr<float>(Y_ROW)[i]` contains y coordinate of the i-th
  feature.
  
    `keypoints.ptr<float>(LAPLACIAN_ROW)[i]` contains the laplacian sign
  of the i-th feature.
  
    `keypoints.ptr<float>(OCTAVE_ROW)[i]` contains the octave of the
  i-th feature.
  
    `keypoints.ptr<float>(SIZE_ROW)[i]` contains the size of the i-th
  feature.
  
    `keypoints.ptr<float>(ANGLE_ROW)[i]` contain orientation of the i-th
  feature.
  
    `keypoints.ptr<float>(HESSIAN_ROW)[i]` contains the response of the
  i-th feature.
  
  
  The `descriptors` matrix is
  $\texttt{nFeatures} \times \texttt{descriptorSize}$ matrix with the
  `CV_32FC1` type.
  
  The class `SURF_OCL` uses some buffers and provides access to it. All
  buffers can be safely released between function calls.
cv::ocl::Canny: Finds edges in an image using the [Canny86]_ algorithm.
cv::ocl::BruteForceMatcher_OCL_base: |-
  Brute-force descriptor matcher. For each descriptor in the first set,
  this matcher finds the closest descriptor in the second set by trying
  each one. This descriptor matcher supports masking permissible matches
  between descriptor sets. :
  
  ```c++
  class BruteForceMatcher_OCL_base
  {
  public:
          enum DistType {L1Dist = 0, L2Dist, HammingDist};
  
      // Add descriptors to train descriptor collection.
      void add(const std::vector<oclMat>& descCollection);
  
      // Get train descriptors collection.
      const std::vector<oclMat>& getTrainDescriptors() const;
  
      // Clear train descriptors collection.
      void clear();
  
      // Return true if there are no train descriptors in collection.
      bool empty() const;
  
      // Return true if the matcher supports mask in match methods.
      bool isMaskSupported() const;
  
      void matchSingle(const oclMat& query, const oclMat& train,
          oclMat& trainIdx, oclMat& distance,
          const oclMat& mask = oclMat());
  
      static void matchDownload(const oclMat& trainIdx,
          const oclMat& distance, std::vector<DMatch>& matches);
      static void matchConvert(const Mat& trainIdx,
          const Mat& distance, std::vector<DMatch>& matches);
  
      void match(const oclMat& query, const oclMat& train,
          std::vector<DMatch>& matches, const oclMat& mask = oclMat());
  
      void makeGpuCollection(oclMat& trainCollection, oclMat& maskCollection,
          const vector<oclMat>& masks = std::vector<oclMat>());
  
      void matchCollection(const oclMat& query, const oclMat& trainCollection,
          oclMat& trainIdx, oclMat& imgIdx, oclMat& distance,
          const oclMat& maskCollection);
  
      static void matchDownload(const oclMat& trainIdx, oclMat& imgIdx,
          const oclMat& distance, std::vector<DMatch>& matches);
      static void matchConvert(const Mat& trainIdx, const Mat& imgIdx,
          const Mat& distance, std::vector<DMatch>& matches);
  
      void match(const oclMat& query, std::vector<DMatch>& matches,
          const std::vector<oclMat>& masks = std::vector<oclMat>());
  
      void knnMatchSingle(const oclMat& query, const oclMat& train,
          oclMat& trainIdx, oclMat& distance, oclMat& allDist, int k,
          const oclMat& mask = oclMat());
  
      static void knnMatchDownload(const oclMat& trainIdx, const oclMat& distance,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
      static void knnMatchConvert(const Mat& trainIdx, const Mat& distance,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
  
      void knnMatch(const oclMat& query, const oclMat& train,
          std::vector< std::vector<DMatch> >& matches, int k,
          const oclMat& mask = oclMat(), bool compactResult = false);
  
      void knnMatch2Collection(const oclMat& query, const oclMat& trainCollection,
          oclMat& trainIdx, oclMat& imgIdx, oclMat& distance,
          const oclMat& maskCollection = oclMat());
  
      static void knnMatch2Download(const oclMat& trainIdx, const oclMat& imgIdx, const oclMat& distance,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
      static void knnMatch2Convert(const Mat& trainIdx, const Mat& imgIdx, const Mat& distance,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
  
      void knnMatch(const oclMat& query, std::vector< std::vector<DMatch> >& matches, int k,
          const std::vector<oclMat>& masks = std::vector<oclMat>(),
          bool compactResult = false);
  
      void radiusMatchSingle(const oclMat& query, const oclMat& train,
          oclMat& trainIdx, oclMat& distance, oclMat& nMatches, float maxDistance,
          const oclMat& mask = oclMat());
  
      static void radiusMatchDownload(const oclMat& trainIdx, const oclMat& distance, const oclMat& nMatches,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
      static void radiusMatchConvert(const Mat& trainIdx, const Mat& distance, const Mat& nMatches,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
  
      void radiusMatch(const oclMat& query, const oclMat& train,
          std::vector< std::vector<DMatch> >& matches, float maxDistance,
          const oclMat& mask = oclMat(), bool compactResult = false);
  
      void radiusMatchCollection(const oclMat& query, oclMat& trainIdx, oclMat& imgIdx, oclMat& distance, oclMat& nMatches, float maxDistance,
          const std::vector<oclMat>& masks = std::vector<oclMat>());
  
      static void radiusMatchDownload(const oclMat& trainIdx, const oclMat& imgIdx, const oclMat& distance, const oclMat& nMatches,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
      static void radiusMatchConvert(const Mat& trainIdx, const Mat& imgIdx, const Mat& distance, const Mat& nMatches,
          std::vector< std::vector<DMatch> >& matches, bool compactResult = false);
  
      void radiusMatch(const oclMat& query, std::vector< std::vector<DMatch> >& matches, float maxDistance,
          const std::vector<oclMat>& masks = std::vector<oclMat>(), bool compactResult = false);
  
              DistType distType;
  
  private:
      std::vector<oclMat> trainDescCollection;
  };
  ```
  
  
  The class `BruteForceMatcher_OCL_base` has an interface similar to the
  class :ocvDescriptorMatcher. It has two groups of `match` methods: for
  matching descriptors of one image with another image or with an image
  set. Also, all functions have an alternative to save results either to
  the GPU memory or to the CPU memory. `BruteForceMatcher_OCL_base`
  supports only the `L1<float>`, `L2<float>`, and `Hamming` distance
  types.
cv::ocl::BruteForceMatcher_OCL_base::match: |-
  Finds the best match for each descriptor from a query set with train
  descriptors.
cv::ocl::BruteForceMatcher_OCL_base::makeGpuCollection: |-
  Performs a GPU collection of train descriptors and masks in a suitable
  format for the :ocvocl::BruteForceMatcher_OCL_base::matchCollection
  function.
cv::ocl::BruteForceMatcher_OCL_base::matchDownload: "Downloads matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::matchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::matchCollection to vector with\n\
  :ocvDMatch."
cv::ocl::BruteForceMatcher_OCL_base::matchConvert: "Converts matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::matchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::matchCollection to vector with\n\
  :ocvDMatch."
cv::ocl::BruteForceMatcher_OCL_base::knnMatch: |-
  Finds the `k` best matches for each descriptor from a query set with
  train descriptors.
  
  The function returns detected `k` (or less if not possible) matches in
  the increasing order by distance.
  
  The third variant of the method stores the results in GPU memory.
cv::ocl::BruteForceMatcher_OCL_base::knnMatchDownload: "Downloads matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::knnMatchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::knnMatch2Collection to vector\n\
  with :ocvDMatch.\n\n\
  If `compactResult` is `true` , the `matches` vector does not contain\n\
  matches for fully masked-out query descriptors."
cv::ocl::BruteForceMatcher_OCL_base::knnMatchConvert: "Converts matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::knnMatchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::knnMatch2Collection to CPU vector\n\
  with :ocvDMatch.\n\n\
  If `compactResult` is `true` , the `matches` vector does not contain\n\
  matches for fully masked-out query descriptors."
cv::ocl::BruteForceMatcher_OCL_base::radiusMatch: |-
  For each query descriptor, finds the best matches with a distance less
  than a given threshold.
  
  The function returns detected matches in the increasing order by
  distance.
  
  The methods work only on devices with the compute capability $>=$ 1.1.
  
  The third variant of the method stores the results in GPU memory and
  does not store the points by the distance.
cv::ocl::BruteForceMatcher_OCL_base::radiusMatchDownload: "Downloads matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::radiusMatchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::radiusMatchCollection to vector\n\
  with :ocvDMatch.\n\n\
  If `compactResult` is `true` , the `matches` vector does not contain\n\
  matches for fully masked-out query descriptors."
cv::ocl::BruteForceMatcher_OCL_base::radiusMatchConvert: "Converts matrices obtained via\n\
  :ocvocl::BruteForceMatcher_OCL_base::radiusMatchSingle or\n\
  :ocvocl::BruteForceMatcher_OCL_base::radiusMatchCollection to vector\n\
  with :ocvDMatch.\n\n\
  If `compactResult` is `true` , the `matches` vector does not contain\n\
  matches for fully masked-out query descriptors."
cv::ocl::HOGDescriptor: |-
  The class implements Histogram of Oriented Gradients ([Dalal2005]_)
  object detector. :
  
  ```c++
  struct CV_EXPORTS HOGDescriptor
  {
      enum { DEFAULT_WIN_SIGMA = -1 };
      enum { DEFAULT_NLEVELS = 64 };
      enum { DESCR_FORMAT_ROW_BY_ROW, DESCR_FORMAT_COL_BY_COL };
  
      HOGDescriptor(Size win_size=Size(64, 128), Size block_size=Size(16, 16),
                    Size block_stride=Size(8, 8), Size cell_size=Size(8, 8),
                    int nbins=9, double win_sigma=DEFAULT_WIN_SIGMA,
                    double threshold_L2hys=0.2, bool gamma_correction=true,
                    int nlevels=DEFAULT_NLEVELS);
  
      size_t getDescriptorSize() const;
      size_t getBlockHistogramSize() const;
  
      void setSVMDetector(const vector<float>& detector);
  
      static vector<float> getDefaultPeopleDetector();
      static vector<float> getPeopleDetector48x96();
      static vector<float> getPeopleDetector64x128();
  
      void detect(const oclMat& img, vector<Point>& found_locations,
                  double hit_threshold=0, Size win_stride=Size(),
                  Size padding=Size());
  
      void detectMultiScale(const oclMat& img, vector<Rect>& found_locations,
                            double hit_threshold=0, Size win_stride=Size(),
                            Size padding=Size(), double scale0=1.05,
                            int group_threshold=2);
  
      void getDescriptors(const oclMat& img, Size win_stride,
                          oclMat& descriptors,
                          int descr_format=DESCR_FORMAT_COL_BY_COL);
  
      Size win_size;
      Size block_size;
      Size block_stride;
      Size cell_size;
      int nbins;
      double win_sigma;
      double threshold_L2hys;
      bool gamma_correction;
      int nlevels;
  
  private:
      // Hidden
  }
  ```
  
  
  Interfaces of all methods are kept similar to the `CPU HOG` descriptor
  and detector analogues as much as possible.
cv::ocl::HOGDescriptor::HOGDescriptor: Creates the `HOG` descriptor and detector.
cv::ocl::HOGDescriptor::getDescriptorSize: Returns the number of coefficients required for the classification.
cv::ocl::HOGDescriptor::getBlockHistogramSize: Returns the block histogram size.
cv::ocl::HOGDescriptor::setSVMDetector: Sets coefficients for the linear SVM classifier.
cv::ocl::HOGDescriptor::getDefaultPeopleDetector: |-
  Returns coefficients of the classifier trained for people detection (for
  default window size).
cv::ocl::HOGDescriptor::getPeopleDetector48x96: |-
  Returns coefficients of the classifier trained for people detection (for
  48x96 windows).
cv::ocl::HOGDescriptor::getPeopleDetector64x128: |-
  Returns coefficients of the classifier trained for people detection (for
  64x128 windows).
cv::ocl::HOGDescriptor::detect: Performs object detection without a multi-scale window.
cv::ocl::HOGDescriptor::detectMultiScale: Performs object detection with a multi-scale window.
cv::ocl::HOGDescriptor::getDescriptors: |-
  Returns block descriptors computed for the whole image.
  
  The function is mainly used to learn the classifier.
  ********************************* features2d. 2D Features
  Framework*********************************
cv::Image Filtering: |
  Functions and classes described in this section are used to perform
  various linear or non-linear filtering operations on 2D images
  (represented as :ocvMat's). It means that for each pixel location
  $(x,y)$ in the source image (normally, rectangular), its neighborhood is
  considered and used to compute the response. In case of a linear filter,
  it is a weighted sum of pixel values. In case of morphological
  operations, it is the minimum or maximum values, and so on. The computed
  response is stored in the destination image at the same location $(x,y)$
  . It means that the output image will be of the same size as the input
  image. Normally, the functions support multi-channel arrays, in which
  case every channel is processed independently. Therefore, the output
  image will also have the same number of channels as the input one.
  
  Another common feature of the functions and classes described in this
  section is that, unlike simple arithmetic functions, they need to
  extrapolate values of some non-existing pixels. For example, if you want
  to smooth an image using a Gaussian $3 \times 3$ filter, then, when
  processing the left-most pixels in each row, you need pixels to the left
  of them, that is, outside of the image. You can let these pixels be the
  same as the left-most image pixels ("replicated border" extrapolation
  method), or assume that all the non-existing pixels are zeros ("constant
  border" extrapolation method), and so on. OpenCV enables you to specify
  the extrapolation method. For details, see the function
  :ocvborderInterpolate and discussion of the `borderType` parameter in
  the section and various functions below. :
  
  ```c++
  /*
   Various border types, image boundaries are denoted with '|'
  
   * BORDER_REPLICATE:     aaaaaa|abcdefgh|hhhhhhh
   * BORDER_REFLECT:       fedcba|abcdefgh|hgfedcb
   * BORDER_REFLECT_101:   gfedcb|abcdefgh|gfedcba
   * BORDER_WRAP:          cdefgh|abcdefgh|abcdefg
   * BORDER_CONSTANT:      iiiiii|abcdefgh|iiiiiii  with some specified 'i'
   */
  ```

cv::BaseColumnFilter: |-
  Base class for filters with single-column kernels. :
  
  ```c++
  class BaseColumnFilter
  {
  public:
      virtual ~BaseColumnFilter();
  
      // To be overriden by the user.
      //
      // runs a filtering operation on the set of rows,
      // "dstcount + ksize - 1" rows on input,
      // "dstcount" rows on output,
      // each input and output row has "width" elements
      // the filtered rows are written into "dst" buffer.
      virtual void operator()(const uchar** src, uchar* dst, int dststep,
                              int dstcount, int width) = 0;
      // resets the filter state (may be needed for IIR filters)
      virtual void reset();
  
      int ksize; // the aperture size
      int anchor; // position of the anchor point,
                  // normally not used during the processing
  };
  ```
  
  
  The class `BaseColumnFilter` is a base class for filtering data using
  single-column kernels. Filtering does not have to be a linear operation.
  In general, it could be written as follows:
  
  $$\texttt{dst} (x,y) = F( \texttt{src} [y](x), \; \texttt{src} [y+1](x), \; ..., \; \texttt{src} [y+ \texttt{ksize} -1](x)$$
  
  where $F$ is a filtering function but, as it is represented as a class,
  it can produce any side effects, memorize previously processed data, and
  so on. The class only defines an interface and is not used directly.
  Instead, there are several functions in OpenCV (and you can add more)
  that return pointers to the derived classes that implement specific
  filtering operations. Those pointers are then passed to the
  :ocvFilterEngine constructor. While the filtering operation interface
  uses the `uchar` type, a particular implementation is not limited to
  8-bit data.
cv::BaseFilter: |-
  Base class for 2D image filters. :
  
  ```c++
  class BaseFilter
  {
  public:
      virtual ~BaseFilter();
  
      // To be overriden by the user.
      //
      // runs a filtering operation on the set of rows,
      // "dstcount + ksize.height - 1" rows on input,
      // "dstcount" rows on output,
      // each input row has "(width + ksize.width-1)*cn" elements
      // each output row has "width*cn" elements.
      // the filtered rows are written into "dst" buffer.
      virtual void operator()(const uchar** src, uchar* dst, int dststep,
                              int dstcount, int width, int cn) = 0;
      // resets the filter state (may be needed for IIR filters)
      virtual void reset();
      Size ksize;
      Point anchor;
  };
  ```
  
  
  The class `BaseFilter` is a base class for filtering data using 2D
  kernels. Filtering does not have to be a linear operation. In general,
  it could be written as follows:
  
  $$\begin{array}{l} \texttt{dst} (x,y) = F(  \texttt{src} [y](x), \; \texttt{src} [y](x+1), \; ..., \; \texttt{src} [y](x+ \texttt{ksize.width} -1),  \ \texttt{src} [y+1](x), \; \texttt{src} [y+1](x+1), \; ..., \; \texttt{src} [y+1](x+ \texttt{ksize.width} -1),  \ .........................................................................................  \ \texttt{src} [y+ \texttt{ksize.height-1} ](x), \ \texttt{src} [y+ \texttt{ksize.height-1} ](x+1), \ ...
     \texttt{src} [y+ \texttt{ksize.height-1} ](x+ \texttt{ksize.width} -1))
     \end{array}$$
  
  where $F$ is a filtering function. The class only defines an interface
  and is not used directly. Instead, there are several functions in OpenCV
  (and you can add more) that return pointers to the derived classes that
  implement specific filtering operations. Those pointers are then passed
  to the :ocvFilterEngine constructor. While the filtering operation
  interface uses the `uchar` type, a particular implementation is not
  limited to 8-bit data.
cv::BaseRowFilter: |-
  Base class for filters with single-row kernels. :
  
  ```c++
  class BaseRowFilter
  {
  public:
      virtual ~BaseRowFilter();
  
      // To be overriden by the user.
      //
      // runs filtering operation on the single input row
      // of "width" element, each element is has "cn" channels.
      // the filtered row is written into "dst" buffer.
      virtual void operator()(const uchar* src, uchar* dst,
                              int width, int cn) = 0;
      int ksize, anchor;
  };
  ```
  
  
  The class `BaseRowFilter` is a base class for filtering data using
  single-row kernels. Filtering does not have to be a linear operation. In
  general, it could be written as follows:
  
  $$\texttt{dst} (x,y) = F( \texttt{src} [y](x), \; \texttt{src} [y](x+1), \; ..., \; \texttt{src} [y](x+ \texttt{ksize.width} -1))$$
  
  where $F$ is a filtering function. The class only defines an interface
  and is not used directly. Instead, there are several functions in OpenCV
  (and you can add more) that return pointers to the derived classes that
  implement specific filtering operations. Those pointers are then passed
  to the :ocvFilterEngine constructor. While the filtering operation
  interface uses the `uchar` type, a particular implementation is not
  limited to 8-bit data.
cv::FilterEngine: |-
  Generic image filtering class. :
  
  ```c++
  class FilterEngine
  {
  public:
      // empty constructor
      FilterEngine();
      // builds a 2D non-separable filter (!_filter2D.empty()) or
      // a separable filter (!_rowFilter.empty() && !_columnFilter.empty())
      // the input data type will be "srcType", the output data type will be "dstType",
      // the intermediate data type is "bufType".
      // _rowBorderType and _columnBorderType determine how the image
      // will be extrapolated beyond the image boundaries.
      // _borderValue is only used when _rowBorderType and/or _columnBorderType
      // == BORDER_CONSTANT
      FilterEngine(const Ptr<BaseFilter>& _filter2D,
                   const Ptr<BaseRowFilter>& _rowFilter,
                   const Ptr<BaseColumnFilter>& _columnFilter,
                   int srcType, int dstType, int bufType,
                   int _rowBorderType=BORDER_REPLICATE,
                   int _columnBorderType=-1, // use _rowBorderType by default
                   const Scalar& _borderValue=Scalar());
      virtual ~FilterEngine();
      // separate function for the engine initialization
      void init(const Ptr<BaseFilter>& _filter2D,
                const Ptr<BaseRowFilter>& _rowFilter,
                const Ptr<BaseColumnFilter>& _columnFilter,
                int srcType, int dstType, int bufType,
                int _rowBorderType=BORDER_REPLICATE, int _columnBorderType=-1,
                const Scalar& _borderValue=Scalar());
      // starts filtering of the ROI in an image of size "wholeSize".
      // returns the starting y-position in the source image.
      virtual int start(Size wholeSize, Rect roi, int maxBufRows=-1);
      // alternative form of start that takes the image
      // itself instead of "wholeSize". Set isolated to true to pretend that
      // there are no real pixels outside of the ROI
      // (so that the pixels are extrapolated using the specified border modes)
      virtual int start(const Mat& src, const Rect& srcRoi=Rect(0,0,-1,-1),
                        bool isolated=false, int maxBufRows=-1);
      // processes the next portion of the source image,
      // "srcCount" rows starting from "src" and
      // stores the results in "dst".
      // returns the number of produced rows
      virtual int proceed(const uchar* src, int srcStep, int srcCount,
                          uchar* dst, int dstStep);
      // higher-level function that processes the whole
      // ROI or the whole image with a single call
      virtual void apply( const Mat& src, Mat& dst,
                          const Rect& srcRoi=Rect(0,0,-1,-1),
                          Point dstOfs=Point(0,0),
                          bool isolated=false);
      bool isSeparable() const { return filter2D.empty(); }
      // how many rows from the input image are not yet processed
      int remainingInputRows() const;
      // how many output rows are not yet produced
      int remainingOutputRows() const;
      ...
      // the starting and the ending rows in the source image
      int startY, endY;
  
      // pointers to the filters
      Ptr<BaseFilter> filter2D;
      Ptr<BaseRowFilter> rowFilter;
      Ptr<BaseColumnFilter> columnFilter;
  };
  ```
  
  
  The class `FilterEngine` can be used to apply an arbitrary filtering
  operation to an image. It contains all the necessary intermediate
  buffers, computes extrapolated values of the "virtual" pixels outside of
  the image, and so on. Pointers to the initialized `FilterEngine`
  instances are returned by various `create*Filter` functions (see below)
  and they are used inside high-level functions such as :ocvfilter2D,
  :ocverode, :ocvdilate, and others. Thus, the class plays a key role in
  many of OpenCV filtering functions.
  
  This class makes it easier to combine filtering operations with other
  operations, such as color space conversions, thresholding, arithmetic
  operations, and others. By combining several operations together you can
  get much better performance because your data will stay in cache. For
  example, see below the implementation of the Laplace operator for
  floating-point images, which is a simplified implementation of
  :ocvLaplacian : :
  
  ```c++
  void laplace_f(const Mat& src, Mat& dst)
  {
      CV_Assert( src.type() == CV_32F );
      dst.create(src.size(), src.type());
  
      // get the derivative and smooth kernels for d2I/dx2.
      // for d2I/dy2 consider using the same kernels, just swapped
      Mat kd, ks;
      getSobelKernels( kd, ks, 2, 0, ksize, false, ktype );
  
      // process 10 source rows at once
      int DELTA = std::min(10, src.rows);
      Ptr<FilterEngine> Fxx = createSeparableLinearFilter(src.type(),
          dst.type(), kd, ks, Point(-1,-1), 0, borderType, borderType, Scalar() );
      Ptr<FilterEngine> Fyy = createSeparableLinearFilter(src.type(),
          dst.type(), ks, kd, Point(-1,-1), 0, borderType, borderType, Scalar() );
  
      int y = Fxx->start(src), dsty = 0, dy = 0;
      Fyy->start(src);
      const uchar* sptr = src.data + y*src.step;
  
      // allocate the buffers for the spatial image derivatives;
      // the buffers need to have more than DELTA rows, because at the
      // last iteration the output may take max(kd.rows-1,ks.rows-1)
      // rows more than the input.
      Mat Ixx( DELTA + kd.rows - 1, src.cols, dst.type() );
      Mat Iyy( DELTA + kd.rows - 1, src.cols, dst.type() );
  
      // inside the loop always pass DELTA rows to the filter
      // (note that the "proceed" method takes care of possibe overflow, since
      // it was given the actual image height in the "start" method)
      // on output you can get:
      //  * < DELTA rows (initial buffer accumulation stage)
      //  * = DELTA rows (settled state in the middle)
      //  * > DELTA rows (when the input image is over, generate
      //                  "virtual" rows using the border mode and filter them)
      // this variable number of output rows is dy.
      // dsty is the current output row.
      // sptr is the pointer to the first input row in the portion to process
      for( ; dsty < dst.rows; sptr += DELTA*src.step, dsty += dy )
      {
          Fxx->proceed( sptr, (int)src.step, DELTA, Ixx.data, (int)Ixx.step );
          dy = Fyy->proceed( sptr, (int)src.step, DELTA, d2y.data, (int)Iyy.step );
          if( dy > 0 )
          {
              Mat dstripe = dst.rowRange(dsty, dsty + dy);
              add(Ixx.rowRange(0, dy), Iyy.rowRange(0, dy), dstripe);
          }
      }
  }
  ```
  
  
  If you do not need that much control of the filtering process, you can
  simply use the `FilterEngine::apply` method. The method is implemented
  as follows: :
  
  ```c++
  void FilterEngine::apply(const Mat& src, Mat& dst,
      const Rect& srcRoi, Point dstOfs, bool isolated)
  {
      // check matrix types
      CV_Assert( src.type() == srcType && dst.type() == dstType );
  
      // handle the "whole image" case
      Rect _srcRoi = srcRoi;
      if( _srcRoi == Rect(0,0,-1,-1) )
          _srcRoi = Rect(0,0,src.cols,src.rows);
  
      // check if the destination ROI is inside dst.
      // and FilterEngine::start will check if the source ROI is inside src.
      CV_Assert( dstOfs.x >= 0 && dstOfs.y >= 0 &&
          dstOfs.x + _srcRoi.width <= dst.cols &&
          dstOfs.y + _srcRoi.height <= dst.rows );
  
      // start filtering
      int y = start(src, _srcRoi, isolated);
  
      // process the whole ROI. Note that "endY - startY" is the total number
      // of the source rows to process
      // (including the possible rows outside of srcRoi but inside the source image)
      proceed( src.data + y*src.step,
               (int)src.step, endY - startY,
               dst.data + dstOfs.y*dst.step +
               dstOfs.x*dst.elemSize(), (int)dst.step );
  }
  ```
  
  
  Unlike the earlier versions of OpenCV, now the filtering operations
  fully support the notion of image ROI, that is, pixels outside of the
  ROI but inside the image can be used in the filtering operations. For
  example, you can take a ROI of a single pixel and filter it. This will
  be a filter response at that particular pixel. However, it is possible
  to emulate the old behavior by passing `isolated=false` to
  `FilterEngine::start` or `FilterEngine::apply` . You can pass the ROI
  explicitly to `FilterEngine::apply` or construct new matrix headers: :
  
  ```c++
  // compute dI/dx derivative at src(x,y)
  
  // method 1:
  // form a matrix header for a single value
  float val1 = 0;
  Mat dst1(1,1,CV_32F,&val1);
  
  Ptr<FilterEngine> Fx = createDerivFilter(CV_32F, CV_32F,
                          1, 0, 3, BORDER_REFLECT_101);
  Fx->apply(src, Rect(x,y,1,1), Point(), dst1);
  
  // method 2:
  // form a matrix header for a single value
  float val2 = 0;
  Mat dst2(1,1,CV_32F,&val2);
  
  Mat pix_roi(src, Rect(x,y,1,1));
  Sobel(pix_roi, dst2, dst2.type(), 1, 0, 3, 1, 0, BORDER_REFLECT_101);
  
  printf("method1 =
  ```
  
  
  Explore the data types. As it was mentioned in the :ocvBaseFilter
  description, the specific filters can process data of any type, despite
  that `Base*Filter::operator()` only takes `uchar` pointers and no
  information about the actual types. To make it all work, the following
  rules are used:
  
  *
  :   In case of separable filtering, `FilterEngine::rowFilter` is applied
      first. It transforms the input image data (of type `srcType` ) to
      the intermediate results stored in the internal buffers (of type
      `bufType` ). Then, these intermediate results are processed as
      *single-channel data* with `FilterEngine::columnFilter` and stored
      in the output image (of type `dstType` ). Thus, the input type for
      `rowFilter` is `srcType` and the output type is `bufType` . The
      input type for `columnFilter` is `CV_MAT_DEPTH(bufType)` and the
      output type is `CV_MAT_DEPTH(dstType)` .
  
  *
  :   In case of non-separable filtering, `bufType` must be the same as
      `srcType` . The source data is copied to the temporary buffer, if
      needed, and then just passed to `FilterEngine::filter2D` . That is,
      the input type for `filter2D` is `srcType` (= `bufType` ) and the
      output type is `dstType` .
cv::bilateralFilter: |-
  Applies the bilateral filter to an image.
  
  The function applies bilateral filtering to the input image, as
  described in
  <http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html>
  `bilateralFilter` can reduce unwanted noise very well while keeping
  edges fairly sharp. However, it is very slow compared to most filters.
  
  *Sigma values*: For simplicity, you can set the 2 sigma values to be the
  same. If they are small (< 10), the filter will not have much effect,
  whereas if they are large (> 150), they will have a very strong effect,
  making the image look "cartoonish".
  
  *Filter size*: Large filters (d > 5) are very slow, so it is
  recommended to use d=5 for real-time applications, and perhaps d=9 for
  offline applications that need heavy noise filtering.
  
  This filter does not work inplace.
cv::blur: |-
  Blurs an image using the normalized box filter.
  
  The function smoothes an image using the kernel:
  
  $$\texttt{K} =  \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 & 1 & 1 &  \cdots & 1 & 1  \ 1 & 1 & 1 &  \cdots & 1 & 1  \ \hdotsfor{6} \ 1 & 1 & 1 &  \cdots & 1 & 1  \ \end{bmatrix}$$
  
  The call `blur(src, dst, ksize, anchor, borderType)` is equivalent to
  `boxFilter(src, dst, src.type(), anchor, true, borderType)` .
cv::boxFilter: |-
  Blurs an image using the box filter.
  
  The function smoothes an image using the kernel:
  
  $$\texttt{K} =  \alpha \begin{bmatrix} 1 & 1 & 1 &  \cdots & 1 & 1  \ 1 & 1 & 1 &  \cdots & 1 & 1  \ \hdotsfor{6} \ 1 & 1 & 1 &  \cdots & 1 & 1 \end{bmatrix}$$
  
  where
  
  $$\alpha = \fork{\frac{1}{\texttt{ksize.width*ksize.height}}}{when \texttt{normalize=true}}{1}{otherwise}$$
  
  Unnormalized box filter is useful for computing various integral
  characteristics over each pixel neighborhood, such as covariance
  matrices of image derivatives (used in dense optical flow algorithms,
  and so on). If you need to compute pixel sums over variable-size
  windows, use :ocvintegral .
cv::buildPyramid: |-
  Constructs the Gaussian pyramid for an image.
  
  The function constructs a vector of images and builds the Gaussian
  pyramid by recursively applying :ocvpyrDown to the previously built
  pyramid layers, starting from `dst[0]==src` .
cv::createBoxFilter: "Returns a box filter engine.\n\n\
  The function is a convenience function that retrieves the horizontal sum\n\
  primitive filter with :ocvgetRowSumFilter , vertical sum filter with\n\
  :ocvgetColumnSumFilter , constructs new :ocvFilterEngine , and passes\n\
  both of the primitive filters there. The constructed filter engine can\n\
  be used for image filtering with normalized or unnormalized box filter.\n\n\
  The function itself is used by :ocvblur and :ocvboxFilter ."
cv::createDerivFilter: |-
  Returns an engine for computing image derivatives.
  
  The function :ocvcreateDerivFilter is a small convenience function that
  retrieves linear filter coefficients for computing image derivatives
  using :ocvgetDerivKernels and then creates a separable linear filter
  with :ocvcreateSeparableLinearFilter . The function is used by :ocvSobel
  and :ocvScharr .
cv::createGaussianFilter: "Returns an engine for smoothing images with the Gaussian filter.\n\n\
  The function :ocvcreateGaussianFilter computes Gaussian kernel\n\
  coefficients and then returns a separable linear filter for that kernel.\n\
  The function is used by :ocvGaussianBlur . Note that while the function\n\
  takes just one data type, both for input and output, you can pass this\n\
  limitation by calling :ocvgetGaussianKernel and then\n\
  :ocvcreateSeparableLinearFilter directly."
cv::createLinearFilter: |-
  Creates a non-separable linear filter engine.
  
  The function returns a pointer to a 2D linear filter for the specified
  kernel, the source array type, and the destination array type. The
  function is a higher-level function that calls `getLinearFilter` and
  passes the retrieved 2D filter to the :ocvFilterEngine constructor.
cv::createMorphologyFilter: "Creates an engine for non-separable morphological operations.\n\n\
  The functions construct primitive morphological filtering operations or\n\
  a filter engine based on them. Normally it is enough to use\n\
  :ocvcreateMorphologyFilter or even higher-level :ocverode, :ocvdilate ,\n\
  or :ocvmorphologyEx . Note that :ocvcreateMorphologyFilter analyzes the\n\
  structuring element shape and builds a separable morphological filter\n\
  engine when the structuring element is square."
cv::createSeparableLinearFilter: "Creates an engine for a separable linear filter.\n\n\
  The functions construct primitive separable linear filtering operations\n\
  or a filter engine based on them. Normally it is enough to use\n\
  :ocvcreateSeparableLinearFilter or even higher-level :ocvsepFilter2D .\n\
  The function :ocvcreateMorphologyFilter is smart enough to figure out\n\
  the `symmetryType` for each of the two kernels, the intermediate\n\
  `bufType` and, if filtering can be done in integer arithmetics, the\n\
  number of `bits` to encode the filter coefficients. If it does not work\n\
  for you, it is possible to call\n\
  `getLinearColumnFilter`,`getLinearRowFilter` directly and then pass them\n\
  to the :ocvFilterEngine constructor."
cv::dilate: |-
  Dilates an image by using a specific structuring element.
  
  The function dilates the source image using the specified structuring
  element that determines the shape of a pixel neighborhood over which the
  maximum is taken:
  
  $$\texttt{dst} (x,y) =  \max _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')$$
  
  The function supports the in-place mode. Dilation can be applied several
  ( `iterations` ) times. In case of multi-channel images, each channel is
  processed independently.
cv::erode: |-
  Erodes an image by using a specific structuring element.
  
  The function erodes the source image using the specified structuring
  element that determines the shape of a pixel neighborhood over which the
  minimum is taken:
  
  $$\texttt{dst} (x,y) =  \min _{(x',y'):  \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')$$
  
  The function supports the in-place mode. Erosion can be applied several
  ( `iterations` ) times. In case of multi-channel images, each channel is
  processed independently.
cv::filter2D: |-
  Convolves an image with the kernel.
  
  The function applies an arbitrary linear filter to an image. In-place
  operation is supported. When the aperture is partially outside the
  image, the function interpolates outlier pixel values according to the
  specified border mode.
  
  The function does actually compute correlation, not the convolution:
  
  $$\texttt{dst} (x,y) =  \sum _{ \stackrel{0\leq x' < \texttt{kernel.cols},}{0\leq y' < \texttt{kernel.rows}} }  \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )$$
  
  That is, the kernel is not mirrored around the anchor point. If you need
  a real convolution, flip the kernel using :ocvflip and set the new
  anchor to `(kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1)` .
  
  The function uses the DFT-based algorithm in case of sufficiently large
  kernels (~`11 x 11` or larger) and the direct algorithm (that uses the
  engine retrieved by :ocvcreateLinearFilter ) for small kernels.
cv::GaussianBlur: |-
  Blurs an image using a Gaussian filter.
  
  The function convolves the source image with the specified Gaussian
  kernel. In-place filtering is supported.
cv::getDerivKernels: "Returns filter coefficients for computing spatial image derivatives.\n\n\
  The function computes and returns the filter coefficients for spatial\n\
  image derivatives. When `ksize=CV_SCHARR` , the Scharr $3 \\times 3$\n\
  kernels are generated (see :ocvScharr ). Otherwise, Sobel kernels are\n\
  generated (see :ocvSobel ). The filters are normally passed to\n\
  :ocvsepFilter2D or to :ocvcreateSeparableLinearFilter ."
cv::getGaussianKernel: "Returns Gaussian filter coefficients.\n\n\
  The function computes and returns the $\\texttt{ksize} \\times 1$ matrix\n\
  of Gaussian filter coefficients:\n\n\
  $$G_i= \\alpha *e^{-(i-( \\texttt{ksize} -1)/2)^2/(2* \\texttt{sigma} )^2},$$\n\n\
  where $i=0..\\texttt{ksize}-1$ and $\\alpha$ is the scale factor chosen so\n\
  that $\\sum_i G_i=1$.\n\n\
  Two of such generated kernels can be passed to :ocvsepFilter2D or to\n\
  :ocvcreateSeparableLinearFilter. Those functions automatically recognize\n\
  smoothing kernels (a symmetrical kernel with sum of weights equal to 1)\n\
  and handle them accordingly. You may also use the higher-level\n\
  :ocvGaussianBlur."
cv::getKernelType: |
  Returns the kernel type.
  
  The function analyzes the kernel coefficients and returns the
  corresponding kernel type:
  
    **KERNEL_GENERAL** The kernel is generic. It is used when there
  is no any type of symmetry or other properties.
  
  
    **KERNEL_SYMMETRICAL** The kernel is symmetrical:
  $\texttt{kernel}_i == \texttt{kernel}_{ksize-i-1}$ , and the
  anchor is at the center.
  
  
    **KERNEL_ASYMMETRICAL** The kernel is asymmetrical:
  $\texttt{kernel}_i == -\texttt{kernel}_{ksize-i-1}$ , and the
  anchor is at the center.
  
  
    **KERNEL_SMOOTH** All the kernel elements are non-negative and
  summed to 1. For example, the Gaussian kernel is both smooth
  kernel and symmetrical, so the function returns
  `KERNEL_SMOOTH | KERNEL_SYMMETRICAL` .
  
  
    **KERNEL_INTEGER** All the kernel coefficients are integer
  numbers. This flag can be combined with `KERNEL_SYMMETRICAL` or
  `KERNEL_ASYMMETRICAL` .

cv::getStructuringElement: "\n\n\
  Returns a structuring element of the specified size and shape for\n\
  morphological operations.\n\n\
  The function constructs and returns the structuring element that can be\n\
  further passed to :ocvcreateMorphologyFilter, :ocverode, :ocvdilate or\n\
  :ocvmorphologyEx . But you can also construct an arbitrary binary mask\n\
  yourself and use it as the structuring element.\n\n\
  **note**\n\n\
  When using OpenCV 1.x C API, the created structuring element\n\
  `IplConvKernel* element` must be released in the end using\n\
  `cvReleaseStructuringElement(&element)`.\n"
cv::medianBlur: |-
  Blurs an image using the median filter.
  
  The function smoothes an image using the median filter with the
  $\texttt{ksize} \times \texttt{ksize}$ aperture. Each channel of a
  multi-channel image is processed independently. In-place operation is
  supported.
cv::morphologyEx: |-
  Performs advanced morphological transformations.
  
  The function can perform advanced morphological transformations using an
  erosion and dilation as basic operations.
  
  Opening operation:
  
  $$\texttt{dst} = \mathrm{open} ( \texttt{src} , \texttt{element} )= \mathrm{dilate} ( \mathrm{erode} ( \texttt{src} , \texttt{element} ))$$
  
  Closing operation:
  
  $$\texttt{dst} = \mathrm{close} ( \texttt{src} , \texttt{element} )= \mathrm{erode} ( \mathrm{dilate} ( \texttt{src} , \texttt{element} ))$$
  
  Morphological gradient:
  
  $$\texttt{dst} = \mathrm{morph_grad} ( \texttt{src} , \texttt{element} )= \mathrm{dilate} ( \texttt{src} , \texttt{element} )- \mathrm{erode} ( \texttt{src} , \texttt{element} )$$
  
  "Top hat":
  
  $$\texttt{dst} = \mathrm{tophat} ( \texttt{src} , \texttt{element} )= \texttt{src} - \mathrm{open} ( \texttt{src} , \texttt{element} )$$
  
  "Black hat":
  
  $$\texttt{dst} = \mathrm{blackhat} ( \texttt{src} , \texttt{element} )= \mathrm{close} ( \texttt{src} , \texttt{element} )- \texttt{src}$$
  
  Any of the operations can be done in-place. In case of multi-channel
  images, each channel is processed independently.
cv::Laplacian: |-
  Calculates the Laplacian of an image.
  
  The function calculates the Laplacian of the source image by adding up
  the second x and y derivatives calculated using the Sobel operator:
  
  $$\texttt{dst} =  \Delta \texttt{src} =  \frac{\partial^2 \texttt{src}}{\partial x^2} +  \frac{\partial^2 \texttt{src}}{\partial y^2}$$
  
  This is done when `ksize > 1` . When `ksize == 1` , the Laplacian is
  computed by filtering the image with the following $3 \times 3$
  aperture:
  
  $$\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}$$
cv::pyrDown: |-
  Blurs an image and downsamples it.
  
  The function performs the downsampling step of the Gaussian pyramid
  construction. First, it convolves the source image with the kernel:
  
  $$\frac{1}{256} \begin{bmatrix} 1 & 4 & 6 & 4 & 1  \ 4 & 16 & 24 & 16 & 4  \ 6 & 24 & 36 & 24 & 6  \ 4 & 16 & 24 & 16 & 4  \ 1 & 4 & 6 & 4 & 1 \end{bmatrix}$$
  
  Then, it downsamples the image by rejecting even rows and columns.
cv::pyrUp: |-
  Upsamples an image and then blurs it.
  
  The function performs the upsampling step of the Gaussian pyramid
  construction, though it can actually be used to construct the Laplacian
  pyramid. First, it upsamples the source image by injecting even zero
  rows and columns and then convolves the result with the same kernel as
  in :ocvpyrDown multiplied by 4.
cv::pyrMeanShiftFiltering: |-
  Performs initial step of meanshift segmentation of an image.
  
  The function implements the filtering stage of meanshift segmentation,
  that is, the output of the function is the filtered "posterized" image
  with color gradients and fine-grain texture flattened. At every pixel
  `(X,Y)` of the input image (or down-sized input image, see below) the
  function executes meanshift iterations, that is, the pixel `(X,Y)`
  neighborhood in the joint space-color hyperspace is considered:
  
  $$(x,y): X- \texttt{sp} \le x  \le X+ \texttt{sp} , Y- \texttt{sp} \le y  \le Y+ \texttt{sp} , ||(R,G,B)-(r,g,b)||   \le \texttt{sr}$$
  
  
  where `(R,G,B)` and `(r,g,b)` are the vectors of color components at
  `(X,Y)` and `(x,y)`, respectively (though, the algorithm does not depend
  on the color space used, so any 3-component color space can be used
  instead). Over the neighborhood the average spatial value `(X',Y')` and
  average color vector `(R',G',B')` are found and they act as the
  neighborhood center on the next iteration:
  
  $$(X,Y)~(X',Y'), (R,G,B)~(R',G',B').$$
  
  
  After the iterations over, the color components of the initial pixel
  (that is, the pixel from where the iterations started) are set to the
  final value (average color at the last iteration):
  
  $$I(X,Y) <- (R*,G*,B*)$$
  
  
  When `maxLevel > 0`, the gaussian pyramid of `maxLevel+1` levels is
  built, and the above procedure is run on the smallest layer first. After
  that, the results are propagated to the larger layer and the iterations
  are run again only on those pixels where the layer colors differ by more
  than `sr` from the lower-resolution layer of the pyramid. That makes
  boundaries of color regions sharper. Note that the results will be
  actually different from the ones obtained by running the meanshift
  procedure on the whole original image (i.e. when `maxLevel==0`).
cv::sepFilter2D: |-
  Applies a separable linear filter to an image.
  
  The function applies a separable linear filter to the image. That is,
  first, every row of `src` is filtered with the 1D kernel `kernelX` .
  Then, every column of the result is filtered with the 1D kernel
  `kernelY` . The final result shifted by `delta` is stored in `dst` .
cv::Smooth: |
  Smooths the image in one of several ways.
  
  The function smooths an image using one of several methods. Every of the
  methods has some features and restrictions listed below:
  
    Blur with no scaling works with single-channel images only and
  supports accumulation of 8-bit to 16-bit format (similar to
  :ocvSobel and :ocvLaplacian) and 32-bit floating point to 32-bit
  floating-point format.
  
  
    Simple blur and Gaussian blur support 1- or 3-channel, 8-bit and
  32-bit floating point images. These two methods can process images
  in-place.
  
  
    Median and bilateral filters work with 1- or 3-channel 8-bit
  images and can not process images in-place.
  
  
  **note**
  
  The function is now obsolete. Use :ocvGaussianBlur, :ocvblur,
  :ocvmedianBlur or :ocvbilateralFilter.

cv::Sobel: |-
  Calculates the first, second, third, or mixed image derivatives using an
  extended Sobel operator.
  
  In all cases except one, the $\texttt{ksize} \times
  \texttt{ksize}$ separable kernel is used to calculate the derivative.
  When $\texttt{ksize = 1}$ , the $3 \times 1$ or $1 \times 3$ kernel is
  used (that is, no Gaussian smoothing is done). `ksize = 1` can only be
  used for the first or the second x- or y- derivatives.
  
  There is also the special value `ksize = CV_SCHARR` (-1) that
  corresponds to the $3\times3$ Scharr filter that may give more accurate
  results than the $3\times3$ Sobel. The Scharr aperture is
  
  $$\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}$$
  
  for the x-derivative, or transposed for the y-derivative.
  
  The function calculates an image derivative by convolving the image with
  the appropriate kernel:
  
  $$\texttt{dst} =  \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}$$
  
  The Sobel operators combine Gaussian smoothing and differentiation, so
  the result is more or less resistant to the noise. Most often, the
  function is called with ( `xorder` = 1, `yorder` = 0, `ksize` = 3) or (
  `xorder` = 0, `yorder` = 1, `ksize` = 3) to calculate the first x- or y-
  image derivative. The first case corresponds to a kernel of:
  
  $$\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}$$
  
  The second case corresponds to a kernel of:
  
  $$\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}$$
cv::Scharr: |-
  Calculates the first x- or y- image derivative using Scharr operator.
  
  The function computes the first x- or y- spatial image derivative using
  the Scharr operator. The call
  
  $$\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}$$
  
  is equivalent to
  
  $$\texttt{Sobel(src, dst, ddepth, dx, dy, CV_SCHARR, scale, delta, borderType)} .$$
cv::flann::hierarchicalClustering<Distance>: |-
  Clusters features using hierarchical k-means algorithm.
  
  The method clusters the given feature vectors by constructing a
  hierarchical k-means tree and choosing a cut in the tree that minimizes
  the cluster's variance. It returns the number of clusters found.
cv::Fast Approximate Nearest Neighbor Search: |-
  This section documents OpenCV's interface to the FLANN library. FLANN
  (Fast Library for Approximate Nearest Neighbors) is a library that
  contains a collection of algorithms optimized for fast nearest neighbor
  search in large datasets and for high dimensional features. More
  information about FLANN can be found in [Muja2009]_ .
cv::flann::Index_: |-
  The FLANN nearest neighbor index class. This class is templated with the
  type of elements for which the index is built.
cv::flann::Index_<T>::Index_: |
  Constructs a nearest neighbor search index for a given dataset.
  
  The method constructs a fast search structure from a set of features
  using the specified algorithm with specified parameters, as defined by
  `params`. `params` is a reference to one of the following class
  `IndexParams` descendants:
  
  *
  
  **LinearIndexParams** When passing an object of this type, the index
  will perform a linear, brute-force search. :
  
  ```c++
  struct LinearIndexParams : public IndexParams
  {
  };
  ```
  
  
  
  *
  
  **KDTreeIndexParams** When passing an object of this type the index
  constructed will consist of a set of randomized kd-trees which will
  be searched in parallel. :
  
  ```c++
  struct KDTreeIndexParams : public IndexParams
  {
      KDTreeIndexParams( int trees = 4 );
  };
  ```
  
  
  
  *
  
  **KMeansIndexParams** When passing an object of this type the index
  constructed will be a hierarchical k-means tree. :
  
  ```c++
  struct KMeansIndexParams : public IndexParams
  {
      KMeansIndexParams(
          int branching = 32,
          int iterations = 11,
          flann_centers_init_t centers_init = CENTERS_RANDOM,
          float cb_index = 0.2 );
  };
  ```
  
  
  
  *
  :   **CompositeIndexParams** When using a parameters object of this
      type the index created combines the randomized kd-trees and the
      hierarchical k-means tree. :
  
  ```c++
      struct CompositeIndexParams : public IndexParams
      {
          CompositeIndexParams(
              int trees = 4,
              int branching = 32,
              int iterations = 11,
              flann_centers_init_t centers_init = CENTERS_RANDOM,
              float cb_index = 0.2 );
      };
  ```
  
  
  *
  :   **LshIndexParams** When using a parameters object of this type the
      index created uses multi-probe LSH (by
      `Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search`
      by Qin Lv, William Josephson, Zhe Wang, Moses Charikar, Kai Li.,
      Proceedings of the 33rd International Conference on Very Large
      Data Bases (VLDB). Vienna, Austria. September 2007) :
  
  ```c++
      struct LshIndexParams : public IndexParams
      {
          LshIndexParams(
              unsigned int table_number,
              unsigned int key_size,
              unsigned int multi_probe_level );
      };
  ```
  
  
  *
  :   **AutotunedIndexParams** When passing an object of this type the
      index created is automatically tuned to offer the best
      performance, by choosing the optimal index type (randomized
      kd-trees, hierarchical kmeans, linear) and parameters for the
      dataset provided. :
  
  ```c++
      struct AutotunedIndexParams : public IndexParams
      {
          AutotunedIndexParams(
              float target_precision = 0.9,
              float build_weight = 0.01,
              float memory_weight = 0,
              float sample_fraction = 0.1 );
      };
  ```
  
  
  *
  :   **SavedIndexParams** This object type is used for loading a
      previously saved index from the disk. :
  
  ```c++
      struct SavedIndexParams : public IndexParams
      {
          SavedIndexParams( String filename );
      };
  ```

cv::flann::Index_<T>::knnSearch: "\n\n\
  Performs a K-nearest neighbor search for a given query point using the\n\
  index."
cv::flann::Index_<T>::radiusSearch: Performs a radius nearest neighbor search for a given query point.
cv::flann::Index_<T>::save: Saves the index to a file.
cv::flann::Index_<T>::getIndexParameters: |-
  Returns the index parameters.
  
  The method is useful in the case of auto-tuned indices, when the
  parameters are chosen during the index construction. Then, the method
  can be used to retrieve the actual parameter values.
cv::Geometric Image Transformations: |-
  The functions in this section perform various geometrical
  transformations of 2D images. They do not change the image content but
  deform the pixel grid and map this deformed grid to the destination
  image. In fact, to avoid sampling artifacts, the mapping is done in the
  reverse order, from destination to the source. That is, for each pixel
  $(x, y)$ of the destination image, the functions compute coordinates of
  the corresponding "donor" pixel in the source image and copy the pixel
  value:
  
  $$\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))$$
  
  In case when you specify the forward mapping
  $\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}$ , the
  OpenCV functions first compute the corresponding inverse mapping
  $\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}$ and then
  use the above formula.
  
  The actual implementations of the geometrical transformations, from the
  most generic :ocvremap and to the simplest and the fastest :ocvresize ,
  need to solve two main problems with the above formula:
  
  *
  :   Extrapolation of non-existing pixels. Similarly to the filtering
      functions described in the previous section, for some $(x,y)$ ,
      either one of $f_x(x,y)$ , or $f_y(x,y)$ , or both of them may fall
      outside of the image. In this case, an extrapolation method needs to
      be used. OpenCV provides the same selection of extrapolation methods
      as in the filtering functions. In addition, it provides the method
      `BORDER_TRANSPARENT` . This means that the corresponding pixels in
      the destination image will not be modified at all.
  
  *
  :   Interpolation of pixel values. Usually $f_x(x,y)$ and $f_y(x,y)$ are
      floating-point numbers. This means that $\left<f_x, f_y\right>$ can
      be either an affine or perspective transformation, or radial lens
      distortion correction, and so on. So, a pixel value at fractional
      coordinates needs to be retrieved. In the simplest case, the
      coordinates can be just rounded to the nearest integer coordinates
      and the corresponding pixel can be used. This is called a
      nearest-neighbor interpolation. However, a better result can be
      achieved by using more sophisticated [interpolation
      methods](http://en.wikipedia.org/wiki/Multivariate_interpolation) ,
      where a polynomial function is fit into some neighborhood of the
      computed pixel $(f_x(x,y), f_y(x,y))$ , and then the value of the
      polynomial at $(f_x(x,y), f_y(x,y))$ is taken as the interpolated
      pixel value. In OpenCV, you can choose between several interpolation
      methods. See :ocvresize for details.
cv::convertMaps: |-
  Converts image transformation maps from one representation to another.
  
  The function converts a pair of maps for :ocvremap from one
  representation to another. The following options (
  `(map1.type(), map2.type())` $\rightarrow$
  `(dstmap1.type(), dstmap2.type())` ) are supported:
  
  *
  :   $\texttt{(CV_32FC1, CV_32FC1)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}$
      . This is the most frequently used conversion operation, in which
      the original floating-point maps (see :ocvremap ) are converted to a
      more compact and much faster fixed-point representation. The first
      output array contains the rounded coordinates and the second array
      (created only when `nninterpolation=false` ) contains indices in the
      interpolation tables.
  
  *
  :   $\texttt{(CV_32FC2)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}$ .
      The same as above but the original maps are stored in one 2-channel
      matrix.
  
  *
  :   Reverse conversion. Obviously, the reconstructed floating-point maps
      will not be exactly the same as the originals.
cv::getAffineTransform: |-
  Calculates an affine transform from three pairs of the corresponding
  points.
  
  The function calculates the $2 \times 3$ matrix of an affine transform
  so that:
  
  $$\begin{bmatrix} x'_i \ y'_i \end{bmatrix} = \texttt{map_matrix} \cdot \begin{bmatrix} x_i \ y_i \ 1 \end{bmatrix}$$
  
  where
  
  $$dst(i)=(x'_i,y'_i),
  src(i)=(x_i, y_i),
  i=0,1,2$$
cv::getPerspectiveTransform: |-
  Calculates a perspective transform from four pairs of the corresponding
  points.
  
  The function calculates the $3 \times 3$ matrix of a perspective
  transform so that:
  
  $$\begin{bmatrix} t_i x'_i \ t_i y'_i \ t_i \end{bmatrix} = \texttt{map_matrix} \cdot \begin{bmatrix} x_i \ y_i \ 1 \end{bmatrix}$$
  
  where
  
  $$dst(i)=(x'_i,y'_i),
  src(i)=(x_i, y_i),
  i=0,1,2,3$$
cv::getRectSubPix: |-
  Retrieves a pixel rectangle from an image with sub-pixel accuracy.
  
  The function `getRectSubPix` extracts pixels from `src` :
  
  $$dst(x, y) = src(x +  \texttt{center.x} - ( \texttt{dst.cols} -1)*0.5, y +  \texttt{center.y} - ( \texttt{dst.rows} -1)*0.5)$$
  
  where the values of the pixels at non-integer coordinates are retrieved
  using bilinear interpolation. Every channel of multi-channel images is
  processed independently. While the center of the rectangle must be
  inside the image, parts of the rectangle may be outside. In this case,
  the replication border mode (see :ocvborderInterpolate ) is used to
  extrapolate the pixel values outside of the image.
cv::getRotationMatrix2D: |-
  Calculates an affine matrix of 2D rotation.
  
  The function calculates the following matrix:
  
  $$\begin{bmatrix} \alpha &  \beta & (1- \alpha )  \cdot \texttt{center.x} -  \beta \cdot \texttt{center.y} \ - \beta &  \alpha &  \beta \cdot \texttt{center.x} + (1- \alpha )  \cdot \texttt{center.y} \end{bmatrix}$$
  
  where
  
  $$\begin{array}{l} \alpha =  \texttt{scale} \cdot \cos \texttt{angle} , \ \beta =  \texttt{scale} \cdot \sin \texttt{angle} \end{array}$$
  
  The transformation maps the rotation center to itself. If this is not
  the target, adjust the shift.
cv::invertAffineTransform: |-
  Inverts an affine transformation.
  
  The function computes an inverse affine transformation represented by
  $2 \times 3$ matrix `M` :
  
  $$\begin{bmatrix} a_{11} & a_{12} & b_1  \ a_{21} & a_{22} & b_2 \end{bmatrix}$$
  
  The result is also a $2 \times 3$ matrix of the same type as `M` .
cv::LogPolar: "Remaps an image to log-polar space.\n\n\
  The function `cvLogPolar` transforms the source image using the\n\
  following transformation:\n\n\
  *\n\
  :   Forward transformation (`CV_WARP_INVERSE_MAP` is not set):\n\n\
  ```c++\n\
  > $$dst( \\phi , \\rho ) = src(x,y)$$\n\
  ```\n\n\n\
  *\n\
  :   Inverse transformation (`CV_WARP_INVERSE_MAP` is set):\n\n\
  ```c++\n\
  > $$dst(x,y) = src( \\phi , \\rho )$$\n\
  ```\n\n\n\
  where\n\n\
  $$\\rho = M  \\cdot \\log{\\sqrt{x^2 + y^2}} , \\phi =atan(y/x)$$\n\n\n\
  The function emulates the human \"foveal\" vision and can be used for fast\n\
  scale and rotation-invariant template matching, for object tracking and\n\
  so forth. The function can not operate in-place."
cv::remap: |-
  Applies a generic geometrical transformation to an image.
  
  The function `remap` transforms the source image using the specified
  map:
  
  $$\texttt{dst} (x,y) =  \texttt{src} (map_x(x,y),map_y(x,y))$$
  
  where values of pixels with non-integer coordinates are computed using
  one of available interpolation methods. $map_x$ and $map_y$ can be
  encoded as separate floating-point maps in $map_1$ and $map_2$
  respectively, or interleaved floating-point maps of $(x,y)$ in $map_1$ ,
  or fixed-point maps created by using :ocvconvertMaps . The reason you
  might want to convert from floating to fixed-point representations of a
  map is that they can yield much faster (~2x) remapping operations. In
  the converted case, $map_1$ contains pairs `(cvFloor(x), cvFloor(y))`
  and $map_2$ contains indices in a table of interpolation coefficients.
  
  This function cannot operate in-place.
cv::resize: |-
  Resizes an image.
  
  The function `resize` resizes the image `src` down to or up to the
  specified size. Note that the initial `dst` type or size are not taken
  into account. Instead, the size and type are derived from the
  `src`,`dsize`,`fx` , and `fy` . If you want to resize `src` so that it
  fits the pre-created `dst` , you may call the function as follows: :
  
  ```c++
  // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
  resize(src, dst, dst.size(), 0, 0, interpolation);
  ```
  
  
  If you want to decimate the image by factor of 2 in each direction, you
  can call the function this way: :
  
  ```c++
  // specify fx and fy and let the function compute the destination image size.
  resize(src, dst, Size(), 0.5, 0.5, interpolation);
  ```
  
  
  To shrink an image, it will generally look best with CV_INTER_AREA
  interpolation, whereas to enlarge an image, it will generally look best
  with CV_INTER_CUBIC (slow) or CV_INTER_LINEAR (faster but still
  looks OK).
cv::warpAffine: |
  Applies an affine transformation to an image.
  
  The function `warpAffine` transforms the source image using the
  specified matrix:
  
  $$\texttt{dst} (x,y) =  \texttt{src} ( \texttt{M} _{11} x +  \texttt{M} _{12} y +  \texttt{M} _{13}, \texttt{M} _{21} x +  \texttt{M} _{22} y +  \texttt{M} _{23})$$
  
  when the flag `WARP_INVERSE_MAP` is set. Otherwise, the transformation
  is first inverted with :ocvinvertAffineTransform and then put in the
  formula above instead of `M` . The function cannot operate in-place.
  
  **note**
  
  `cvGetQuadrangleSubPix` is similar to `cvWarpAffine`, but the outliers
  are extrapolated using replication border mode.

cv::warpPerspective: |-
  Applies a perspective transformation to an image.
  
  The function `warpPerspective` transforms the source image using the
  specified matrix:
  
  $$\texttt{dst} (x,y) =  \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} ,
       \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )$$
  
  when the flag `WARP_INVERSE_MAP` is set. Otherwise, the transformation
  is first inverted with :ocvinvert and then put in the formula above
  instead of `M` . The function cannot operate in-place.
cv::initUndistortRectifyMap: "Computes the undistortion and rectification transformation map.\n\n\
  The function computes the joint undistortion and rectification\n\
  transformation and represents the result in the form of maps for\n\
  :ocvremap . The undistorted image looks like original, as if it is\n\
  captured with a camera using the camera matrix `=newCameraMatrix` and\n\
  zero distortion. In case of a monocular camera, `newCameraMatrix` is\n\
  usually equal to `cameraMatrix` , or it can be computed by\n\
  :ocvgetOptimalNewCameraMatrix for a better control over scaling. In case\n\
  of a stereo camera, `newCameraMatrix` is normally set to `P1` or `P2`\n\
  computed by :ocvstereoRectify .\n\n\
  Also, this new camera is oriented differently in the coordinate space,\n\
  according to `R` . That, for example, helps to align two heads of a\n\
  stereo camera so that the epipolar lines on both images become\n\
  horizontal and have the same y- coordinate (in case of a horizontally\n\
  aligned stereo camera).\n\n\
  The function actually builds the maps for the inverse mapping algorithm\n\
  that is used by :ocvremap . That is, for each pixel $(u, v)$ in the\n\
  destination (corrected and rectified) image, the function computes the\n\
  corresponding coordinates in the source image (that is, in the original\n\
  image from camera). The following process is applied:\n\n\
  $$\\begin{array}{l} x  \\leftarrow (u - {c'}_x)/{f'}_x  \\ y  \\leftarrow (v - {c'}_y)/{f'}_y  \\{[X\\,Y\\,W]} ^T  \\leftarrow R^{-1}*[x \\, y \\, 1]^T  \\ x'  \\leftarrow X/W  \\ y'  \\leftarrow Y/W  \\ x\"  \\leftarrow x' (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + 2p_1 x' y' + p_2(r^2 + 2 x'^2)  \\ y\"  \\leftarrow y' (1 + k_1 r^2 + k_2 r^4 + k_3 r^6) + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y'  \\ map_x(u,v)  \\leftarrow x\" f_x + c_x  \\ map_y(u,v)  \\leftarrow y\" f_y + c_y \\end{array}$$\n\n\
  where $(k_1, k_2, p_1, p_2[, k_3])$ are the distortion coefficients.\n\n\
  In case of a stereo camera, this function is called twice: once for each\n\
  camera head, after :ocvstereoRectify , which in its turn is called after\n\
  :ocvstereoCalibrate . But if the stereo camera was not calibrated, it is\n\
  still possible to compute the rectification transformations directly\n\
  from the fundamental matrix using :ocvstereoRectifyUncalibrated . For\n\
  each camera, the function computes homography `H` as the rectification\n\
  transformation in a pixel domain, not a rotation matrix `R` in 3D space.\n\
  `R` can be computed from `H` as\n\n\
  $$\\texttt{R} =  \\texttt{cameraMatrix} ^{-1}  \\cdot \\texttt{H} \\cdot \\texttt{cameraMatrix}$$\n\n\
  where `cameraMatrix` can be chosen arbitrarily."
cv::getDefaultNewCameraMatrix: "Returns the default new camera matrix.\n\n\
  The function returns the camera matrix that is either an exact copy of\n\
  the input `cameraMatrix` (when `centerPrinicipalPoint=false` ), or the\n\
  modified one (when `centerPrincipalPoint=true`).\n\n\
  In the latter case, the new camera matrix will be:\n\n\
  $$\\begin{bmatrix} f_x && 0 && ( \\texttt{imgSize.width} -1)*0.5  \\ 0 && f_y && ( \\texttt{imgSize.height} -1)*0.5  \\ 0 && 0 && 1 \\end{bmatrix} ,$$\n\n\
  where $f_x$ and $f_y$ are $(0,0)$ and $(1,1)$ elements of `cameraMatrix`\n\
  , respectively.\n\n\
  By default, the undistortion functions in OpenCV (see\n\
  :ocvinitUndistortRectifyMap, :ocvundistort) do not move the principal\n\
  point. However, when you work with stereo, it is important to move the\n\
  principal points in both views to the same y-coordinate (which is\n\
  required by most of stereo correspondence algorithms), and may be to the\n\
  same x-coordinate too. So, you can form the new camera matrix for each\n\
  view where the principal points are located at the center."
cv::undistort: "Transforms an image to compensate for lens distortion.\n\n\
  The function transforms an image to compensate radial and tangential\n\
  lens distortion.\n\n\
  The function is simply a combination of :ocvinitUndistortRectifyMap\n\
  (with unity `R` ) and :ocvremap (with bilinear interpolation). See the\n\
  former function for details of the transformation being performed.\n\n\
  Those pixels in the destination image, for which there is no\n\
  correspondent pixels in the source image, are filled with zeros (black\n\
  color).\n\n\
  A particular subset of the source image that will be visible in the\n\
  corrected image can be regulated by `newCameraMatrix` . You can use\n\
  :ocvgetOptimalNewCameraMatrix to compute the appropriate\n\
  `newCameraMatrix` depending on your requirements.\n\n\
  The camera matrix and the distortion parameters can be determined using\n\
  :ocvcalibrateCamera . If the resolution of images is different from the\n\
  resolution used at the calibration stage, $f_x, f_y, c_x$ and $c_y$ need\n\
  to be scaled accordingly, while the distortion coefficients remain the\n\
  same."
cv::undistortPoints: |-
  Computes the ideal point coordinates from the observed point
  coordinates.
  
  The function is similar to :ocvundistort and :ocvinitUndistortRectifyMap
  but it operates on a sparse set of points instead of a raster image.
  Also the function performs a reverse transformation to :ocvprojectPoints
  . In case of a 3D object, it does not reconstruct its 3D coordinates,
  but for a planar object, it does, up to a translation vector, if the
  proper `R` is specified. :
  
  ```c++
  // (u,v) is the input point, (u', v') is the output point
  // camera_matrix=[fx 0 cx; 0 fy cy; 0 0 1]
  // P=[fx' 0 cx' tx; 0 fy' cy' ty; 0 0 1 tz]
  x" = (u - cx)/fx
  y" = (v - cy)/fy
  (x',y') = undistort(x",y",dist_coeffs)
  [X,Y,W]T = R*[x' y' 1]T
  x = X/W, y = Y/W
  // only performed if P=[fx' 0 cx' [tx]; 0 fy' cy' [ty]; 0 0 1 [tz]] is specified
  u' = x*fx' + cx'
  v' = y*fy' + cy',
  ```
  
  
  where `undistort()` is an approximate iterative algorithm that estimates
  the normalized original point coordinates out of the normalized
  distorted point coordinates ("normalized" means that the coordinates do
  not depend on the camera matrix).
  
  The function can be used for both a stereo camera head or a monocular
  camera (when R is empty).
cv::Global Motion Estimation: |-
  The video stabilization module contains a set of functions and classes
  for global motion estimation between point clouds or between images. In
  the last case features are extracted and matched internally. For the
  sake of convenience the motion estimation functions are wrapped into
  classes. Both the functions and the classes are available.
cv::videostab::MotionModel: Describes motion model between two point clouds.
cv::videostab::RansacParams: |
  Describes RANSAC method parameters.
  
  ```c++
  struct RansacParams
  {
      int size; // subset size
      float thresh; // max error to classify as inlier
      float eps; // max outliers ratio
      float prob; // probability of success
  
      RansacParams() : size(0), thresh(0), eps(0), prob(0) {}
      RansacParams(int size, float thresh, float eps, float prob);
  
      int niters() const;
  
      static RansacParams default2dMotion(MotionModel model);
  };
  ```

cv::videostab::estimateGlobalMotionLeastSquares: |
  Estimates best global motion between two 2D point clouds in the
  least-squares sense.
  
  **note**
  
  Works in-place and changes input point arrays.

cv::videostab::estimateGlobalMotionRansac: |-
  Estimates best global motion between two 2D point clouds robustly (using
  RANSAC method).
cv::videostab::getMotion: |-
  Computes motion between two frames assuming that all the intermediate
  motions are known.
cv::videostab::MotionEstimatorBase: |
  Base class for all global motion estimation methods.
  
  ```c++
  class MotionEstimatorBase
  {
  public:
      virtual ~MotionEstimatorBase();
  
      virtual void setMotionModel(MotionModel val);
      virtual MotionModel motionModel() const;
  
      virtual Mat estimate(InputArray points0, InputArray points1, bool *ok = 0) = 0;
  };
  ```

cv::videostab::MotionEstimatorBase::setMotionModel: Sets motion model.
cv::videostab::MotionEstimatorBase::estimate: Estimates global motion between two 2D point clouds.
cv::videostab::MotionEstimatorRansacL2: |
  Describes a robust RANSAC-based global 2D motion estimation method which
  minimizes L2 error.
  
  ```c++
  class MotionEstimatorRansacL2 : public MotionEstimatorBase
  {
  public:
      MotionEstimatorRansacL2(MotionModel model = MM_AFFINE);
  
      void setRansacParams(const RansacParams &val);
      RansacParams ransacParams() const;
  
      void setMinInlierRatio(float val);
      float minInlierRatio() const;
  
      virtual Mat estimate(InputArray points0, InputArray points1, bool *ok = 0);
  };
  ```

cv::videostab::MotionEstimatorL1: |
  Describes a global 2D motion estimation method which minimizes L1 error.
  
  **note**
  
  To be able to use this method you must build OpenCV with CLP library
  support.
  
  
  ```c++
  class MotionEstimatorL1 : public MotionEstimatorBase
  {
  public:
      MotionEstimatorL1(MotionModel model = MM_AFFINE);
  
      virtual Mat estimate(InputArray points0, InputArray points1, bool *ok = 0);
  };
  ```

cv::videostab::ImageMotionEstimatorBase: |
  Base class for global 2D motion estimation methods which take frames as
  input.
  
  ```c++
  class ImageMotionEstimatorBase
  {
  public:
      virtual ~ImageMotionEstimatorBase();
  
      virtual void setMotionModel(MotionModel val);
      virtual MotionModel motionModel() const;
  
      virtual Mat estimate(const Mat &frame0, const Mat &frame1, bool *ok = 0) = 0;
  };
  ```

cv::videostab::KeypointBasedMotionEstimator: |
  Describes a global 2D motion estimation method which uses keypoints
  detection and optical flow for matching.
  
  ```c++
  class KeypointBasedMotionEstimator : public ImageMotionEstimatorBase
  {
  public:
      KeypointBasedMotionEstimator(Ptr<MotionEstimatorBase> estimator);
  
      virtual void setMotionModel(MotionModel val);
      virtual MotionModel motionModel() const;
  
      void setDetector(Ptr<FeatureDetector> val);
      Ptr<FeatureDetector> detector() const;
  
      void setOpticalFlowEstimator(Ptr<ISparseOptFlowEstimator> val);
      Ptr<ISparseOptFlowEstimator> opticalFlowEstimator() const;
  
      void setOutlierRejector(Ptr<IOutlierRejector> val);
      Ptr<IOutlierRejector> outlierRejector() const;
  
      virtual Mat estimate(const Mat &frame0, const Mat &frame1, bool *ok = 0);
  };
  ```

cv::Gradient Boosted Trees: |-
  Gradient Boosted Trees (GBT) is a generalized boosting algorithm
  introduced by Jerome Friedman:
  <http://www.salfordsystems.com/doc/GreedyFuncApproxSS.pdf> . In contrast
  to the AdaBoost.M1 algorithm, GBT can deal with both multiclass
  classification and regression problems. Moreover, it can use any
  differential loss function, some popular ones are implemented. Decision
  trees (:ocvCvDTree) usage as base learners allows to process ordered and
  categorical variables.
cv::Training the GBT model: |-
  Gradient Boosted Trees model represents an ensemble of single regression
  trees built in a greedy fashion. Training procedure is an iterative
  process similar to the numerical optimization via the gradient descent
  method. Summary loss on the training set depends only on the current
  model predictions for the training samples, in other words
  $\sum^N_{i=1}L(y_i, F(x_i)) \equiv \mathcal{L}(F(x_1), F(x_2), ... , F(x_N))
  \equiv \mathcal{L}(F)$. And the $\mathcal{L}(F)$ gradient can be
  computed as follows:
  
  $$grad(\mathcal{L}(F)) = \left( \dfrac{\partial{L(y_1, F(x_1))}}{\partial{F(x_1)}},
   \dfrac{\partial{L(y_2, F(x_2))}}{\partial{F(x_2)}}, ... ,
   \dfrac{\partial{L(y_N, F(x_N))}}{\partial{F(x_N)}} \right) .$$
  
  At every training step, a single regression tree is built to predict an
  antigradient vector components. Step length is computed corresponding to
  the loss function and separately for every region determined by the tree
  leaf. It can be eliminated by changing values of the leaves directly.
  
  See below the main scheme of the training process:
  
  #.
  :   Find the best constant model.
  
  #.
  :   For $i$ in $[1,M]$:
  
  ```c++
  \#.
  :   Compute the antigradient.
  
  \#.
  :   Grow a regression tree to predict antigradient components.
  
  \#.
  :   Change values in the tree leaves.
  
  \#.
  :   Add the tree to the model.
  ```
  
  
  The following loss functions are implemented for regression problems:
  
  *
  :   Squared loss (`CvGBTrees::SQUARED_LOSS`):
      $L(y,f(x))=\dfrac{1}{2}(y-f(x))^2$
  
  *
  :   Absolute loss (`CvGBTrees::ABSOLUTE_LOSS`): $L(y,f(x))=|y-f(x)|$
  
  *
  :   Huber loss (`CvGBTrees::HUBER_LOSS`):
      $L(y,f(x)) = \left{ \begin{array}{lr}
      \delta\cdot\left(|y-f(x)|-\dfrac{\delta}{2}\right) & : |y-f(x)|>\delta\
      \dfrac{1}{2}\cdot(y-f(x))^2 & : |y-f(x)|\leq\delta \end{array} \right.$,
  
  ```c++
  where $\delta$ is the $\alpha$-quantile estimation of the
  $|y-f(x)|$. In the current implementation $\alpha=0.2$.
  ```
  
  
  The following loss functions are implemented for classification
  problems:
  
  *
  :   Deviance or cross-entropy loss (`CvGBTrees::DEVIANCE_LOSS`): $K$
      functions are built, one function for each output class, and
      $L(y,f_1(x),...,f_K(x)) = -\sum^K_{k=0}1(y=k)\ln{p_k(x)}$, where
      $p_k(x)=\dfrac{\exp{f_k(x)}}{\sum^K_{i=1}\exp{f_i(x)}}$ is the
      estimation of the probability of $y=k$.
  
  As a result, you get the following model:
  
  $$f(x) = f_0 + \nu\cdot\sum^M_{i=1}T_i(x) ,$$
  
  where $f_0$ is the initial guess (the best constant model) and $\nu$ is
  a regularization parameter from the interval $(0,1]$, further called
  *shrinkage*.
cv::Predicting with the GBT Model: |-
  To get the GBT model prediction, you need to compute the sum of
  responses of all the trees in the ensemble. For regression problems, it
  is the answer. For classification problems, the result is
  $\arg\max_{i=1..K}(f_i(x))$.
cv::CvGBTreesParams: |-
  GBT training parameters.
  
  The structure contains parameters for each single decision tree in the
  ensemble, as well as the whole model characteristics. The structure is
  derived from :ocvCvDTreeParams but not all of the decision tree
  parameters are supported: cross-validation, pruning, and class
  priorities are not used.
cv::CvGBTreesParams::CvGBTreesParams: |-
  By default the following constructor is used:
  
  ~~~~ {.sourceCode .cpp}
  CvGBTreesParams(CvGBTrees::SQUARED_LOSS, 200, 0.01f, 0.8f, 3, false)
      : CvDTreeParams( 3, 10, 0, false, 10, 0, false, false, 0 )
  ~~~~
cv::CvGBTrees: |-
  The class implements the Gradient boosted tree model as described in the
  beginning of this section.
cv::CvGBTrees::CvGBTrees: "Default and training constructors.\n\n\
  The constructors follow conventions of :ocvCvStatModel::CvStatModel. See\n\
  :ocvCvStatModel::train for parameters descriptions."
cv::CvGBTrees::train: "Trains a Gradient boosted tree model.\n\n\
  The first train method follows the common template (see\n\
  :ocvCvStatModel::train). Both `tflag` values (`CV_ROW_SAMPLE`,\n\
  `CV_COL_SAMPLE`) are supported. `trainData` must be of the `CV_32F`\n\
  type. `responses` must be a matrix of type `CV_32S` or `CV_32F`. In both\n\
  cases it is converted into the `CV_32F` matrix inside the training\n\
  procedure. `varIdx` and `sampleIdx` must be a list of indices (`CV_32S`)\n\
  or a mask (`CV_8U` or `CV_8S`). `update` is a dummy parameter.\n\n\
  The second form of :ocvCvGBTrees::train function uses :ocvCvMLData as a\n\
  data set container. `update` is still a dummy parameter.\n\n\
  All parameters specific to the GBT model are passed into the training\n\
  function as a :ocvCvGBTreesParams structure."
cv::CvGBTrees::predict: |-
  Predicts a response for an input sample.
  
  The method predicts the response corresponding to the given sample (see
  Predicting with GBT). The result is either the class label or the
  estimated function value. The :ocvCvGBTrees::predict method enables
  using the parallel version of the GBT model prediction if the OpenCV is
  built with the TBB library. In this case, predictions of single trees
  are computed in a parallel fashion.
cv::CvGBTrees::clear: |-
  Clears the model.
  
  The function deletes the data set information and all the weak models
  and sets all internal variables to the initial state. The function is
  called in :ocvCvGBTrees::train and in the destructor.
cv::CvGBTrees::calc_error: "Calculates a training or testing error.\n\n\
  If the :ocvCvMLData data is used to store the data set,\n\
  :ocvCvGBTrees::calc_error can be used to get a training/testing error\n\
  easily and (optionally) all predictions on the training/testing set. If\n\
  the Intel* TBB* library is used, the error is computed in a parallel\n\
  way, namely, predictions for different samples are computed at the same\n\
  time. In case of a regression problem, a mean squared error is returned.\n\
  For classifications, the result is a misclassification error in percent."
cv::Stitcher: |
  High level image stitcher. It's possible to use this class without being
  aware of the entire stitching pipeline. However, to be able to achieve
  higher stitching stability and quality of the final images at least
  being familiar with the theory is recommended (see stitching-pipeline).
  :
  
  ```c++
  class CV_EXPORTS Stitcher
  {
  public:
      enum { ORIG_RESOL = -1 };
      enum Status { OK, ERR_NEED_MORE_IMGS };
  
      // Creates stitcher with default parameters
      static Stitcher createDefault(bool try_use_gpu = false);
  
      Status estimateTransform(InputArray images);
      Status estimateTransform(InputArray images, const std::vector<std::vector<Rect> > &rois);
  
      Status composePanorama(OutputArray pano);
      Status composePanorama(InputArray images, OutputArray pano);
  
      Status stitch(InputArray images, OutputArray pano);
      Status stitch(InputArray images, const std::vector<std::vector<Rect> > &rois, OutputArray pano);
  
      double registrationResol() const { return registr_resol_; }
      void setRegistrationResol(double resol_mpx) { registr_resol_ = resol_mpx; }
  
      double seamEstimationResol() const { return seam_est_resol_; }
      void setSeamEstimationResol(double resol_mpx) { seam_est_resol_ = resol_mpx; }
  
      double compositingResol() const { return compose_resol_; }
      void setCompositingResol(double resol_mpx) { compose_resol_ = resol_mpx; }
  
      double panoConfidenceThresh() const { return conf_thresh_; }
      void setPanoConfidenceThresh(double conf_thresh) { conf_thresh_ = conf_thresh; }
  
      bool waveCorrection() const { return do_wave_correct_; }
      void setWaveCorrection(bool flag) { do_wave_correct_ = flag; }
  
      detail::WaveCorrectKind waveCorrectKind() const { return wave_correct_kind_; }
      void setWaveCorrectKind(detail::WaveCorrectKind kind) { wave_correct_kind_ = kind; }
  
      Ptr<detail::FeaturesFinder> featuresFinder() { return features_finder_; }
      const Ptr<detail::FeaturesFinder> featuresFinder() const { return features_finder_; }
      void setFeaturesFinder(Ptr<detail::FeaturesFinder> features_finder)
          { features_finder_ = features_finder; }
  
      Ptr<detail::FeaturesMatcher> featuresMatcher() { return features_matcher_; }
      const Ptr<detail::FeaturesMatcher> featuresMatcher() const { return features_matcher_; }
      void setFeaturesMatcher(Ptr<detail::FeaturesMatcher> features_matcher)
          { features_matcher_ = features_matcher; }
  
      const cv::Mat& matchingMask() const { return matching_mask_; }
      void setMatchingMask(const cv::Mat &mask)
      {
          CV_Assert(mask.type() == CV_8U && mask.cols == mask.rows);
          matching_mask_ = mask.clone();
      }
  
      Ptr<detail::BundleAdjusterBase> bundleAdjuster() { return bundle_adjuster_; }
      const Ptr<detail::BundleAdjusterBase> bundleAdjuster() const { return bundle_adjuster_; }
      void setBundleAdjuster(Ptr<detail::BundleAdjusterBase> bundle_adjuster)
          { bundle_adjuster_ = bundle_adjuster; }
  
      Ptr<WarperCreator> warper() { return warper_; }
      const Ptr<WarperCreator> warper() const { return warper_; }
      void setWarper(Ptr<WarperCreator> warper) { warper_ = warper; }
  
      Ptr<detail::ExposureCompensator> exposureCompensator() { return exposure_comp_; }
      const Ptr<detail::ExposureCompensator> exposureCompensator() const { return exposure_comp_; }
      void setExposureCompensator(Ptr<detail::ExposureCompensator> exposure_comp)
          { exposure_comp_ = exposure_comp; }
  
      Ptr<detail::SeamFinder> seamFinder() { return seam_finder_; }
      const Ptr<detail::SeamFinder> seamFinder() const { return seam_finder_; }
      void setSeamFinder(Ptr<detail::SeamFinder> seam_finder) { seam_finder_ = seam_finder; }
  
      Ptr<detail::Blender> blender() { return blender_; }
      const Ptr<detail::Blender> blender() const { return blender_; }
      void setBlender(Ptr<detail::Blender> blender) { blender_ = blender; }
  
  private:
      /* hidden */
  };
  ```

cv::Stitcher::createDefault: Creates a stitcher with the default parameters.
cv::Stitcher::estimateTransform: |
  These functions try to match the given images and to estimate rotations
  of each camera.
  
  **note**
  
  Use the functions only if you're aware of the stitching pipeline,
  otherwise use :ocvStitcher::stitch.

cv::Stitcher::composePanorama: |
  These functions try to compose the given images (or images stored
  internally from the other function calls) into the final pano under the
  assumption that the image transformations were estimated before.
  
  **note**
  
  Use the functions only if you're aware of the stitching pipeline,
  otherwise use :ocvStitcher::stitch.

cv::Stitcher::stitch: These functions try to stitch the given images.
cv::WarperCreator: |
  Image warper factories base class. :
  
  ```c++
  class WarperCreator
  {
  public:
      virtual ~WarperCreator() {}
      virtual Ptr<detail::RotationWarper> create(float scale) const = 0;
  };
  ```

cv::PlaneWarper: |
  Plane warper factory class. :
  
  ```c++
  class PlaneWarper : public WarperCreator
  {
  public:
      Ptr<detail::RotationWarper> create(float scale) const { return new detail::PlaneWarper(scale); }
  };
  ```

cv::CylindricalWarper: |
  Cylindrical warper factory class. :
  
  ```c++
  class CylindricalWarper: public WarperCreator
  {
  public:
      Ptr<detail::RotationWarper> create(float scale) const { return new detail::CylindricalWarper(scale); }
  };
  ```

cv::SphericalWarper: |
  Spherical warper factory class. :
  
  ```c++
  class SphericalWarper: public WarperCreator
  {
  public:
      Ptr<detail::RotationWarper> create(float scale) const { return new detail::SphericalWarper(scale); }
  };
  ```

cv::highgui. High-level GUI and Media I/O: |
  While OpenCV was designed for use in full-scale applications and can be
  used within functionally rich UI frameworks (such as Qt*, WinForms*,
  or Cocoa*) or without any UI at all, sometimes there it is required to
  try functionality quickly and visualize the results. This is what the
  HighGUI module has been designed for.
  
  It provides easy interface to:
    Create and manipulate windows that can display images and "remember"
  their content (no need to handle repaint events from OS).
  
    Add trackbars to the windows, handle simple mouse events as well as
  keyboard commands.
  
    Read and write images to/from disk or memory.
  
    Read video from camera or file and write video to a file.

cv::gpu::evenLevels: Computes levels with even distribution.
cv::gpu::histEven: Calculates a histogram with evenly distributed bins.
cv::gpu::histRange: Calculates a histogram with bins determined by the `levels` array.
cv::gpu::calcHist: Calculates histogram for one channel 8-bit image.
cv::gpu::equalizeHist: Equalizes the histogram of a grayscale image.
cv::CalcPGH: |-
  Calculates a pair-wise geometrical histogram for a contour.
  
  The function calculates a 2D pair-wise geometrical histogram (PGH),
  described in [Iivarinen97]_ for the contour. The algorithm considers
  every pair of contour edges. The angle between the edges and the
  minimum/maximum distances are determined for every pair. To do this,
  each of the edges in turn is taken as the base, while the function loops
  through all the other edges. When the base edge and any other edge are
  considered, the minimum and maximum distances from the points on the
  non-base edge and line of the base edge are selected. The angle between
  the edges defines the row of the histogram in which all the bins that
  correspond to the distance between the calculated minimum and maximum
  distances are incremented (that is, the histogram is transposed
  relatively to the definition in the original paper). The histogram can
  be used for contour matching.
cv::QueryHistValue*D: |-
  Queries the value of the histogram bin.
  
  The macros return the value of the specified bin of the 1D, 2D, 3D, or
  N-D histogram. In case of a sparse histogram, the function returns 0. If
  the bin is not present in the histogram, no new bin is created.
cv::GetHistValue_?D: |-
  Returns a pointer to the histogram bin.
  
  ```c++
  #define cvGetHistValue_1D( hist, idx0 )
      ((float*)(cvPtr1D( (hist)->bins, (idx0), 0 ))
  #define cvGetHistValue_2D( hist, idx0, idx1 )
      ((float*)(cvPtr2D( (hist)->bins, (idx0), (idx1), 0 )))
  #define cvGetHistValue_3D( hist, idx0, idx1, idx2 )
      ((float*)(cvPtr3D( (hist)->bins, (idx0), (idx1), (idx2), 0 )))
  #define cvGetHistValue_nD( hist, idx )
      ((float*)(cvPtrND( (hist)->bins, (idx), 0 )))
  ```
  
  
  The macros `GetHistValue` return a pointer to the specified bin of the
  1D, 2D, 3D, or N-D histogram. In case of a sparse histogram, the
  function creates a new bin and sets it to 0, unless it exists already.
cv::gpu::HoughLines: Finds lines in a binary image using the classical Hough transform.
cv::gpu::HoughLinesDownload: Downloads results from :ocvgpu::HoughLines to host memory.
cv::gpu::HoughCircles: Finds circles in a grayscale image using the Hough transform.
cv::gpu::HoughCirclesDownload: Downloads results from :ocvgpu::HoughCircles to host memory.
cv::ocl::Sobel: |-
  Returns void
  
  The function computes the first x- or y- spatial image derivative using
  Sobel operator. Surpport 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4 data type.
cv::ocl::Scharr: |-
  Returns void
  
  The function computes the first x- or y- spatial image derivative using
  Scharr operator. Surpport 8UC1 8UC4 32SC1 32SC4 32FC1 32FC4 data type.
cv::ocl::GaussianBlur: |-
  Returns void
  
  The function convolves the source image with the specified Gaussian
  kernel. In-place filtering is supported. Surpport 8UC1 8UC4 32SC1 32SC4
  32FC1 32FC4 data type.
cv::ocl::boxFilter: |-
  Returns void
  
  Smoothes image using box filter.Supports data type: CV_8UC1, CV_8UC4,
  CV_32FC1 and CV_32FC4.
cv::ocl::Laplacian: |-
  Returns void
  
  The function calculates the Laplacian of the source image by adding up
  the second x and y derivatives calculated using the Sobel operator.
cv::ocl::ConvolveBuf: |-
  Class providing a memory buffer for :ocvocl::convolve function, plus it
  allows to adjust some specific parameters. :
  
  ```c++
  struct CV_EXPORTS ConvolveBuf
  {
      Size result_size;
      Size block_size;
      Size user_block_size;
      Size dft_size;
      int spect_len;
  
      oclMat image_spect, templ_spect, result_spect;
      oclMat image_block, templ_block, result_data;
  
      void create(Size image_size, Size templ_size);
      static Size estimateBlockSize(Size result_size, Size templ_size);
  };
  ```
  
  
  You can use field user_block_size to set specific block size for
  :ocvocl::convolve function. If you leave its default value Size(0,0)
  then automatic estimation of block size will be used (which is optimized
  for speed). By varying user_block_size you can reduce memory
  requirements at the cost of speed.
cv::ocl::ConvolveBuf::create: |-
  Constructs a buffer for :ocvocl::convolve function with respective
  arguments.
cv::ocl::convolve: |-
  Returns void
  
  Convolves an image with the kernel. Supports only CV_32FC1 data types
  and do not support ROI.
cv::ocl::bilateralFilter: |-
  Returns void
  
  Applies bilateral filter to the image. Supports 8UC1 8UC4 data types.
cv::ocl::copyMakeBorder: |-
  Returns void
  
  Forms a border around the image. Supports 8UC1 8UC4 32SC1 32SC4 32FC1
  32FC4 data types.
cv::ocl::dilate: |-
  Returns void
  
  The function dilates the source image using the specified structuring
  element that determines the shape of a pixel neighborhood over which the
  maximum is taken. Supports 8UC1 8UC4 data types.
cv::ocl::erode: |-
  Returns void
  
  The function erodes the source image using the specified structuring
  element that determines the shape of a pixel neighborhood over which the
  minimum is taken. Supports 8UC1 8UC4 data types.
cv::ocl::morphologyEx: |-
  Returns void
  
  A wrapper for erode and dilate. Supports 8UC1 8UC4 data types.
cv::ocl::pyrDown: Smoothes an image and downsamples it.
cv::ocl::pyrUp: Upsamples an image and then smoothes it.
cv::ocl::columnSum: Computes a vertical (column) sum.
cv::ocl::blendLinear: Performs linear blending of two images.
cv::ocl::cornerHarris: |-
  Returns void
  
  Calculate Harris corner.
cv::ocl::cornerMinEigenVal: |-
  Returns void
  
  Calculate MinEigenVal.
cv::ocl::calcHist: |-
  Returns void
  
  Calculates histogram of one or more arrays. Supports only 8UC1 data
  type.
cv::ocl::remap: |-
  Returns void
  
  The function remap transforms the source image using the specified map:
  dst (x ,y) = src (map1(x , y) , map2(x , y)) where values of pixels with
  non-integer coordinates are computed using one of available
  interpolation methods. map1 and map2 can be encoded as separate
  floating-point maps in map1 and map2 respectively, or interleaved
  floating-point maps of (x,y) in map1. Supports CV_8UC1, CV_8UC3,
  CV_8UC4, CV_32FC1 , CV_32FC3 and CV_32FC4 data types.
cv::ocl::resize: |-
  Returns void
  
  Resizes an image. Supports CV_8UC1, CV_8UC3, CV_8UC4, CV_32FC1 ,
  CV_32FC3 and CV_32FC4 data types.
cv::ocl::warpAffine: |-
  Returns void
  
  The function warpAffine transforms the source image using the specified
  matrix. Supports INTER_NEAREST, INTER_LINEAR, INTER_CUBIC types.
cv::ocl::warpPerspective: |-
  Returns void
  
  Applies a perspective transformation to an image. Supports
  INTER_NEAREST, INTER_LINEAR, INTER_CUBIC types.
cv::ocl::cvtColor: |-
  Returns void
  
  Converts image from one color space to another.For now, only RGB2GRAY is
  supportted.
  Supports.CV_8UC1,CV_8UC4,CV_32SC1,CV_32SC4,CV_32FC1,CV_32FC4
cv::ocl::threshold: |-
  Returns Threshold value
  
  The function applies fixed-level thresholding to a single-channel array.
  The function is typically used to get a bi-level (binary) image out of a
  grayscale image or for removing a noise, i.e. filtering out pixels with
  too small or too large values. There are several types of thresholding
  that the function supports that are determined by thresholdType.
  Supports only CV_32FC1 and CV_8UC1 data type.
cv::ocl::buildWarpPlaneMaps: Builds plane warping maps.
cv::ocl::buildWarpCylindricalMaps: Builds cylindrical warping maps.
cv::ocl::buildWarpSphericalMaps: Builds spherical warping maps.
cv::ocl::buildWarpPerspectiveMaps: Builds transformation maps for perspective transformation.
cv::ocl::buildWarpAffineMaps: Builds transformation maps for affine transformation.
cv::ocl::PyrLKOpticalFlow: |-
  Class used for calculating an optical flow. :
  
  ```c++
  class PyrLKOpticalFlow
  {
  public:
      PyrLKOpticalFlow();
  
      void sparse(const oclMat& prevImg, const oclMat& nextImg, const oclMat& prevPts, oclMat& nextPts,
          oclMat& status, oclMat* err = 0);
  
      void dense(const oclMat& prevImg, const oclMat& nextImg, oclMat& u, oclMat& v, oclMat* err = 0);
  
      Size winSize;
      int maxLevel;
      int iters;
      double derivLambda;
      bool useInitialFlow;
      float minEigThreshold;
      bool getMinEigenVals;
  
      void releaseMemory();
  };
  ```
  
  
  The class can calculate an optical flow for a sparse feature set or
  dense optical flow using the iterative Lucas-Kanade method with
  pyramids.
cv::ocl::PyrLKOpticalFlow::sparse: Calculate an optical flow for a sparse feature set.
cv::ocl::PyrLKOpticalFlow::dense: Calculate dense optical flow.
cv::ocl::PyrLKOpticalFlow::releaseMemory: Releases inner buffers memory.
cv::ocl::interpolateFrames: |-
  Interpolate frames (images) using provided optical flow (displacement
  field).
cv::ocl::HoughCircles: |
  Finds circles in a grayscale image using the Hough transform.
  
  **note**
  
  Currently only non-ROI oclMat is supported for src.

cv::Preliminary illustration: |
  As a preliminary presentation, let's start with a visual example. We
  propose to apply the filter on a low quality color jpeg image with
  backlight problems. Here is the considered input... *"Well, my eyes were
  able to see more that this strange black shadow..."*
  
  ![a low quality color jpeg image with backlight problems.](images/retinaInput.jpg)
  
  Below, the retina foveal model applied on the entire image with default
  parameters. Here contours are enforced, halo effects are voluntary
  visible with this configuration. See parameters discussion below and
  increase horizontalCellsGain near 1 to remove them.
  
  ![the retina foveal model applied on the entire image with default parameters. Here contours are enforced, luminance is corrected and halo effects are voluntary visible with this configuration, increase horizontalCellsGain near 1 to remove them.](images/retinaOutput_default.jpg)
  
  Below, a second retina foveal model output applied on the entire image
  with a parameters setup focused on naturalness perception. *"Hey, i now
  recognize my cat, looking at the mountains at the end of the day !"*.
  Here contours are enforced, luminance is corrected but halos are avoided
  with this configuration. The backlight effect is corrected and highlight
  details are still preserved. Then, even on a low quality jpeg image, if
  some luminance information remains, the retina is able to reconstruct a
  proper visual signal. Such configuration is also usefull for High
  Dynamic Range (*HDR*) images compression to 8bit images as discussed in
  [benoit2010]_ and in the demonstration codes discussed below. As shown
  at the end of the page, parameters change from defaults are :
  
    horizontalCellsGain=0.3
  
  
    photoreceptorsLocalAdaptationSensitivity=ganglioncellsSensitivity=0.89.
  
  
  ![the retina foveal model applied on the entire image with 'naturalness' parameters. Here contours are enforced but are avoided with this configuration, horizontalCellsGain is 0.3 and photoreceptorsLocalAdaptationSensitivity=ganglioncellsSensitivity=0.89.](images/retinaOutput_realistic.jpg)
  
  As observed in this preliminary demo, the retina can be settled up with
  various parameters, by default, as shown on the figure above, the retina
  strongly reduces mean luminance energy and enforces all details of the
  visual scene. Luminance energy and halo effects can be modulated
  (exagerated to cancelled as shown on the two examples). In order to use
  your own parameters, you can use at least one time the *write(String
  fs)* method which will write a proper XML file with all default
  parameters. Then, tweak it on your own and reload them at any time using
  method *setup(String fs)*. These methods update a
  *Retina::RetinaParameters* member structure that is described hereafter.
  XML parameters file samples are shown at the end of the page.
  
  Here is an overview of the abstract Retina interface, allocate one
  instance with the *createRetina* functions.:
  
  ```c++
  class Retina : public Algorithm
  {
  public:
    // parameters setup instance
    struct RetinaParameters; // this class is detailled later
  
    // main method for input frame processing
    void run (InputArray inputImage);
  
    // output buffers retreival methods
    // -> foveal color vision details channel with luminance and noise correction
    void getParvo (OutputArray retinaOutput_parvo);
    void getParvoRAW (OutputArray retinaOutput_parvo);// retreive original output buffers without any normalisation
    const Mat getParvoRAW () const;// retreive original output buffers without any normalisation
    // -> peripheral monochrome motion and events (transient information) channel
    void getMagno (OutputArray retinaOutput_magno);
    void getMagnoRAW (OutputArray retinaOutput_magno); // retreive original output buffers without any normalisation
    const Mat getMagnoRAW () const;// retreive original output buffers without any normalisation
  
    // reset retina buffers... equivalent to closing your eyes for some seconds
    void clearBuffers ();
  
    // retreive input and output buffers sizes
    Size getInputSize ();
    Size getOutputSize ();
  
    // setup methods with specific parameters specification of global xml config file loading/write
    void setup (String retinaParameterFile="", const bool applyDefaultSetupOnFailure=true);
    void setup (FileStorage &fs, const bool applyDefaultSetupOnFailure=true);
    void setup (RetinaParameters newParameters);
    struct Retina::RetinaParameters getParameters ();
    const String printSetup ();
    virtual void write (String fs) const;
    virtual void write (FileStorage &fs) const;
    void setupOPLandIPLParvoChannel (const bool colorMode=true, const bool normaliseOutput=true, const float photoreceptorsLocalAdaptationSensitivity=0.7, const float photoreceptorsTemporalConstant=0.5, const float photoreceptorsSpatialConstant=0.53, const float horizontalCellsGain=0, const float HcellsTemporalConstant=1, const float HcellsSpatialConstant=7, const float ganglionCellsSensitivity=0.7);
    void setupIPLMagnoChannel (const bool normaliseOutput=true, const float parasolCells_beta=0, const float parasolCells_tau=0, const float parasolCells_k=7, const float amacrinCellsTemporalCutFrequency=1.2, const float V0CompressionParameter=0.95, const float localAdaptintegration_tau=0, const float localAdaptintegration_k=7);
    void setColorSaturation (const bool saturateColors=true, const float colorSaturationValue=4.0);
    void activateMovingContoursProcessing (const bool activate);
    void activateContoursProcessing (const bool activate);
  };
  
    // Allocators
    cv::Ptr<Retina> createRetina (Size inputSize);
    cv::Ptr<Retina> createRetina (Size inputSize, const bool colorMode, RETINA_COLORSAMPLINGMETHOD colorSamplingMethod=RETINA_COLOR_BAYER, const bool useRetinaLogSampling=false, const double reductionFactor=1.0, const double samplingStrenght=10.0);
  ```

cv::Literature: |
  For more information, refer to the following papers :
    Please have a look at the reference work of Jeanny Herault that you
  can read in his book :
  
  
  This retina filter code includes the research contributions of
  phd/research collegues from which code has been redrawn by the author :
  
    take a look at the *retinacolor.hpp* module to discover Brice Chaix
  de Lavarene phD color mosaicing/demosaicing and his reference paper:
  
  
    take a look at *imagelogpolprojection.hpp* to discover retina
  spatial log sampling which originates from Barthelemy Durette phd
  with Jeanny Herault. A Retina / V1 cortex projection is also
  proposed and originates from Jeanny's discussions. More informations
  in the above cited Jeanny Heraults's book.

cv::Demos and experiments !: |+
  **NOTE : Complementary to the following examples, have a look at the
  Retina tutorial in the tutorial/contrib section for complementary
  explanations.**
  
  Take a look at the provided C++ examples provided with OpenCV :
  
    **samples/cpp/retinademo.cpp** shows how to use the retina module for details enhancement (Parvo channel output) and transient maps observation (Magno channel output). You can play with images, video sequences and webcam video.
  :   Typical uses are (provided your OpenCV installation is situated
      in folder *OpenCVReleaseFolder*)
  
  ```c++
  -   image processing : **OpenCVReleaseFolder/bin/retinademo
      -image myPicture.jpg**
  
  -   video processing : **OpenCVReleaseFolder/bin/retinademo
      -video myMovie.avi**
  
  -   webcam processing: **OpenCVReleaseFolder/bin/retinademo
      -video**
  ```
  
  
  **Note :** This demo generates the file
  *RetinaDefaultParameters.xml* which contains the default
  parameters of the retina. Then, rename this as
  *RetinaSpecificParameters.xml*, adjust the parameters the way you
  want and reload the program to check the effect.
  
  
  
    **samples/cpp/OpenEXRimages_HighDynamicRange_Retina_toneMapping.cpp**
  shows how to use the retina to perform High Dynamic Range (HDR)
  luminance compression
  
  Then, take a HDR image using bracketing with your camera and
  generate an OpenEXR image and then process it using the demo.
  
  Typical use, supposing that you have the OpenEXR image
  *memorial.exr* (present in the samples/cpp/ folder)
  
  **OpenCVReleaseFolder/bin/OpenEXRimages_HighDynamicRange_Retina_toneMapping
  memorial.exr**
  
  Note that some sliders are made available to allow you to play
  with luminance compression.
  
  
cv::Methods description: Here are detailled the main methods to control the retina model
cv::Retina parameters files examples: |-
  Here is the default configuration file of the retina module. It gives
  results such as the first retina output shown on the top of this page.
  
  ~~~~ {.sourceCode .cpp}
  <?xml version="1.0"?>
  <opencv_storage>
  <OPLandIPLparvo>
      <colorMode>1</colorMode>
      <normaliseOutput>1</normaliseOutput>
      <photoreceptorsLocalAdaptationSensitivity>7.5e-01</photoreceptorsLocalAdaptationSensitivity>
      <photoreceptorsTemporalConstant>9.0e-01</photoreceptorsTemporalConstant>
      <photoreceptorsSpatialConstant>5.3e-01</photoreceptorsSpatialConstant>
      <horizontalCellsGain>0.01</horizontalCellsGain>
      <hcellsTemporalConstant>0.5</hcellsTemporalConstant>
      <hcellsSpatialConstant>7.</hcellsSpatialConstant>
      <ganglionCellsSensitivity>7.5e-01</ganglionCellsSensitivity></OPLandIPLparvo>
  <IPLmagno>
      <normaliseOutput>1</normaliseOutput>
      <parasolCells_beta>0.</parasolCells_beta>
      <parasolCells_tau>0.</parasolCells_tau>
      <parasolCells_k>7.</parasolCells_k>
      <amacrinCellsTemporalCutFrequency>2.0e+00</amacrinCellsTemporalCutFrequency>
      <V0CompressionParameter>9.5e-01</V0CompressionParameter>
      <localAdaptintegration_tau>0.</localAdaptintegration_tau>
      <localAdaptintegration_k>7.</localAdaptintegration_k></IPLmagno>
  </opencv_storage>
  ~~~~
  
  Here is the 'realistic" setup used to obtain the second retina output
  shown on the top of this page.
  
  ~~~~ {.sourceCode .cpp}
  <?xml version="1.0"?>
  <opencv_storage>
  <OPLandIPLparvo>
    <colorMode>1</colorMode>
    <normaliseOutput>1</normaliseOutput>
    <photoreceptorsLocalAdaptationSensitivity>8.9e-01</photoreceptorsLocalAdaptationSensitivity>
    <photoreceptorsTemporalConstant>9.0e-01</photoreceptorsTemporalConstant>
    <photoreceptorsSpatialConstant>5.3e-01</photoreceptorsSpatialConstant>
    <horizontalCellsGain>0.3</horizontalCellsGain>
    <hcellsTemporalConstant>0.5</hcellsTemporalConstant>
    <hcellsSpatialConstant>7.</hcellsSpatialConstant>
    <ganglionCellsSensitivity>8.9e-01</ganglionCellsSensitivity></OPLandIPLparvo>
  <IPLmagno>
    <normaliseOutput>1</normaliseOutput>
    <parasolCells_beta>0.</parasolCells_beta>
    <parasolCells_tau>0.</parasolCells_tau>
    <parasolCells_k>7.</parasolCells_k>
    <amacrinCellsTemporalCutFrequency>2.0e+00</amacrinCellsTemporalCutFrequency>
    <V0CompressionParameter>9.5e-01</V0CompressionParameter>
    <localAdaptintegration_tau>0.</localAdaptintegration_tau>
    <localAdaptintegration_k>7.</localAdaptintegration_k></IPLmagno>
  </opencv_storage>
  ~~~~
cv::gpu::getCudaEnabledDeviceCount: |-
  Returns the number of installed CUDA-enabled devices.
  
  Use this function before any other GPU functions calls. If OpenCV is
  compiled without GPU support, this function returns 0.
cv::gpu::setDevice: |-
  Sets a device and initializes it for the current thread.
  
  If the call of this function is omitted, a default device is initialized
  at the fist GPU usage.
cv::gpu::getDevice: |-
  Returns the current device index set by :ocvgpu::setDevice or
  initialized by default.
cv::gpu::resetDevice: |-
  Explicitly destroys and cleans up all resources associated with the
  current device in the current process.
  
  Any subsequent API call to this device will reinitialize the device.
cv::gpu::FeatureSet: Enumeration providing GPU computing features.
cv::gpu::TargetArchs: |-
  Class providing a set of static methods to check what NVIDIA* card
  architecture the GPU module was built for.
  
  The following method checks whether the module was built with the
  support of the given feature:
  
  There is a set of methods to check whether the module contains
  intermediate (PTX) or binary GPU code for the given architecture(s):
  
  According to the CUDA C Programming Guide Version 3.2: "PTX code
  produced for some specific compute capability can always be compiled to
  binary code of greater or equal compute capability".
cv::gpu::DeviceInfo: |
  Class providing functionality for querying the specified GPU properties.
  :
  
  ```c++
  class CV_EXPORTS DeviceInfo
  {
  public:
      //! creates DeviceInfo object for the current GPU
      DeviceInfo();
  
      //! creates DeviceInfo object for the given GPU
      DeviceInfo(int device_id);
  
      //! ASCII string identifying device
      const char* name() const;
  
      //! global memory available on device in bytes
      size_t totalGlobalMem() const;
  
      //! shared memory available per block in bytes
      size_t sharedMemPerBlock() const;
  
      //! 32-bit registers available per block
      int regsPerBlock() const;
  
      //! warp size in threads
      int warpSize() const;
  
      //! maximum pitch in bytes allowed by memory copies
      size_t memPitch() const;
  
      //! maximum number of threads per block
      int maxThreadsPerBlock() const;
  
      //! maximum size of each dimension of a block
      Vec3i maxThreadsDim() const;
  
      //! maximum size of each dimension of a grid
      Vec3i maxGridSize() const;
  
      //! clock frequency in kilohertz
      int clockRate() const;
  
      //! constant memory available on device in bytes
      size_t totalConstMem() const;
  
      //! major compute capability
      int majorVersion() const;
  
      //! minor compute capability
      int minorVersion() const;
  
      //! alignment requirement for textures
      size_t textureAlignment() const;
  
      //! pitch alignment requirement for texture references bound to pitched memory
      size_t texturePitchAlignment() const;
  
      //! number of multiprocessors on device
      int multiProcessorCount() const;
  
      //! specified whether there is a run time limit on kernels
      bool kernelExecTimeoutEnabled() const;
  
      //! device is integrated as opposed to discrete
      bool integrated() const;
  
      //! device can map host memory with cudaHostAlloc/cudaHostGetDevicePointer
      bool canMapHostMemory() const;
  
      enum ComputeMode
      {
          ComputeModeDefault,         /**< default compute mode (Multiple threads can use ::cudaSetDevice() with this device) */
          ComputeModeExclusive,       /**< compute-exclusive-thread mode (Only one thread in one process will be able to use ::cudaSetDevice() with this device) */
          ComputeModeProhibited,      /**< compute-prohibited mode (No threads can use ::cudaSetDevice() with this device) */
          ComputeModeExclusiveProcess /**< compute-exclusive-process mode (Many threads in one process will be able to use ::cudaSetDevice() with this device) */
      };
  
      //! compute mode
      ComputeMode computeMode() const;
  
      //! maximum 1D texture size
      int maxTexture1D() const;
  
      //! maximum 1D mipmapped texture size
      int maxTexture1DMipmap() const;
  
      //! maximum size for 1D textures bound to linear memory
      int maxTexture1DLinear() const;
  
      //! maximum 2D texture dimensions
      Vec2i maxTexture2D() const;
  
      //! maximum 2D mipmapped texture dimensions
      Vec2i maxTexture2DMipmap() const;
  
      //! maximum dimensions (width, height, pitch) for 2D textures bound to pitched memory
      Vec3i maxTexture2DLinear() const;
  
      //! maximum 2D texture dimensions if texture gather operations have to be performed
      Vec2i maxTexture2DGather() const;
  
      //! maximum 3D texture dimensions
      Vec3i maxTexture3D() const;
  
      //! maximum Cubemap texture dimensions
      int maxTextureCubemap() const;
  
      //! maximum 1D layered texture dimensions
      Vec2i maxTexture1DLayered() const;
  
      //! maximum 2D layered texture dimensions
      Vec3i maxTexture2DLayered() const;
  
      //! maximum Cubemap layered texture dimensions
      Vec2i maxTextureCubemapLayered() const;
  
      //! maximum 1D surface size
      int maxSurface1D() const;
  
      //! maximum 2D surface dimensions
      Vec2i maxSurface2D() const;
  
      //! maximum 3D surface dimensions
      Vec3i maxSurface3D() const;
  
      //! maximum 1D layered surface dimensions
      Vec2i maxSurface1DLayered() const;
  
      //! maximum 2D layered surface dimensions
      Vec3i maxSurface2DLayered() const;
  
      //! maximum Cubemap surface dimensions
      int maxSurfaceCubemap() const;
  
      //! maximum Cubemap layered surface dimensions
      Vec2i maxSurfaceCubemapLayered() const;
  
      //! alignment requirements for surfaces
      size_t surfaceAlignment() const;
  
      //! device can possibly execute multiple kernels concurrently
      bool concurrentKernels() const;
  
      //! device has ECC support enabled
      bool ECCEnabled() const;
  
      //! PCI bus ID of the device
      int pciBusID() const;
  
      //! PCI device ID of the device
      int pciDeviceID() const;
  
      //! PCI domain ID of the device
      int pciDomainID() const;
  
      //! true if device is a Tesla device using TCC driver, false otherwise
      bool tccDriver() const;
  
      //! number of asynchronous engines
      int asyncEngineCount() const;
  
      //! device shares a unified address space with the host
      bool unifiedAddressing() const;
  
      //! peak memory clock frequency in kilohertz
      int memoryClockRate() const;
  
      //! global memory bus width in bits
      int memoryBusWidth() const;
  
      //! size of L2 cache in bytes
      int l2CacheSize() const;
  
      //! maximum resident threads per multiprocessor
      int maxThreadsPerMultiProcessor() const;
  
      //! gets free and total device memory
      void queryMemory(size_t& totalMemory, size_t& freeMemory) const;
      size_t freeMemory() const;
      size_t totalMemory() const;
  
      //! checks whether device supports the given feature
      bool supports(FeatureSet feature_set) const;
  
      //! checks whether the GPU module can be run on the given device
      bool isCompatible() const;
  };
  ```

cv::gpu::DeviceInfo::DeviceInfo: |-
  The constructors.
  
  Constructs the `DeviceInfo` object for the specified device. If
  `device_id` parameter is missed, it constructs an object for the current
  device.
cv::gpu::DeviceInfo::name: Returns the device name.
cv::gpu::DeviceInfo::majorVersion: Returns the major compute capability version.
cv::gpu::DeviceInfo::minorVersion: Returns the minor compute capability version.
cv::gpu::DeviceInfo::freeMemory: Returns the amount of free memory in bytes.
cv::gpu::DeviceInfo::totalMemory: Returns the amount of total memory in bytes.
cv::gpu::DeviceInfo::supports: |-
  Provides information on GPU feature support.
  
  This function returns `true` if the device has the specified GPU
  feature. Otherwise, it returns `false` .
cv::gpu::DeviceInfo::isCompatible: |-
  Checks the GPU module and device compatibility.
  
  This function returns `true` if the GPU module can be run on the
  specified device. Otherwise, it returns `false` .
cv::gpu::DeviceInfo::deviceID: Returns system index of the GPU device starting with 0.
cv::inpaint: |-
  Restores the selected region in an image using the region neighborhood.
  
  The function reconstructs the selected image area from the pixel near
  the area boundary. The function may be used to remove dust and scratches
  from a scanned photo, or to remove undesirable objects from still images
  or video. See <http://en.wikipedia.org/wiki/Inpainting> for more
  details.
cv::`cv` Namespace: |-
  All the OpenCV classes and functions are placed into the `cv` namespace.
  Therefore, to access this functionality from your code, use the `cv::`
  specifier or `using namespace cv;` directive:
  
  ~~~~ {.sourceCode .c}
cv::include "opencv2/core.hpp": |
  ...
  cv::Mat H = cv::findHomography(points1, points2, CV_RANSAC, 5);
  ...
  ~~~~
  
  or :
  
  ```c++
  #include "opencv2/core.hpp"
  using namespace cv;
  ...
  Mat H = findHomography(points1, points2, CV_RANSAC, 5 );
  ...
  ```
  
  
  Some of the current or future OpenCV external names may conflict with
  STL or other libraries. In this case, use explicit namespace specifiers
  to resolve the name conflicts: :
  
  ```c++
  Mat a(100, 100, CV_32F);
  randu(a, Scalar::all(1), Scalar::all(std::rand()));
  cv::log(a, a);
  a /= std::log(2.);
  ```

cv::Automatic Memory Management: |-
  OpenCV handles all the memory automatically.
  
  First of all, `std::vector`, `Mat`, and other data structures used by
  the functions and methods have destructors that deallocate the
  underlying memory buffers when needed. This means that the destructors
  do not always deallocate the buffers as in case of `Mat`. They take into
  account possible data sharing. A destructor decrements the reference
  counter associated with the matrix data buffer. The buffer is
  deallocated if and only if the reference counter reaches zero, that is,
  when no other structures refer to the same buffer. Similarly, when a
  `Mat` instance is copied, no actual data is really copied. Instead, the
  reference counter is incremented to memorize that there is another owner
  of the same data. There is also the `Mat::clone` method that creates a
  full copy of the matrix data. See the example below: :
  
  ```c++
  // create a big 8Mb matrix
  Mat A(1000, 1000, CV_64F);
  
  // create another header for the same matrix;
  // this is an instant operation, regardless of the matrix size.
  Mat B = A;
  // create another header for the 3-rd row of A; no data is copied either
  Mat C = B.row(3);
  // now create a separate copy of the matrix
  Mat D = B.clone();
  // copy the 5-th row of B to C, that is, copy the 5-th row of A
  // to the 3-rd row of A.
  B.row(5).copyTo(C);
  // now let A and D share the data; after that the modified version
  // of A is still referenced by B and C.
  A = D;
  // now make B an empty matrix (which references no memory buffers),
  // but the modified version of A will still be referenced by C,
  // despite that C is just a single row of the original A
  B.release();
  
  // finally, make a full copy of C. As a result, the big modified
  // matrix will be deallocated, since it is not referenced by anyone
  C = C.clone();
  ```
  
  
  You see that the use of `Mat` and other basic structures is simple. But
  what about high-level classes or even user data types created without
  taking automatic memory management into account? For them, OpenCV offers
  the `Ptr<>` template class that is similar to `std::shared_ptr` from C++
  TR1. So, instead of using plain pointers:
  
  ```c++
  T* ptr = new T(...);
  ```
  
  
  you can use:
  
  ```c++
  Ptr<T> ptr = new T(...);
  ```
  
  
  That is, `Ptr<T> ptr` encapsulates a pointer to a `T` instance and a
  reference counter associated with the pointer. See the :ocvPtr
  description for details.
cv::Automatic Allocation of the Output Data: |-
  OpenCV deallocates the memory automatically, as well as automatically
  allocates the memory for output function parameters most of the time.
  So, if a function has one or more input arrays (`cv::Mat` instances) and
  some output arrays, the output arrays are automatically allocated or
  reallocated. The size and type of the output arrays are determined from
  the size and type of input arrays. If needed, the functions take extra
  parameters that help to figure out the output array properties.
  
  Example: :
  
  ```c++
  #include "opencv2/imgproc.hpp"
  #include "opencv2/highgui.hpp"
  
  using namespace cv;
  
  int main(int, char**)
  {
      VideoCapture cap(0);
      if(!cap.isOpened()) return -1;
  
      Mat frame, edges;
      namedWindow("edges",1);
      for(;;)
      {
          cap >> frame;
          cvtColor(frame, edges, COLOR_BGR2GRAY);
          GaussianBlur(edges, edges, Size(7,7), 1.5, 1.5);
          Canny(edges, edges, 0, 30, 3);
          imshow("edges", edges);
          if(waitKey(30) >= 0) break;
      }
      return 0;
  }
  ```
  
  
  The array `frame` is automatically allocated by the `>>` operator since
  the video frame resolution and the bit-depth is known to the video
  capturing module. The array `edges` is automatically allocated by the
  `cvtColor` function. It has the same size and the bit-depth as the input
  array. The number of channels is 1 because the color conversion code
  `COLOR_BGR2GRAY` is passed, which means a color to grayscale conversion.
  Note that `frame` and `edges` are allocated only once during the first
  execution of the loop body since all the next video frames have the same
  resolution. If you somehow change the video resolution, the arrays are
  automatically reallocated.
  
  The key component of this technology is the `Mat::create` method. It
  takes the desired array size and type. If the array already has the
  specified size and type, the method does nothing. Otherwise, it releases
  the previously allocated data, if any (this part involves decrementing
  the reference counter and comparing it with zero), and then allocates a
  new buffer of the required size. Most functions call the `Mat::create`
  method for each output array, and so the automatic output data
  allocation is implemented.
  
  Some notable exceptions from this scheme are `cv::mixChannels`,
  `cv::RNG::fill`, and a few other functions and methods. They are not
  able to allocate the output array, so you have to do this in advance.
cv::Saturation Arithmetics: |
  As a computer vision library, OpenCV deals a lot with image pixels that
  are often encoded in a compact, 8- or 16-bit per channel, form and thus
  have a limited value range. Furthermore, certain operations on images,
  like color space conversions, brightness/contrast adjustments,
  sharpening, complex interpolation (bi-cubic, Lanczos) can produce values
  out of the available range. If you just store the lowest 8 (16) bits of
  the result, this results in visual artifacts and may affect a further
  image analysis. To solve this problem, the so-called *saturation*
  arithmetics is used. For example, to store `r`, the result of an
  operation, to an 8-bit image, you find the nearest value within the
  0..255 range:
  
  $$I(x,y)= \min ( \max (\textrm{round}(r), 0), 255)$$
  
  Similar rules are applied to 8-bit signed, 16-bit signed and unsigned
  types. This semantics is used everywhere in the library. In C++ code, it
  is done using the `saturate_cast<>` functions that resemble standard C++
  cast operations. See below the implementation of the formula provided
  above:
  
  ```c++
  I.at<uchar>(y, x) = saturate_cast<uchar>(r);
  ```
  
  
  where `cv::uchar` is an OpenCV 8-bit unsigned integer type. In the
  optimized SIMD code, such SSE2 instructions as `paddusb`, `packuswb`,
  and so on are used. They help achieve exactly the same behavior as in
  C++ code.
  
  **note**
  
  Saturation is not applied when the result is 32-bit integer.

cv::Fixed Pixel Types. Limited Use of Templates: |-
  Templates is a great feature of C++ that enables implementation of very
  powerful, efficient and yet safe data structures and algorithms.
  However, the extensive use of templates may dramatically increase
  compilation time and code size. Besides, it is difficult to separate an
  interface and implementation when templates are used exclusively. This
  could be fine for basic algorithms but not good for computer vision
  libraries where a single algorithm may span thousands lines of code.
  Because of this and also to simplify development of bindings for other
  languages, like Python, Java, Matlab that do not have templates at all
  or have limited template capabilities, the current OpenCV implementation
  is based on polymorphism and runtime dispatching over templates. In
  those places where runtime dispatching would be too slow (like pixel
  access operators), impossible (generic `Ptr<>` implementation), or just
  very inconvenient (`saturate_cast<>()`) the current implementation
  introduces small template classes, methods, and functions. Anywhere else
  in the current OpenCV version the use of templates is limited.
  
  Consequently, there is a limited fixed set of primitive data types the
  library can operate on. That is, array elements should have one of the
  following types:
    8-bit unsigned integer (uchar)
  
    8-bit signed integer (schar)
  
    16-bit unsigned integer (ushort)
  
    16-bit signed integer (short)
  
    32-bit signed integer (int)
  
    32-bit floating-point number (float)
  
    64-bit floating-point number (double)
  
    a tuple of several elements where all elements have the same type
  (one of the above). An array whose elements are such tuples, are
  called multi-channel arrays, as opposite to the single-channel
  arrays, whose elements are scalar values. The maximum possible
  number of channels is defined by the `CV_CN_MAX` constant, which
  is currently set to 512.
  
  
  
  For these basic types, the following enumeration is applied:
  
  ```c++
  enum { CV_8U=0, CV_8S=1, CV_16U=2, CV_16S=3, CV_32S=4, CV_32F=5, CV_64F=6 };
  ```
  
  
  Multi-channel (`n`-channel) types can be specified using the following
  options:
    `CV_8UC1` ... `CV_64FC4` constants (for a number of channels from 1
  to 4)
  
    `CV_8UC(n)` ... `CV_64FC(n)` or `CV_MAKETYPE(CV_8U, n)` ...
  `CV_MAKETYPE(CV_64F, n)` macros when the number of channels is more
  than 4 or unknown at the compilation time.
  
  
  **note**
  
  `CV_32FC1 == CV_32F`,
  `CV_32FC2 == CV_32FC(2) == CV_MAKETYPE(CV_32F, 2)`, and
  `CV_MAKETYPE(depth, n) == ((x&7)<<3) + (n-1)`. This means that the
  constant type is formed from the `depth`, taking the lowest 3 bits,
  and the number of channels minus 1, taking the next `log2(CV_CN_MAX)`
  bits.
  
  
  Examples: :
  
  ```c++
  Mat mtx(3, 3, CV_32F); // make a 3x3 floating-point matrix
  Mat cmtx(10, 1, CV_64FC2); // make a 10x1 2-channel floating-point
                             // matrix (10-element complex vector)
  Mat img(Size(1920, 1080), CV_8UC3); // make a 3-channel (color) image
                                      // of 1920 columns and 1080 rows.
  Mat grayscale(image.size(), CV_MAKETYPE(image.depth(), 1)); // make a 1-channel image of
                                                              // the same size and same
                                                              // channel type as img
  ```
  
  
  Arrays with more complex elements cannot be constructed or processed
  using OpenCV. Furthermore, each function or method can handle only a
  subset of all possible array types. Usually, the more complex the
  algorithm is, the smaller the supported subset of formats is. See below
  typical examples of such limitations:
    The face detection algorithm only works with 8-bit grayscale or
  color images.
  
    Linear algebra functions and most of the machine learning
  algorithms work with floating-point arrays only.
  
    Basic functions, such as `cv::add`, support all types.
  
    Color space conversion functions support 8-bit unsigned, 16-bit
  unsigned, and 32-bit floating-point types.
  
  
  
  The subset of supported types for each function has been defined from
  practical needs and could be extended in future based on user requests.
cv::InputArray and OutputArray: |-
  Many OpenCV functions process dense 2-dimensional or multi-dimensional
  numerical arrays. Usually, such functions take cppMat as parameters, but
  in some cases it's more convenient to use `std::vector<>` (for a point
  set, for example) or `Matx<>` (for 3x3 homography matrix and such). To
  avoid many duplicates in the API, special "proxy" classes have been
  introduced. The base "proxy" class is `InputArray`. It is used for
  passing read-only arrays on a function input. The derived from
  `InputArray` class `OutputArray` is used to specify an output array for
  a function. Normally, you should not care of those intermediate types
  (and you should not declare variables of those types explicitly) - it
  will all just work automatically. You can assume that instead of
  `InputArray`/`OutputArray` you can always use `Mat`, `std::vector<>`,
  `Matx<>`, `Vec<>` or `Scalar`. When a function has an optional input or
  output array, and you do not have or do not want one, pass
  `cv::noArray()`.
cv::Error Handling: |
  OpenCV uses exceptions to signal critical errors. When the input data
  has a correct format and belongs to the specified value range, but the
  algorithm cannot succeed for some reason (for example, the optimization
  algorithm did not converge), it returns a special error code (typically,
  just a boolean variable).
  
  The exceptions can be instances of the `cv::Exception` class or its
  derivatives. In its turn, `cv::Exception` is a derivative of
  `std::exception`. So it can be gracefully handled in the code using
  other standard C++ library components.
  
  The exception is typically thrown either using the
  `CV_Error(errcode, description)` macro, or its printf-like
  `CV_Error_(errcode, printf-spec, (printf-args))` variant, or using the
  `CV_Assert(condition)` macro that checks the condition and throws an
  exception when it is not satisfied. For performance-critical code, there
  is `CV_DbgAssert(condition)` that is only retained in the Debug
  configuration. Due to the automatic memory management, all the
  intermediate buffers are automatically deallocated in case of a sudden
  error. You only need to add a try statement to catch exceptions, if
  needed: :
  
  ```c++
  try
  {
      ... // call OpenCV
  }
  catch( cv::Exception& e )
  {
      const char* err_msg = e.what();
      std::cout << "exception caught: " << err_msg << std::endl;
  }
  ```

cv::Multi-threading and Re-enterability: |-
  The current OpenCV implementation is fully re-enterable. That is, the
  same function, the same *constant* method of a class instance, or the
  same *non-constant* method of different class instances can be called
  from different threads. Also, the same `cv::Mat` can be used in
  different threads because the reference-counting operations use the
  architecture-specific atomic instructions.
cv::K-Nearest Neighbors: |-
  The algorithm caches all training samples and predicts the response for
  a new sample by analyzing a certain number (**K**) of the nearest
  neighbors of the sample using voting, calculating weighted sum, and so
  on. The method is sometimes referred to as "learning by example" because
  for prediction it looks for the feature vector with a known response
  that is closest to the given vector.
cv::CvKNearest: |-
  The class implements K-Nearest Neighbors model as described in the
  beginning of this section.
cv::CvKNearest::CvKNearest: |-
  Default and training constructors.
  
  See :ocvCvKNearest::train for additional parameters descriptions.
cv::CvKNearest::train: |
  Trains the model.
  
  The method trains the K-Nearest model. It follows the conventions of the
  generic :ocvCvStatModel::train approach with the following limitations:
    Only `CV_ROW_SAMPLE` data layout is supported.
  
    Input variables are all ordered.
  
    Output variables can be either categorical ( `is_regression=false` )
  or ordered ( `is_regression=true` ).
  
    Variable subsets (`var_idx`) and missing measurements are not
  supported.

cv::CvKNearest::find_nearest: |-
  Finds the neighbors and predicts responses for input vectors.
  
  For each input vector (a row of the matrix `samples`), the method finds
  the `k` nearest neighbors. In case of regression, the predicted result
  is a mean value of the particular vector's neighbor responses. In case
  of classification, the class is determined by voting.
  
  For each input vector, the neighbors are sorted by their distances to
  the vector.
  
  In case of C++ interface you can use output pointers to empty matrices
  and the function will allocate memory itself.
  
  If only a single input vector is passed, all output matrices are
  optional and the predicted value is returned by the method.
  
  The function is parallelized with the TBB library.
cv::CvKNearest::get_max_k: "Returns the number of maximum neighbors that may be passed to the method\n\
  :ocvCvKNearest::find_nearest."
cv::CvKNearest::get_var_count: Returns the number of used features (variables count).
cv::CvKNearest::get_sample_count: Returns the total number of train samples.
cv::CvKNearest::is_regression: |
  Returns type of the problem: `true` for regression and `false` for
  classification.
  
  The sample below (currently using the obsolete `CvMat` structures)
  demonstrates the use of the k-nearest classifier for 2D point
  classification: :
  
  ```c++
  #include "ml.h"
  #include "highgui.h"
  
  int main( int argc, char** argv )
  {
      const int K = 10;
      int i, j, k, accuracy;
      float response;
      int train_sample_count = 100;
      CvRNG rng_state = cvRNG(-1);
      CvMat* trainData = cvCreateMat( train_sample_count, 2, CV_32FC1 );
      CvMat* trainClasses = cvCreateMat( train_sample_count, 1, CV_32FC1 );
      IplImage* img = cvCreateImage( cvSize( 500, 500 ), 8, 3 );
      float _sample[2];
      CvMat sample = cvMat( 1, 2, CV_32FC1, _sample );
      cvZero( img );
  
      CvMat trainData1, trainData2, trainClasses1, trainClasses2;
  
      // form the training samples
      cvGetRows( trainData, &trainData1, 0, train_sample_count/2 );
      cvRandArr( &rng_state, &trainData1, CV_RAND_NORMAL, cvScalar(200,200), cvScalar(50,50) );
  
      cvGetRows( trainData, &trainData2, train_sample_count/2, train_sample_count );
      cvRandArr( &rng_state, &trainData2, CV_RAND_NORMAL, cvScalar(300,300), cvScalar(50,50) );
  
      cvGetRows( trainClasses, &trainClasses1, 0, train_sample_count/2 );
      cvSet( &trainClasses1, cvScalar(1) );
  
      cvGetRows( trainClasses, &trainClasses2, train_sample_count/2, train_sample_count );
      cvSet( &trainClasses2, cvScalar(2) );
  
      // learn classifier
      CvKNearest knn( trainData, trainClasses, 0, false, K );
      CvMat* nearests = cvCreateMat( 1, K, CV_32FC1);
  
      for( i = 0; i < img->height; i++ )
      {
          for( j = 0; j < img->width; j++ )
          {
              sample.data.fl[0] = (float)j;
              sample.data.fl[1] = (float)i;
  
              // estimate the response and get the neighbors' labels
              response = knn.find_nearest(&sample,K,0,0,nearests,0);
  
              // compute the number of neighbors representing the majority
              for( k = 0, accuracy = 0; k < K; k++ )
              {
                  if( nearests->data.fl[k] == response)
                      accuracy++;
              }
              // highlight the pixel depending on the accuracy (or confidence)
              cvSet2D( img, i, j, response == 1 ?
                  (accuracy > 5 ? CV_RGB(180,0,0) : CV_RGB(180,120,0)) :
                  (accuracy > 5 ? CV_RGB(0,180,0) : CV_RGB(120,120,0)) );
          }
      }
  
      // display the original training samples
      for( i = 0; i < train_sample_count/2; i++ )
      {
          CvPoint pt;
          pt.x = cvRound(trainData1.data.fl[i*2]);
          pt.y = cvRound(trainData1.data.fl[i*2+1]);
          cvCircle( img, pt, 2, CV_RGB(255,0,0), CV_FILLED );
          pt.x = cvRound(trainData2.data.fl[i*2]);
          pt.y = cvRound(trainData2.data.fl[i*2+1]);
          cvCircle( img, pt, 2, CV_RGB(0,255,0), CV_FILLED );
      }
  
      cvNamedWindow( "classifier result", 1 );
      cvShowImage( "classifier result", img );
      cvWaitKey(0);
  
      cvReleaseMat( &trainClasses );
      cvReleaseMat( &trainData );
      return 0;
  }
  ```

cv::Discriminatively Trained Part Based Models for Object Detection: "The object detector described below has been initially proposed by P.F.\n\
  Felzenszwalb in [Felzenszwalb2010]_. It is based on a Dalal-Triggs\n\
  detector that uses a single filter on histogram of oriented gradients\n\
  (HOG) features to represent an object category. This detector uses a\n\
  sliding window approach, where a filter is applied at all positions and\n\
  scales of an image. The first innovation is enriching the Dalal-Triggs\n\
  model using a star-structured part-based model defined by a \"root\"\n\
  filter (analogous to the Dalal-Triggs filter) plus a set of parts\n\
  filters and associated deformation models. The score of one of star\n\
  models at a particular position and scale within an image is the score\n\
  of the root filter at the given location plus the sum over parts of the\n\
  maximum, over placements of that part, of the part filter score on its\n\
  location minus a deformation cost easuring the deviation of the part\n\
  from its ideal location relative to the root. Both root and part filter\n\
  scores are defined by the dot product between a filter (a set of\n\
  weights) and a subwindow of a feature pyramid computed from the input\n\
  image. Another improvement is a representation of the class of models by\n\
  a mixture of star models. The score of a mixture model at a particular\n\
  position and scale is the maximum over components, of the score of that\n\
  component model at the given location.\n\n\
  In OpenCV there are C implementation of Latent SVM and C++ wrapper of\n\
  it. C version is the structure :ocvCvObjectDetection and a set of\n\
  functions working with this structure (see :ocvcvLoadLatentSvmDetector,\n\
  :ocvcvReleaseLatentSvmDetector, :ocvcvLatentSvmDetectObjects). C++\n\
  version is the class :ocvLatentSvmDetector and has slightly different\n\
  functionality in contrast with C version - it supports loading and\n\
  detection of several models.\n\n\
  There are two examples of Latent SVM usage:\n\
  `samples/c/latentsvmdetect.cpp` and\n\
  `samples/cpp/latentsvm_multidetect.cpp`."
cv::cvLoadLatentSvmDetector: Loads trained detector from a file.
cv::cvReleaseLatentSvmDetector: Release memory allocated for CvLatentSvmDetector structure.
cv::cvLatentSvmDetectObjects: |-
  Find rectangular regions in the given image that are likely to contain
  objects and corresponding confidence levels.
cv::LatentSvmDetector: |-
  This is a C++ wrapping class of Latent SVM. It contains internal
  representation of several trained Latent SVM detectors (models) and a
  set of methods to load the detectors and detect objects using them.
cv::LatentSvmDetector::LatentSvmDetector: Two types of constructors.
cv::LatentSvmDetector::~LatentSvmDetector: Destructor.
cv::LatentSvmDetector::~clear: Clear all trained models and their names stored in an class object.
cv::LatentSvmDetector::load: |-
  Load the trained models from given `.xml` files and return `true` if at
  least one model was loaded.
cv::LatentSvmDetector::detect: |-
  Find rectangular regions in the given image that are likely to contain
  objects of loaded classes (models) and corresponding confidence levels.
cv::LatentSvmDetector::getClassNames: |-
  Return the class (model) names that were passed in constructor or method
  `load` or extracted from models filenames in those methods.
cv::LatentSvmDetector::getClassCount: Return a count of loaded models (classes).
cv::detail::ImageFeatures: |
  Structure containing image keypoints and descriptors. :
  
  ```c++
  struct CV_EXPORTS ImageFeatures
  {
      int img_idx;
      Size img_size;
      std::vector<KeyPoint> keypoints;
      Mat descriptors;
  };
  ```

cv::detail::FeaturesFinder: |
  Feature finders base class. :
  
  ```c++
  class CV_EXPORTS FeaturesFinder
  {
  public:
      virtual ~FeaturesFinder() {}
      void operator ()(const Mat &image, ImageFeatures &features);
      void operator ()(const Mat &image, ImageFeatures &features, const std::vector<cv::Rect> &rois);
      virtual void collectGarbage() {}
  
  protected:
      virtual void find(const Mat &image, ImageFeatures &features) = 0;
  };
  ```

cv::detail::FeaturesFinder::operator(): Finds features in the given image.
cv::detail::FeaturesFinder::collectGarbage: Frees unused memory allocated before if there is any.
cv::detail::FeaturesFinder::find: |-
  This method must implement features finding logic in order to make the
  wrappers detail::FeaturesFinder::operator()_ work.
cv::detail::SurfFeaturesFinder: |
  SURF features finder. :
  
  ```c++
  class CV_EXPORTS SurfFeaturesFinder : public FeaturesFinder
  {
  public:
      SurfFeaturesFinder(double hess_thresh = 300., int num_octaves = 3, int num_layers = 4,
                         int num_octaves_descr = /*4*/3, int num_layers_descr = /*2*/4);
  
  private:
      /* hidden */
  };
  ```

cv::detail::OrbFeaturesFinder: |
  ORB features finder. :
  
  ```c++
  class CV_EXPORTS OrbFeaturesFinder : public FeaturesFinder
  {
  public:
      OrbFeaturesFinder(Size _grid_size = Size(3,1), size_t n_features = 1500,
                        const ORB::CommonParams &detector_params = ORB::CommonParams(1.3f, 5));
  
  private:
      /* hidden */
  };
  ```

cv::detail::MatchesInfo: |
  Structure containing information about matches between two images. It's
  assumed that there is a homography between those images. :
  
  ```c++
  struct CV_EXPORTS MatchesInfo
  {
      MatchesInfo();
      MatchesInfo(const MatchesInfo &other);
      const MatchesInfo& operator =(const MatchesInfo &other);
  
      int src_img_idx, dst_img_idx;       // Images indices (optional)
      std::vector<DMatch> matches;
      std::vector<uchar> inliers_mask;    // Geometrically consistent matches mask
      int num_inliers;                    // Number of geometrically consistent matches
      Mat H;                              // Estimated homography
      double confidence;                  // Confidence two images are from the same panorama
  };
  ```

cv::detail::FeaturesMatcher: |
  Feature matchers base class. :
  
  ```c++
  class CV_EXPORTS FeaturesMatcher
  {
  public:
      virtual ~FeaturesMatcher() {}
  
      void operator ()(const ImageFeatures &features1, const ImageFeatures &features2,
                       MatchesInfo& matches_info) { match(features1, features2, matches_info); }
  
      void operator ()(const std::vector<ImageFeatures> &features, std::vector<MatchesInfo> &pairwise_matches,
                       const Mat &mask = cv::Mat());
  
      bool isThreadSafe() const { return is_thread_safe_; }
  
      virtual void collectGarbage() {}
  
  protected:
      FeaturesMatcher(bool is_thread_safe = false) : is_thread_safe_(is_thread_safe) {}
  
      virtual void match(const ImageFeatures &features1, const ImageFeatures &features2,
                         MatchesInfo& matches_info) = 0;
  
      bool is_thread_safe_;
  };
  ```

cv::detail::FeaturesMatcher::operator(): |-
  Performs images matching.
  
  The function is parallelized with the TBB library.
cv::detail::FeaturesMatcher::collectGarbage: Frees unused memory allocated before if there is any.
cv::detail::FeaturesMatcher::match: |-
  This method must implement matching logic in order to make the wrappers
  detail::FeaturesMatcher::operator()_ work.
cv::detail::BestOf2NearestMatcher: |
  Features matcher which finds two best matches for each feature and
  leaves the best one only if the ratio between descriptor distances is
  greater than the threshold `match_conf`. :
  
  ```c++
  class CV_EXPORTS BestOf2NearestMatcher : public FeaturesMatcher
  {
  public:
      BestOf2NearestMatcher(bool try_use_gpu = false, float match_conf = 0.65f,
                            int num_matches_thresh1 = 6, int num_matches_thresh2 = 6);
  
      void collectGarbage();
  
  protected:
      void match(const ImageFeatures &features1, const ImageFeatures &features2, MatchesInfo &matches_info);
  
      int num_matches_thresh1_;
      int num_matches_thresh2_;
      Ptr<FeaturesMatcher> impl_;
  };
  ```

cv::detail::BestOf2NearestMatcher::BestOf2NearestMatcher: Constructs a "best of 2 nearest" matcher.
cv::ocl::countNonZero: |-
  Returns the number of non-zero elements in src
  
  Counts non-zero array elements.
cv::ocl::minMax: |-
  Returns void
  
  Finds global minimum and maximum in a whole array or sub-array. Supports
  all data types.
cv::ocl::minMaxLoc: |-
  Returns void
  
  The functions minMaxLoc find minimum and maximum element values and
  their positions. The extremums are searched across the whole array, or,
  if mask is not an empty array, in the specified array region. The
  functions do not work with multi-channel arrays.
cv::ocl::Sum: |-
  Returns the sum of matrix elements for each channel
  
  Counts the sum of matrix elements for each channel.
cv::ocl::sqrSum: |-
  Returns the squared sum of matrix elements for each channel
  
  Counts the squared sum of matrix elements for each channel.
  Miscellaneous Image Transformations ===================================
cv::adaptiveThreshold: |-
  Applies an adaptive threshold to an array.
  
  The function transforms a grayscale image to a binary image according to
  the formulae:
  
    **THRESH_BINARY**
  
  $$dst(x,y) =  \fork{\texttt{maxValue}}{if $src(x,y) > T(x,y)$}{0}{otherwise}$$
  
  
  
    **THRESH_BINARY_INV**
  
  $$dst(x,y) =  \fork{0}{if $src(x,y) > T(x,y)$}{\texttt{maxValue}}{otherwise}$$
  
  
  
  where $T(x,y)$ is a threshold calculated individually for each pixel.
  
  
  *
  :   For the method `ADAPTIVE_THRESH_MEAN_C` , the threshold value
      $T(x,y)$ is a mean of the
      $\texttt{blockSize} \times \texttt{blockSize}$ neighborhood of
      $(x, y)$ minus `C` .
  
  *
  :   For the method `ADAPTIVE_THRESH_GAUSSIAN_C` , the threshold value
      $T(x, y)$ is a weighted sum (cross-correlation with a Gaussian
      window) of the $\texttt{blockSize} \times \texttt{blockSize}$
      neighborhood of $(x, y)$ minus `C` . The default sigma (standard
      deviation) is used for the specified `blockSize` . See
      :ocvgetGaussianKernel .
  
  The function can process the image in-place.
cv::cvtColor: |
  Converts an image from one color space to another.
  
  The function converts an input image from one color space to another. In
  case of a transformation to-from RGB color space, the order of the
  channels should be specified explicitly (RGB or BGR). Note that the
  default color format in OpenCV is often referred to as RGB but it is
  actually BGR (the bytes are reversed). So the first byte in a standard
  (24-bit) color image will be an 8-bit Blue component, the second byte
  will be Green, and the third byte will be Red. The fourth, fifth, and
  sixth bytes would then be the second pixel (Blue, then Green, then Red),
  and so on.
  
  The conventional ranges for R, G, and B channel values are:
  
  *
  :   0 to 255 for `CV_8U` images
  
  *
  :   0 to 65535 for `CV_16U` images
  
  *
  :   0 to 1 for `CV_32F` images
  
  In case of linear transformations, the range does not matter. But in
  case of a non-linear transformation, an input RGB image should be
  normalized to the proper value range to get the correct results, for
  example, for RGB $\rightarrow$ L*u*v* transformation. For example, if
  you have a 32-bit floating-point image directly converted from an 8-bit
  image without any scaling, then it will have the 0..255 value range
  instead of 0..1 assumed by the function. So, before calling `cvtColor` ,
  you need first to scale the image down: :
  
  ```c++
  img *= 1./255;
  cvtColor(img, img, COLOR_BGR2Luv);
  ```
  
  
  If you use `cvtColor` with 8-bit images, the conversion will have some
  information lost. For many applications, this will not be noticeable but
  it is recommended to use 32-bit images in applications that need the
  full range of colors or that convert an image before an operation and
  then convert back.
  
  The function can do the following transformations:
  
  *
  :   RGB $\leftrightarrow$ GRAY (
      `CV_BGR2GRAY, CV_RGB2GRAY, CV_GRAY2BGR, CV_GRAY2RGB` )
      Transformations within RGB space like adding/removing the alpha
      channel, reversing the channel order, conversion to/from 16-bit RGB
      color (R5:G6:B5 or R5:G5:B5), as well as conversion to/from
      grayscale using:
  
  ```c++
  $$\text{RGB[A] to Gray:} \quad Y  \leftarrow 0.299  \cdot R + 0.587  \cdot G + 0.114  \cdot B$$
  
  and
  
  $$\text{Gray to RGB[A]:} \quad R  \leftarrow Y, G  \leftarrow Y, B  \leftarrow Y, A  \leftarrow 0$$
  
  The conversion from a RGB image to gray is done with:
  
      cvtColor(src, bwsrc, COLOR_RGB2GRAY);
  
  More advanced channel reordering can also be done with
  :ocvmixChannels .
  ```
  
  
  *
  :   RGB $\leftrightarrow$ CIE XYZ.Rec 709 with D65 white point (
      `COLOR_BGR2XYZ, COLOR_RGB2XYZ, COLOR_XYZ2BGR, COLOR_XYZ2RGB` ):
  
  ```c++
  $$\begin{bmatrix} X  \\ Y  \\ Z
    \end{bmatrix} \leftarrow \begin{bmatrix} 0.412453 & 0.357580 & 0.180423 \\ 0.212671 & 0.715160 & 0.072169 \\ 0.019334 & 0.119193 & 0.950227
    \end{bmatrix} \cdot \begin{bmatrix} R  \\ G  \\ B
    \end{bmatrix}$$
  
  $$\begin{bmatrix} R  \\ G  \\ B
    \end{bmatrix} \leftarrow \begin{bmatrix} 3.240479 & -1.53715 & -0.498535 \\ -0.969256 &  1.875991 & 0.041556 \\ 0.055648 & -0.204043 & 1.057311
    \end{bmatrix} \cdot \begin{bmatrix} X  \\ Y  \\ Z
    \end{bmatrix}$$
  
  $X$, $Y$ and $Z$ cover the whole value range (in case of
  floating-point images, $Z$ may exceed 1).
  ```
  
  
  *
  :   RGB $\leftrightarrow$ YCrCb JPEG (or YCC) (
      `COLOR_BGR2YCrCb, COLOR_RGB2YCrCb, COLOR_YCrCb2BGR, COLOR_YCrCb2RGB`
      )
  
  ```c++
  $$Y  \leftarrow 0.299  \cdot R + 0.587  \cdot G + 0.114  \cdot B$$
  
  $$Cr  \leftarrow (R-Y)  \cdot 0.713 + delta$$
  
  $$Cb  \leftarrow (B-Y)  \cdot 0.564 + delta$$
  
  $$R  \leftarrow Y + 1.403  \cdot (Cr - delta)$$
  
  $$G  \leftarrow Y - 0.714  \cdot (Cr - delta) - 0.344  \cdot (Cb - delta)$$
  
  $$B  \leftarrow Y + 1.773  \cdot (Cb - delta)$$
  
  where
  
  $$delta =  \left \{ \begin{array}{l l} 128 &  \mbox{for 8-bit images} \\ 32768 &  \mbox{for 16-bit images} \\ 0.5 &  \mbox{for floating-point images} \end{array} \right .$$
  
  Y, Cr, and Cb cover the whole value range.
  ```
  
  
  *
  :   RGB $\leftrightarrow$ HSV ( `COLOR_BGR2HSV, COLOR_RGB2HSV, COLOR_HSV2BGR, COLOR_HSV2RGB` )
      :   In case of 8-bit and 16-bit images, R, G, and B are converted to
          the floating-point format and scaled to fit the 0 to 1 range.
  
  ```c++
  $$V  \leftarrow max(R,G,B)$$
  
  $$S  \leftarrow \fork{\frac{V-min(R,G,B)}{V}}{if $V \neq 0$}{0}{otherwise}$$
  
  $$H  \leftarrow \forkthree{{60(G - B)}/{(V-min(R,G,B))}}{if $V=R$}{{120+60(B - R)}/{(V-min(R,G,B))}}{if $V=G$}{{240+60(R - G)}/{(V-min(R,G,B))}}{if $V=B$}$$
  
  If $H<0$ then $H \leftarrow H+360$ . On output $0 \leq V \leq 1$,
  $0 \leq S \leq 1$, $0 \leq H \leq 360$ .
  
  The values are then converted to the destination data type:
  
  -   8-bit images
  
      > $$V  \leftarrow 255 V, S  \leftarrow 255 S, H  \leftarrow H/2  \text{(to fit to 0 to 255)}$$
  
  -   16-bit images (currently not supported)
  
      > $$V <- 65535 V, S <- 65535 S, H <- H$$
  
  -   32-bit images
      :   H, S, and V are left as is
  ```
  
  
  *
  :   RGB $\leftrightarrow$ HLS ( `COLOR_BGR2HLS, COLOR_RGB2HLS, COLOR_HLS2BGR, COLOR_HLS2RGB` ).
      :   In case of 8-bit and 16-bit images, R, G, and B are converted to
          the floating-point format and scaled to fit the 0 to 1 range.
  
  ```c++
  $$V_{max}  \leftarrow {max}(R,G,B)$$
  
  $$V_{min}  \leftarrow {min}(R,G,B)$$
  
  $$L  \leftarrow \frac{V_{max} + V_{min}}{2}$$
  
  $$S  \leftarrow \fork { \frac{V_{max} - V_{min}}{V_{max} + V_{min}} }{if  $L < 0.5$ }
      { \frac{V_{max} - V_{min}}{2 - (V_{max} + V_{min})} }{if  $L \ge 0.5$ }$$
  
  $$H  \leftarrow \forkthree {{60(G - B)}/{S}}{if  $V_{max}=R$ }
    {{120+60(B - R)}/{S}}{if  $V_{max}=G$ }
    {{240+60(R - G)}/{S}}{if  $V_{max}=B$ }$$
  
  If $H<0$ then $H \leftarrow H+360$ . On output $0 \leq L \leq 1$,
  $0 \leq S \leq 1$, $0 \leq H \leq 360$ .
  
  The values are then converted to the destination data type:
  
  -   8-bit images
  
      > $$V  \leftarrow 255 \cdot V, S  \leftarrow 255 \cdot S, H  \leftarrow H/2 \; \text{(to fit to 0 to 255)}$$
  
  -   16-bit images (currently not supported)
  
      > $$V <- 65535 \cdot V, S <- 65535 \cdot S, H <- H$$
  
  -   32-bit images
      :   H, S, V are left as is
  ```
  
  
  *
  :   RGB $\leftrightarrow$ CIE L*a*b* ( `COLOR_BGR2Lab, COLOR_RGB2Lab, COLOR_Lab2BGR, COLOR_Lab2RGB` ).
      :   In case of 8-bit and 16-bit images, R, G, and B are converted to
          the floating-point format and scaled to fit the 0 to 1 range.
  
  ```c++
  $$\vecthree{X}{Y}{Z} \leftarrow \vecthreethree{0.412453}{0.357580}{0.180423}{0.212671}{0.715160}{0.072169}{0.019334}{0.119193}{0.950227} \cdot \vecthree{R}{G}{B}$$
  
  $$X  \leftarrow X/X_n,  \text{where} X_n = 0.950456$$
  
  $$Z  \leftarrow Z/Z_n,  \text{where} Z_n = 1.088754$$
  
  $$L  \leftarrow \fork{116*Y^{1/3}-16}{for $Y>0.008856$}{903.3*Y}{for $Y \le 0.008856$}$$
  
  $$a  \leftarrow 500 (f(X)-f(Y)) + delta$$
  
  $$b  \leftarrow 200 (f(Y)-f(Z)) + delta$$
  
  where
  
  $$f(t)= \fork{t^{1/3}}{for $t>0.008856$}{7.787 t+16/116}{for $t\leq 0.008856$}$$
  
  and
  
  $$delta =  \fork{128}{for 8-bit images}{0}{for floating-point images}$$
  
  This outputs $0 \leq L \leq 100$, $-127 \leq a \leq 127$,
  $-127 \leq b \leq 127$ . The values are then converted to the
  destination data type:
  
  -   8-bit images
  
      > $$L  \leftarrow L*255/100, \; a  \leftarrow a + 128, \; b  \leftarrow b + 128$$
  
  -   16-bit images
      :   (currently not supported)
  
  -   32-bit images
      :   L, a, and b are left as is
  ```
  
  
  *
  :   RGB $\leftrightarrow$ CIE L*u*v* ( `COLOR_BGR2Luv, COLOR_RGB2Luv, COLOR_Luv2BGR, COLOR_Luv2RGB` ).
      :   In case of 8-bit and 16-bit images, R, G, and B are converted to
          the floating-point format and scaled to fit 0 to 1 range.
  
  ```c++
  $$\vecthree{X}{Y}{Z} \leftarrow \vecthreethree{0.412453}{0.357580}{0.180423}{0.212671}{0.715160}{0.072169}{0.019334}{0.119193}{0.950227} \cdot \vecthree{R}{G}{B}$$
  
  $$L  \leftarrow \fork{116 Y^{1/3}}{for $Y>0.008856$}{903.3 Y}{for $Y\leq 0.008856$}$$
  
  $$u'  \leftarrow 4*X/(X + 15*Y + 3 Z)$$
  
  $$v'  \leftarrow 9*Y/(X + 15*Y + 3 Z)$$
  
  $$u  \leftarrow 13*L*(u' - u_n)  \quad \text{where} \quad u_n=0.19793943$$
  
  $$v  \leftarrow 13*L*(v' - v_n)  \quad \text{where} \quad v_n=0.46831096$$
  
  This outputs $0 \leq L \leq 100$, $-134 \leq u \leq 220$,
  $-140 \leq v \leq 122$ .
  
  The values are then converted to the destination data type:
  
  -   8-bit images
  
      > $$L  \leftarrow 255/100 L, \; u  \leftarrow 255/354 (u + 134), \; v  \leftarrow 255/256 (v + 140)$$
  
  -   16-bit images
      :   (currently not supported)
  
  -   32-bit images
      :   L, u, and v are left as is
  
  The above formulae for converting RGB to/from various color spaces
  have been taken from multiple sources on the web, primarily from the
  Charles Poynton site <http://www.poynton.com/ColorFAQ.html>
  ```
  
  
  *
  :   Bayer $\rightarrow$ RGB (
      `COLOR_BayerBG2BGR, COLOR_BayerGB2BGR, COLOR_BayerRG2BGR, COLOR_BayerGR2BGR, COLOR_BayerBG2RGB, COLOR_BayerGB2RGB, COLOR_BayerRG2RGB, COLOR_BayerGR2RGB`
      ). The Bayer pattern is widely used in CCD and CMOS cameras. It
      enables you to get color pictures from a single plane where R,G, and
      B pixels (sensors of a particular component) are interleaved as
      follows:
  
  ```c++
  ![image](pics/bayer.png)
  
  The output RGB components of a pixel are interpolated from 1, 2, or
  4 neighbors of the pixel having the same color. There are several
  modifications of the above pattern that can be achieved by shifting
  the pattern one pixel left and/or one pixel up. The two letters
  $C_1$ and $C_2$ in the conversion constants `CV_Bayer` $C_1 C_2$
  `2BGR` and `CV_Bayer` $C_1 C_2$ `2RGB` indicate the particular
  pattern type. These are components from the second row, second and
  third columns, respectively. For example, the above pattern has a
  very popular "BG" type.
  ```

cv::distanceTransform: |-
  Calculates the distance to the closest zero pixel for each pixel of the
  source image.
  
  The functions `distanceTransform` calculate the approximate or precise
  distance from every binary image pixel to the nearest zero pixel. For
  zero image pixels, the distance will obviously be zero.
  
  When `maskSize == CV_DIST_MASK_PRECISE` and `distanceType == CV_DIST_L2`
  , the function runs the algorithm described in [Felzenszwalb04]_. This
  algorithm is parallelized with the TBB library.
  
  In other cases, the algorithm [Borgefors86]_ is used. This means that
  for a pixel the function finds the shortest path to the nearest zero
  pixel consisting of basic shifts: horizontal, vertical, diagonal, or
  knight's move (the latest is available for a $5\times 5$ mask). The
  overall distance is calculated as a sum of these basic distances. Since
  the distance function should be symmetric, all of the horizontal and
  vertical shifts must have the same cost (denoted as `a` ), all the
  diagonal shifts must have the same cost (denoted as `b` ), and all
  knight's moves must have the same cost (denoted as `c` ). For the
  `CV_DIST_C` and `CV_DIST_L1` types, the distance is calculated
  precisely, whereas for `CV_DIST_L2` (Euclidean distance) the distance
  can be calculated only with a relative error (a $5\times 5$ mask gives
  more accurate results). For `a`,`b` , and `c` , OpenCV uses the values
  suggested in the original paper:
  
  Typically, for a fast, coarse distance estimation `CV_DIST_L2`, a
  $3\times 3$ mask is used. For a more accurate distance estimation
  `CV_DIST_L2` , a $5\times 5$ mask or the precise algorithm is used. Note
  that both the precise and the approximate algorithms are linear on the
  number of pixels.
  
  The second variant of the function does not only compute the minimum
  distance for each pixel $(x, y)$ but also identifies the nearest
  connected component consisting of zero pixels
  (`labelType==DIST_LABEL_CCOMP`) or the nearest zero pixel
  (`labelType==DIST_LABEL_PIXEL`). Index of the component/pixel is stored
  in $\texttt{labels}(x, y)$ . When `labelType==DIST_LABEL_CCOMP`, the
  function automatically finds connected components of zero pixels in the
  input image and marks them with distinct labels. When
  `labelType==DIST_LABEL_CCOMP`, the function scans through the input
  image and marks all the zero pixels with distinct labels.
  
  In this mode, the complexity is still linear. That is, the function
  provides a very fast way to compute the Voronoi diagram for a binary
  image. Currently, the second variant can use only the approximate
  distance transform algorithm, i.e. `maskSize=CV_DIST_MASK_PRECISE` is
  not supported yet.
cv::floodFill: |-
  Fills a connected component with the given color.
  
  The functions `floodFill` fill a connected component starting from the
  seed point with the specified color. The connectivity is determined by
  the color/brightness closeness of the neighbor pixels. The pixel at
  $(x,y)$ is considered to belong to the repainted domain if:
  
  *
  :   $$\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} (x',y')+ \texttt{upDiff}$$
  
  ```c++
  in case of a grayscale image and floating range
  ```
  
  
  *
  
  $$\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y)  \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}$$
  
  in case of a grayscale image and fixed range
  
  
  *
  
  $$\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,$$
  
  $$\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g$$
  
  and
  
  $$\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b$$
  
  in case of a color image and floating range
  
  
  *
  
  $$\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,$$
  
  $$\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g$$
  
  and
  
  $$\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b$$
  
  in case of a color image and fixed range
  
  
  where $src(x',y')$ is the value of one of pixel neighbors that is
  already known to belong to the component. That is, to be added to the
  connected component, a color/brightness of the pixel should be close
  enough to:
  
  *
  :   Color/brightness of one of its neighbors that already belong to the
      connected component in case of a floating range.
  
  *
  :   Color/brightness of the seed point in case of a fixed range.
  
  Use these functions to either mark a connected component with the
  specified color in-place, or build a mask and then extract the contour,
  or copy the region to another image, and so on. Various modes of the
  function are demonstrated in the `floodfill.cpp` sample.
cv::integral: |-
  Calculates the integral of an image.
  
  The functions calculate one or more integral images for the source image
  as follows:
  
  $$\texttt{sum} (X,Y) =  \sum _{x<X,y<Y}  \texttt{image} (x,y)$$
  
  $$\texttt{sqsum} (X,Y) =  \sum _{x<X,y<Y}  \texttt{image} (x,y)^2$$
  
  $$\texttt{tilted} (X,Y) =  \sum _{y<Y,abs(x-X+1) \leq Y-y-1}  \texttt{image} (x,y)$$
  
  Using these integral images, you can calculate sa um, mean, and standard
  deviation over a specific up-right or rotated rectangular region of the
  image in a constant time, for example:
  
  $$\sum _{x_1 \leq x < x_2,  \, y_1  \leq y < y_2}  \texttt{image} (x,y) =  \texttt{sum} (x_2,y_2)- \texttt{sum} (x_1,y_2)- \texttt{sum} (x_2,y_1)+ \texttt{sum} (x_1,y_1)$$
  
  It makes possible to do a fast blurring or fast block correlation with a
  variable window size, for example. In case of multi-channel images, sums
  for each channel are accumulated independently.
  
  As a practical example, the next figure shows the calculation of the
  integral of a straight rectangle `Rect(3,3,3,2)` and of a tilted
  rectangle `Rect(5,1,2,3)` . The selected pixels in the original `image`
  are shown, as well as the relative pixels in the integral images `sum`
  and `tilted` .
  
  ![image](pics/integral.png)
cv::threshold: |-
  Applies a fixed-level threshold to each array element.
  
  The function applies fixed-level thresholding to a single-channel array.
  The function is typically used to get a bi-level (binary) image out of a
  grayscale image ( :ocvcompare could be also used for this purpose) or
  for removing a noise, that is, filtering out pixels with too small or
  too large values. There are several types of thresholding supported by
  the function. They are determined by `type` :
  
    **THRESH_BINARY**
  
  $$\texttt{dst} (x,y) =  \fork{\texttt{maxval}}{if $\texttt{src}(x,y) > \texttt{thresh}$}{0}{otherwise}$$
  
  
  
    **THRESH_BINARY_INV**
  
  $$\texttt{dst} (x,y) =  \fork{0}{if $\texttt{src}(x,y) > \texttt{thresh}$}{\texttt{maxval}}{otherwise}$$
  
  
  
    **THRESH_TRUNC**
  
  $$\texttt{dst} (x,y) =  \fork{\texttt{threshold}}{if $\texttt{src}(x,y) > \texttt{thresh}$}{\texttt{src}(x,y)}{otherwise}$$
  
  
  
    **THRESH_TOZERO**
  
  $$\texttt{dst} (x,y) =  \fork{\texttt{src}(x,y)}{if $\texttt{src}(x,y) > \texttt{thresh}$}{0}{otherwise}$$
  
  
  
    **THRESH_TOZERO_INV**
  
  $$\texttt{dst} (x,y) =  \fork{0}{if $\texttt{src}(x,y) > \texttt{thresh}$}{\texttt{src}(x,y)}{otherwise}$$
  
  
  
  Also, the special value `THRESH_OTSU` may be combined with one of the
  above values. In this case, the function determines the optimal
  threshold value using the Otsu's algorithm and uses it instead of the
  specified `thresh` . The function returns the computed threshold value.
  Currently, the Otsu's method is implemented only for 8-bit images.
  
  
  ![image](pics/threshold.png)
cv::watershed: |
  Performs a marker-based image segmentation using the watershed
  algorithm.
  
  The function implements one of the variants of watershed, non-parametric
  marker-based segmentation algorithm, described in [Meyer92]_.
  
  Before passing the image to the function, you have to roughly outline
  the desired regions in the image `markers` with positive (`>0`) indices.
  So, every region is represented as one or more connected components with
  the pixel values 1, 2, 3, and so on. Such markers can be retrieved from
  a binary mask using :ocvfindContours and :ocvdrawContours (see the
  `watershed.cpp` demo). The markers are "seeds" of the future image
  regions. All the other pixels in `markers` , whose relation to the
  outlined regions is not known and should be defined by the algorithm,
  should be set to 0's. In the function output, each pixel in markers is
  set to a value of the "seed" components or to -1 at boundaries between
  the regions.
  
  Visual demonstration and usage example of the function can be found in
  the OpenCV samples directory (see the `watershed.cpp` demo).
  
  **note**
  
  Any two neighbor connected components are not necessarily separated by
  a watershed boundary (-1's pixels); for example, they can touch each
  other in the initial marker image passed to the function.

cv::grabCut: |-
  Runs the GrabCut algorithm.
  
  The function implements the [GrabCut image segmentation
  algorithm](http://en.wikipedia.org/wiki/GrabCut). See the sample
  `grabcut.cpp` to learn how to use the function.
cv::ml. Machine Learning: |-
  The Machine Learning Library (MLL) is a set of classes and functions for
  statistical classification, regression, and clustering of data.
  
  Most of the classification and regression algorithms are implemented as
  C++ classes. As the algorithms have different sets of features (like an
  ability to handle missing measurements or categorical input variables),
  there is a little common ground between the classes. This common ground
  is defined by the class CvStatModel that all the other ML classes are
  derived from.
cv::MLData: |-
  For the machine learning algorithms, the data set is often stored in a
  file of the `.csv`-like format. The file contains a table of predictor
  and response values where each row of the table corresponds to a sample.
  Missing values are supported. The UC Irvine Machine Learning Repository
  (<http://archive.ics.uci.edu/ml/>) provides many data sets stored in
  such a format to the machine learning community. The class `MLData` is
  implemented to easily load the data for training one of the OpenCV
  machine learning algorithms. For float values, only the `'.'` separator
  is supported. The table can have a header and in such case the user have
  to set the number of the header lines to skip them duaring the file
  reading.
cv::CvMLData: |
  Class for loading the data from a `.csv` file. :
  
  ```c++
  class CV_EXPORTS CvMLData
  {
  public:
      CvMLData();
      virtual ~CvMLData();
  
      int read_csv(const char* filename);
  
      const CvMat* get_values() const;
      const CvMat* get_responses();
      const CvMat* get_missing() const;
  
      void set_response_idx( int idx );
      int get_response_idx() const;
  
  
      void set_train_test_split( const CvTrainTestSplit * spl);
      const CvMat* get_train_sample_idx() const;
      const CvMat* get_test_sample_idx() const;
      void mix_train_and_test_idx();
  
      const CvMat* get_var_idx();
      void change_var_idx( int vi, bool state );
  
      const CvMat* get_var_types();
      void set_var_types( const char* str );
  
      int get_var_type( int var_idx ) const;
      void change_var_type( int var_idx, int type);
  
      void set_delimiter( char ch );
      char get_delimiter() const;
  
      void set_miss_ch( char ch );
      char get_miss_ch() const;
  
      const std::map<String, int>& get_class_labels_map() const;
  
  protected:
      ...
  };
  ```

cv::CvMLData::read_csv: "Reads the data set from a `.csv`-like `filename` file and stores all\n\
  read values in a matrix.\n\n\
  While reading the data, the method tries to define the type of variables\n\
  (predictors and responses): ordered or categorical. If a value of the\n\
  variable is not numerical (except for the label for a missing value),\n\
  the type of the variable is set to `CV_VAR_CATEGORICAL`. If all existing\n\
  values of the variable are numerical, the type of the variable is set to\n\
  `CV_VAR_ORDERED`. So, the default definition of variables types works\n\
  correctly for all cases except the case of a categorical variable with\n\
  numerical class labels. In this case, the type `CV_VAR_ORDERED` is set.\n\
  You should change the type to `CV_VAR_CATEGORICAL` using the method\n\
  :ocvCvMLData::change_var_type. For categorical variables, a common map\n\
  is built to convert a string class label to the numerical class label.\n\
  Use :ocvCvMLData::get_class_labels_map to obtain this map.\n\n\
  Also, when reading the data, the method constructs the mask of missing\n\
  values. For example, values are equal to '?'."
cv::CvMLData::get_values: |-
  Returns a pointer to the matrix of predictors and response values
  
  The method returns a pointer to the matrix of predictor and response
  `values` or `0` if the data has not been loaded from the file yet.
  
  The row count of this matrix equals the sample count. The column count
  equals predictors `+ 1` for the response (if exists) count. This means
  that each row of the matrix contains values of one sample predictor and
  response. The matrix type is `CV_32FC1`.
cv::CvMLData::get_responses: |-
  Returns a pointer to the matrix of response values
  
  The method returns a pointer to the matrix of response values or throws
  an exception if the data has not been loaded from the file yet.
  
  This is a single-column matrix of the type `CV_32FC1`. Its row count is
  equal to the sample count, one column and .
cv::CvMLData::get_missing: "Returns a pointer to the mask matrix of missing values\n\n\
  The method returns a pointer to the mask matrix of missing values or\n\
  throws an exception if the data has not been loaded from the file yet.\n\n\
  This matrix has the same size as the `values` matrix (see\n\
  :ocvCvMLData::get_values) and the type `CV_8UC1`."
cv::CvMLData::set_response_idx: |-
  Specifies index of response column in the data matrix
  
  The method sets the index of a response column in the `values` matrix
  (see :ocvCvMLData::get_values) or throws an exception if the data has
  not been loaded from the file yet.
  
  The old response columns become predictors. If `idx < 0`, there is no
  response.
cv::CvMLData::get_response_idx: |-
  Returns index of the response column in the loaded data matrix
  
  The method returns the index of a response column in the `values` matrix
  (see :ocvCvMLData::get_values) or throws an exception if the data has
  not been loaded from the file yet.
  
  If `idx < 0`, there is no response.
cv::CvMLData::set_train_test_split: "Divides the read data set into two disjoint training and test subsets.\n\n\
  This method sets parameters for such a split using `spl` (see\n\
  :ocvCvTrainTestSplit) or throws an exception if the data has not been\n\
  loaded from the file yet."
cv::CvMLData::get_train_sample_idx: |-
  Returns the matrix of sample indices for a training subset
  
  The method returns the matrix of sample indices for a training subset.
  This is a single-row matrix of the type `CV_32SC1`. If data split is not
  set, the method returns `0`. If the data has not been loaded from the
  file yet, an exception is thrown.
cv::CvMLData::get_test_sample_idx: Returns the matrix of sample indices for a testing subset
cv::CvMLData::mix_train_and_test_idx: "Mixes the indices of training and test samples\n\n\
  The method shuffles the indices of training and test samples preserving\n\
  sizes of training and test subsets if the data split is set by\n\
  :ocvCvMLData::get_values. If the data has not been loaded from the file\n\
  yet, an exception is thrown."
cv::CvMLData::get_var_idx: |-
  Returns the indices of the active variables in the data matrix
  
  The method returns the indices of variables (columns) used in the
  `values` matrix (see :ocvCvMLData::get_values).
  
  It returns `0` if the used subset is not set. It throws an exception if
  the data has not been loaded from the file yet. Returned matrix is a
  single-row matrix of the type `CV_32SC1`. Its column count is equal to
  the size of the used variable subset.
cv::CvMLData::change_var_idx: |-
  Enables or disables particular variable in the loaded data
  
  By default, after reading the data set all variables in the `values`
  matrix (see :ocvCvMLData::get_values) are used. But you may want to use
  only a subset of variables and include/exclude (depending on `state`
  value) a variable with the `vi` index from the used subset. If the data
  has not been loaded from the file yet, an exception is thrown.
cv::CvMLData::get_var_types: |-
  Returns a matrix of the variable types.
  
  The function returns a single-row matrix of the type `CV_8UC1`, where
  each element is set to either `CV_VAR_ORDERED` or `CV_VAR_CATEGORICAL`.
  The number of columns is equal to the number of variables. If data has
  not been loaded from file yet an exception is thrown.
cv::CvMLData::set_var_types: |-
  Sets the variables types in the loaded data.
  
  In the string, a variable type is followed by a list of variables
  indices. For example: `"ord[0-17],cat[18]"`,
  `"ord[0,2,4,10-12], cat[1,3,5-9,13,14]"`, `"cat"` (all variables are
  categorical), `"ord"` (all variables are ordered).
cv::CvMLData::get_header_lines_number: Returns a number of the table header lines.
cv::CvMLData::set_header_lines_number: |-
  Sets a number of the table header lines.
  
  By default it is supposed that the table does not have a header, i.e. it
  contains only the data.
cv::CvMLData::get_var_type: |-
  Returns type of the specified variable
  
  The method returns the type of a variable by the index `var_idx` (
  `CV_VAR_ORDERED` or `CV_VAR_CATEGORICAL`).
cv::CvMLData::change_var_type: |-
  Changes type of the specified variable
  
  The method changes type of variable with index `var_idx` from existing
  type to `type` ( `CV_VAR_ORDERED` or `CV_VAR_CATEGORICAL`).
cv::CvMLData::set_delimiter: |-
  Sets the delimiter in the file used to separate input numbers
  
  The method sets the delimiter for variables in a file. For example:
  `','` (default), `';'`, `' '` (space), or other characters. The
  floating-point separator `'.'` is not allowed.
cv::CvMLData::get_delimiter: Returns the currently used delimiter character.
cv::CvMLData::set_miss_ch: |-
  Sets the character used to specify missing values
  
  The method sets the character used to specify missing values. For
  example: `'?'` (default), `'-'`. The floating-point separator `'.'` is
  not allowed.
cv::CvMLData::get_miss_ch: Returns the currently used missing value character.
cv::CvMLData::get_class_labels_map: |-
  Returns a map that converts strings to labels.
  
  The method returns a map that converts string class labels to the
  numerical class labels. It can be used to get an original class label as
  in a file.
cv::CvTrainTestSplit: |
  Structure setting the split of a data set read by :ocvCvMLData. :
  
  ```c++
  struct CvTrainTestSplit
  {
      CvTrainTestSplit();
      CvTrainTestSplit( int train_sample_count, bool mix = true);
      CvTrainTestSplit( float train_sample_portion, bool mix = true);
  
      union
      {
          int count;
          float portion;
      } train_sample_part;
      int train_sample_part_mode;
  
      bool mix;
  };
  ```
  
  
  There are two ways to construct a split:
  
    Set the training sample count (subset size) `train_sample_count`.
  Other existing samples are located in a test subset.
  
  
    Set a training sample portion in `[0,..1]`. The flag `mix` is used
  to mix training and test samples indices when the split is set.
  Otherwise, the data set is split in the storing order: the first
  part of samples of a given size is a training subset, the second
  part is a test subset.

cv::CalcOpticalFlowBM: |-
  Calculates the optical flow for two images by using the block matching
  method.
  
  The function calculates the optical flow for overlapped blocks
  `block_size.width x block_size.height` pixels each, thus the velocity
  fields are smaller than the original images. For every block in `prev`
  the functions tries to find a similar block in `curr` in some
  neighborhood of the original block or shifted by
  `(velx(x0,y0), vely(x0,y0))` block as has been calculated by previous
  function call (if `use_previous=1`)
cv::CalcOpticalFlowHS: |-
  Calculates the optical flow for two images using Horn-Schunck algorithm.
  
  The function computes the flow for every pixel of the first input image
  using the Horn and Schunck algorithm [Horn81]_. The function is
  obsolete. To track sparse features, use :ocvcalcOpticalFlowPyrLK. To
  track all the pixels, use :ocvcalcOpticalFlowFarneback.
cv::CalcOpticalFlowLK: |-
  Calculates the optical flow for two images using Lucas-Kanade algorithm.
  
  The function computes the flow for every pixel of the first input image
  using the Lucas and Kanade algorithm [Lucas81]_. The function is
  obsolete. To track sparse features, use :ocvcalcOpticalFlowPyrLK. To
  track all the pixels, use :ocvcalcOpticalFlowFarneback.
cv::calcOpticalFlowPyrLK: |-
  Calculates an optical flow for a sparse feature set using the iterative
  Lucas-Kanade method with pyramids.
  
  The function implements a sparse iterative version of the Lucas-Kanade
  optical flow in pyramids. See [Bouguet00]_. The function is
  parallelized with the TBB library.
cv::buildOpticalFlowPyramid: "Constructs the image pyramid which can be passed to\n\
  :ocvcalcOpticalFlowPyrLK."
cv::calcOpticalFlowFarneback: |-
  Computes a dense optical flow using the Gunnar Farneback's algorithm.
  
  The function finds an optical flow for each `prev` pixel using the
  [Farneback2003]_ algorithm so that
  
  $$\texttt{prev} (y,x)  \sim \texttt{next} ( y + \texttt{flow} (y,x)[1],  x + \texttt{flow} (y,x)[0])$$
cv::estimateRigidTransform: |
  Computes an optimal affine transformation between two 2D point sets.
  
  The function finds an optimal affine transform *[A|b]* (a `2 x 3`
  floating-point matrix) that approximates best the affine transformation
  between:
  
  *
  :   Two point sets
  
  *
  :   Two raster images. In this case, the function first finds some
      features in the `src` image and finds the corresponding features
      in `dst` image. After that, the problem is reduced to the first
      case.
  
  In case of point sets, the problem is formulated as follows: you need to
  find a 2x2 matrix *A* and 2x1 vector *b* so that:
  
  $$[A^*|b^*] = arg  \min _{[A|b]}  \sum _i  | \texttt{dst}[i] - A { \texttt{src}[i]}^T - b  | ^2$$
  
  where `src[i]` and `dst[i]` are the i-th points in `src` and `dst`,
  respectively
  
  $[A|b]$ can be either arbitrary (when `fullAffine=true` ) or have a
  form of
  
  $$\begin{bmatrix} a_{11} & a_{12} & b_1  \ -a_{12} & a_{11} & b_2  \end{bmatrix}$$
  
  when `fullAffine=false` .

cv::findTransformECC: "Finds the geometric transform (warp) between two images in terms of the\n\
  ECC criterion [EP08]_.\n\n\
  The function estimates the optimum transformation (`warpMatrix`) with\n\
  respect to ECC criterion ([EP08]_), that is\n\n\
  $$\\texttt{warpMatrix} = \\texttt{warpMatrix} = \\arg\\max_{W} \\texttt{ECC}(\\texttt{templateImage}(x,y),\\texttt{inputImage}(x',y'))$$\n\n\
  where\n\n\
  $$\\begin{bmatrix} x' \\ y' \\end{bmatrix} = W \\cdot \\begin{bmatrix} x \\ y \\ 1 \\end{bmatrix}$$\n\n\
  (the equation holds with homogeneous coordinates for homography). It\n\
  returns the final enhanced correlation coefficient, that is the\n\
  correlation coefficient between the template image and the final warped\n\
  input image. When a $3\\times 3$ matrix is given with `motionType` =0, 1\n\
  or 2, the third row is ignored.\n\n\
  Unlike :ocvfindHomography and :ocvestimateRigidTransform, the function\n\
  :ocvfindTransformECC implements an area-based alignment that builds on\n\
  intensity similarities. In essence, the function updates the initial\n\
  transformation that roughly aligns the images. If this information is\n\
  missing, the identity warp (unity matrix) should be given as input. Note\n\
  that if images undergo strong displacements/rotations, an initial\n\
  transformation that roughly aligns the images is necessary (e.g., a\n\
  simple euclidean/similarity transform that allows for the images showing\n\
  the same image content approximately). Use inverse warping in the second\n\
  image to take an image close to the first one, i.e. use the flag\n\
  `WARP_INVERSE_MAP` with :ocvwarpAffine or :ocvwarpPerspective. See also\n\
  the OpenCV sample `image_alignment.cpp` that demonstrates the use of the\n\
  function. Note that the function throws an exception if algorithm does\n\
  not converges."
cv::updateMotionHistory: "Updates the motion history image by a moving silhouette.\n\n\
  The function updates the motion history image as follows:\n\n\
  $$\\texttt{mhi} (x,y)= \\forkthree{\\texttt{timestamp}}{if $\\texttt{silhouette}(x,y) \\ne 0$}{0}{if $\\texttt{silhouette}(x,y) = 0$ and $\\texttt{mhi} < (\\texttt{timestamp} - \\texttt{duration})$}{\\texttt{mhi}(x,y)}{otherwise}$$\n\n\
  That is, MHI pixels where the motion occurs are set to the current\n\
  `timestamp` , while the pixels where the motion happened last time a\n\
  long time ago are cleared.\n\n\
  The function, together with :ocvcalcMotionGradient and\n\
  :ocvcalcGlobalOrientation , implements a motion templates technique\n\
  described in [Davis97]_ and [Bradski00]_. See also the OpenCV sample\n\
  `motempl.c` that demonstrates the use of all the motion template\n\
  functions."
cv::calcMotionGradient: |-
  Calculates a gradient orientation of a motion history image.
  
  The function calculates a gradient orientation at each pixel $(x, y)$
  as:
  
  $$\texttt{orientation} (x,y)= \arctan{\frac{d\texttt{mhi}/dy}{d\texttt{mhi}/dx}}$$
  
  In fact, :ocvfastAtan2 and :ocvphase are used so that the computed angle
  is measured in degrees and covers the full range 0..360. Also, the
  `mask` is filled to indicate pixels where the computed angle is valid.
cv::calcGlobalOrientation: |-
  Calculates a global motion orientation in a selected region.
  
  The function calculates an average motion direction in the selected
  region and returns the angle between 0 degrees and 360 degrees. The
  average direction is computed from the weighted orientation histogram,
  where a recent motion has a larger weight and the motion occurred in the
  past has a smaller weight, as recorded in `mhi` .
cv::segmentMotion: "Splits a motion history image into a few parts corresponding to separate\n\
  independent motions (for example, left hand, right hand).\n\n\
  The function finds all of the motion segments and marks them in\n\
  `segmask` with individual values (1,2,...). It also computes a vector\n\
  with ROIs of motion connected components. After that the motion\n\
  direction for every component can be calculated with\n\
  :ocvcalcGlobalOrientation using the extracted mask of the particular\n\
  component."
cv::CamShift: |-
  Finds an object center, size, and orientation.
  
  The function implements the CAMSHIFT object tracking algorithm
  [Bradski98]_. First, it finds an object center using :ocvmeanShift and
  then adjusts the window size and finds the optimal rotation. The
  function returns the rotated rectangle structure that includes the
  object position, size, and orientation. The next position of the search
  window can be obtained with `RotatedRect::boundingRect()` .
  
  See the OpenCV sample `camshiftdemo.c` that tracks colored objects.
cv::meanShift: "Finds an object on a back projection image.\n\n\
  The function implements the iterative object search algorithm. It takes\n\
  the input back projection of an object and the initial position. The\n\
  mass center in `window` of the back projection image is computed and the\n\
  search window center shifts to the mass center. The procedure is\n\
  repeated until the specified number of iterations `criteria.maxCount` is\n\
  done or until the window center shifts by less than `criteria.epsilon` .\n\
  The algorithm is used inside :ocvCamShift and, unlike :ocvCamShift , the\n\
  search window size or orientation do not change during the search. You\n\
  can simply pass the output of :ocvcalcBackProject to this function. But\n\
  better results can be obtained if you pre-filter the back projection and\n\
  remove the noise. For example, you can do this by retrieving connected\n\
  components with :ocvfindContours , throwing away contours with small\n\
  area ( :ocvcontourArea ), and rendering the remaining contours with\n\
  :ocvdrawContours ."
cv::KalmanFilter: |-
  The class implements a standard Kalman filter
  <http://en.wikipedia.org/wiki/Kalman_filter>, [Welch95]_. However, you
  can modify `transitionMatrix`, `controlMatrix`, and `measurementMatrix`
  to get an extended Kalman filter functionality. See the OpenCV sample
  `kalman.cpp` .
cv::KalmanFilter::KalmanFilter: |
  The constructors.
  
  **note**
  
  In C API when `CvKalman* kalmanFilter` structure is not needed
  anymore, it should be released with `cvReleaseKalman(&kalmanFilter)`

cv::KalmanFilter::init: Re-initializes Kalman filter. The previous content is destroyed.
cv::KalmanFilter::predict: Computes a predicted state.
cv::KalmanFilter::correct: Updates the predicted state from the measurement.
cv::BackgroundSubtractor: |-
  Base class for background/foreground segmentation. :
  
  ```c++
  class BackgroundSubtractor : public Algorithm
  {
  public:
      virtual ~BackgroundSubtractor();
      virtual void apply(InputArray image, OutputArray fgmask, double learningRate=0);
      virtual void getBackgroundImage(OutputArray backgroundImage) const;
  };
  ```
  
  
  The class is only used to define the common interface for the whole
  family of background/foreground segmentation algorithms.
cv::BackgroundSubtractor::apply: Computes a foreground mask.
cv::BackgroundSubtractor::getBackgroundImage: |
  Computes a background image.
  
  **note**
  
  Sometimes the background image can be very blurry, as it contain the
  average background statistics.

cv::BackgroundSubtractorMOG: |-
  Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
  
  The class implements the algorithm described in [KB2001]_.
cv::createBackgroundSubtractorMOG: Creates mixture-of-gaussian background subtractor
cv::BackgroundSubtractorMOG2: |-
  Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
  
  The class implements the Gaussian mixture model background subtraction
  described in [Zivkovic2004]_ and [Zivkovic2006]_ .
cv::createBackgroundSubtractorMOG2: Creates MOG2 Background Subtractor
cv::BackgroundSubtractorMOG2::getHistory: Returns the number of last frames that affect the background model
cv::BackgroundSubtractorMOG2::setHistory: Sets the number of last frames that affect the background model
cv::BackgroundSubtractorMOG2::getNMixtures: Returns the number of gaussian components in the background model
cv::BackgroundSubtractorMOG2::setNMixtures: Sets the number of gaussian components in the background model
cv::BackgroundSubtractorMOG2::getBackgroundRatio: |-
  Returns the "background ratio" parameter of the algorithm
  
  If a foreground pixel keeps semi-constant value for about
  `backgroundRatio*history` frames, it's considered background and added
  to the model as a center of a new component. It corresponds to `TB`
  parameter in the paper.
cv::BackgroundSubtractorMOG2::setBackgroundRatio: Sets the "background ratio" parameter of the algorithm
cv::BackgroundSubtractorMOG2::getVarThresholdGen: |-
  Returns the variance scale factor for the pixel-model match
  
  Threshold for the squared Mahalanobis distance that helps decide when a
  sample is close to the existing components (corresponds to `Tg` in the
  paper). If a pixel is not close to any component, it is considered
  foreground or added as a new component. `3 sigma => Tg=3*3=9` is
  default. A smaller `Tg` value generates more components. A higher `Tg`
  value may result in a small number of components but they can grow too
  large.
cv::BackgroundSubtractorMOG2::setVarThresholdGen: Sets the variance scale factor for the pixel-model match
cv::BackgroundSubtractorMOG2::getVarInit: Returns the initial variance of each gaussian component
cv::BackgroundSubtractorMOG2::setVarInit: Sets the initial variance of each gaussian component
cv::BackgroundSubtractorMOG2::getComplexityReductionThreshold: |-
  Returns the complexity reduction threshold
  
  This parameter defines the number of samples needed to accept to prove
  the component exists. `CT=0.05` is a default value for all the samples.
  By setting `CT=0` you get an algorithm very similar to the standard
  Stauffer&Grimson algorithm.
cv::BackgroundSubtractorMOG2::setComplexityReductionThreshold: Sets the complexity reduction threshold
cv::BackgroundSubtractorMOG2::getDetectShadows: |-
  Returns the shadow detection flag
  
  If true, the algorithm detects shadows and marks them. See
  createBackgroundSubtractorMOG2 for details.
cv::BackgroundSubtractorMOG2::setDetectShadows: Enables or disables shadow detection
cv::BackgroundSubtractorMOG2::getShadowValue: |-
  Returns the shadow value
  
  Shadow value is the value used to mark shadows in the foreground mask.
  Default value is 127. Value 0 in the mask always means background, 255
  means foreground.
cv::BackgroundSubtractorMOG2::setShadowValue: Sets the shadow value
cv::BackgroundSubtractorMOG2::getShadowThreshold: |-
  Returns the shadow threshold
  
  A shadow is detected if pixel is a darker version of the background. The
  shadow threshold (`Tau` in the paper) is a threshold defining how much
  darker the shadow can be. `Tau= 0.5` means that if a pixel is more than
  twice darker then it is not shadow. See Prati, Mikic, Trivedi and
  Cucchiarra, *Detecting Moving Shadows...*, IEEE PAMI,2003.
cv::BackgroundSubtractorMOG2::setShadowThreshold: Sets the shadow threshold
cv::calcOpticalFlowSF: |-
  Calculate an optical flow using "SimpleFlow" algorithm.
  
  See [Tao2012]_. And site of project -
  <http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/>.
cv::createOptFlow_DualTVL1: "\"Dual TV L1\" Optical Flow Algorithm."
cv::DenseOpticalFlow::calc: Calculates an optical flow.
cv::DenseOpticalFlow::collectGarbage: Releases all inner buffers.
cv::detail::Estimator: |
  Rotation estimator base class. It takes features of all images, pairwise
  matches between all images and estimates rotations of all cameras.
  
  **note**
  
  The coordinate system origin is implementation-dependent, but you can
  always normalize the rotations in respect to the first camera, for
  instance.
  
  
  ```c++
  class CV_EXPORTS Estimator
  {
  public:
      virtual ~Estimator() {}
  
      void operator ()(const std::vector<ImageFeatures> &features, const std::vector<MatchesInfo> &pairwise_matches,
                       std::vector<CameraParams> &cameras)
          { estimate(features, pairwise_matches, cameras); }
  
  protected:
      virtual void estimate(const std::vector<ImageFeatures> &features, const std::vector<MatchesInfo> &pairwise_matches,
                            std::vector<CameraParams> &cameras) = 0;
  };
  ```

cv::detail::Estimator::operator(): Estimates camera parameters.
cv::detail::Estimator::estimate: |-
  This method must implement camera parameters estimation logic in order
  to make the wrapper detail::Estimator::operator()_ work.
cv::detail::HomographyBasedEstimator: |
  Homography based rotation estimator. :
  
  ```c++
  class CV_EXPORTS HomographyBasedEstimator : public Estimator
  {
  public:
      HomographyBasedEstimator(bool is_focals_estimated = false)
          : is_focals_estimated_(is_focals_estimated) {}
  
  private:
      /* hidden */
  };
  ```

cv::detail::BundleAdjusterBase: "Base class for all camera parameters refinement methods. :\n\n\
  ```c++\n\
  class CV_EXPORTS BundleAdjusterBase : public Estimator\n\
  {\n\
  public:\n    const Mat refinementMask() const { return refinement_mask_.clone(); }\n    void setRefinementMask(const Mat &mask)\n    {\n        CV_Assert(mask.type() == CV_8U && mask.size() == Size(3, 3));\n        refinement_mask_ = mask.clone();\n    }\n\n    double confThresh() const { return conf_thresh_; }\n    void setConfThresh(double conf_thresh) { conf_thresh_ = conf_thresh; }\n\n    CvTermCriteria termCriteria() { return term_criteria_; }\n    void setTermCriteria(const CvTermCriteria& term_criteria) { term_criteria_ = term_criteria; }\n\n\
  protected:\n    BundleAdjusterBase(int num_params_per_cam, int num_errs_per_measurement)\n        : num_params_per_cam_(num_params_per_cam),\n          num_errs_per_measurement_(num_errs_per_measurement)\n    {\n        setRefinementMask(Mat::ones(3, 3, CV_8U));\n        setConfThresh(1.);\n        setTermCriteria(cvTermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 1000, DBL_EPSILON));\n    }\n\n    // Runs bundle adjustment\n    virtual void estimate(const std::vector<ImageFeatures> &features,\n                          const std::vector<MatchesInfo> &pairwise_matches,\n                          std::vector<CameraParams> &cameras);\n\n    virtual void setUpInitialCameraParams(const std::vector<CameraParams> &cameras) = 0;\n    virtual void obtainRefinedCameraParams(std::vector<CameraParams> &cameras) const = 0;\n    virtual void calcError(Mat &err) = 0;\n    virtual void calcJacobian(Mat &jac) = 0;\n\n    // 3x3 8U mask, where 0 means don't refine respective parameter, != 0 means refine\n    Mat refinement_mask_;\n\n    int num_images_;\n    int total_num_matches_;\n\n    int num_params_per_cam_;\n    int num_errs_per_measurement_;\n\n    const ImageFeatures *features_;\n    const MatchesInfo *pairwise_matches_;\n\n    // Threshold to filter out poorly matched image pairs\n    double conf_thresh_;\n\n    //Levenberg\xE2\x80\x93Marquardt algorithm termination criteria\n    CvTermCriteria term_criteria_;\n\n    // Camera parameters matrix (CV_64F)\n    Mat cam_params_;\n\n    // Connected images pairs\n    std::vector<std::pair<int,int> > edges_;\n\
  };\n\
  ```\n"
cv::detail::BundleAdjusterBase::BundleAdjusterBase: Construct a bundle adjuster base instance.
cv::detail::BundleAdjusterBase::setUpInitialCameraParams: Sets initial camera parameter to refine.
cv::detail::BundleAdjusterBase::calcError: Calculates error vector.
cv::detail::BundleAdjusterBase::calcJacobian: Calculates the cost function jacobian.
cv::detail::BundleAdjusterBase::obtainRefinedCameraParams: Gets the refined camera parameters.
cv::detail::BundleAdjusterReproj: |
  Implementation of the camera parameters refinement algorithm which
  minimizes sum of the reprojection error squares. :
  
  ```c++
  class CV_EXPORTS BundleAdjusterReproj : public BundleAdjusterBase
  {
  public:
      BundleAdjusterReproj() : BundleAdjusterBase(7, 2) {}
  
  private:
      /* hidden */
  };
  ```

cv::detail::BundleAdjusterRay: |
  Implementation of the camera parameters refinement algorithm which
  minimizes sum of the distances between the rays passing through the
  camera center and a feature. :
  
  ```c++
  class CV_EXPORTS BundleAdjusterRay : public BundleAdjusterBase
  {
  public:
      BundleAdjusterRay() : BundleAdjusterBase(4, 3) {}
  
  private:
      /* hidden */
  };
  ```

cv::detail::WaveCorrectKind: Wave correction kind.
cv::detail::waveCorrect: Tries to make panorama more horizontal (or vertical).
cv::Neural Networks: |-
  ML implements feed-forward artificial neural networks or, more
  particularly, multi-layer perceptrons (MLP), the most commonly used type
  of neural networks. MLP consists of the input layer, output layer, and
  one or more hidden layers. Each layer of MLP includes one or more
  neurons directionally linked with the neurons from the previous and the
  next layer. The example below represents a 3-layer perceptron with three
  inputs, two outputs, and the hidden layer including five neurons:
  
  ![image](pics/mlp.png)
  
  All the neurons in MLP are similar. Each of them has several input links
  (it takes the output values from several neurons in the previous layer
  as input) and several output links (it passes the response to several
  neurons in the next layer). The values retrieved from the previous layer
  are summed up with certain weights, individual for each neuron, plus the
  bias term. The sum is transformed using the activation function $f$ that
  may be also different for different neurons.
  
  ![image](pics/neuron_model.png)
  
  In other words, given the outputs $x_j$ of the layer $n$ , the outputs
  $y_i$ of the layer $n+1$ are computed as:
  
  $$u_i =  \sum _j (w^{n+1}_{i,j}*x_j) + w^{n+1}_{i,bias}$$
  
  $$y_i = f(u_i)$$
  
  Different activation functions may be used. ML implements three standard
  functions:
  
  *
  :   Identity function ( `CvANN_MLP::IDENTITY` ): $f(x)=x$
  
  *
  :   Symmetrical sigmoid ( `CvANN_MLP::SIGMOID_SYM` ):
      $f(x)=\beta*(1-e^{-\alpha x})/(1+e^{-\alpha x}$ ), which is the
      default choice for MLP. The standard sigmoid with
      $\beta =1, \alpha =1$ is shown below:
  
  ```c++
  ![image](pics/sigmoid_bipolar.png)
  ```
  
  
  *
  :   Gaussian function ( `CvANN_MLP::GAUSSIAN` ):
      $f(x)=\beta e^{-\alpha x*x}$ , which is not completely supported at
      the moment.
  
  In ML, all the neurons have the same activation functions, with the same
  free parameters ( $\alpha, \beta$ ) that are specified by user and are
  not altered by the training algorithms.
  
  So, the whole trained network works as follows:
  
   Take the feature vector as input. The vector size is equal to the
  size of the input layer.
  
  
   Pass values as input to the first hidden layer.
  
  
   Compute outputs of the hidden layer using the weights and the
  activation functions.
  
  
   Pass outputs further downstream until you compute the output layer.
  
  
  So, to compute the network, you need to know all the weights
  $w^{n+1)}_{i,j}$ . The weights are computed by the training algorithm.
  The algorithm takes a training set, multiple input vectors with the
  corresponding output vectors, and iteratively adjusts the weights to
  enable the network to give the desired response to the provided input
  vectors.
  
  The larger the network size (the number of hidden layers and their
  sizes) is, the more the potential network flexibility is. The error on
  the training set could be made arbitrarily small. But at the same time
  the learned network also "learns" the noise present in the training set,
  so the error on the test set usually starts increasing after the network
  size reaches a limit. Besides, the larger networks are trained much
  longer than the smaller ones, so it is reasonable to pre-process the
  data, using :ocvPCA::operator() or similar technique, and train a
  smaller network on only essential features.
  
  Another MLP feature is an inability to handle categorical data as is.
  However, there is a workaround. If a certain feature in the input or
  output (in case of `n` -class classifier for $n>2$ ) layer is
  categorical and can take $M>2$ different values, it makes sense to
  represent it as a binary tuple of `M` elements, where the `i` -th
  element is 1 if and only if the feature is equal to the `i` -th value
  out of `M` possible. It increases the size of the input/output layer but
  speeds up the training algorithm convergence and at the same time
  enables "fuzzy" values of such variables, that is, a tuple of
  probabilities instead of a fixed value.
  
  ML implements two algorithms for training MLP's. The first algorithm is
  a classical random sequential back-propagation algorithm. The second
  (default) one is a batch RPROP algorithm.
cv::CvANN_MLP_TrainParams::CvANN_MLP_TrainParams: |
  The constructors.
  
  By default the RPROP algorithm is used:
  
  ```c++
  CvANN_MLP_TrainParams::CvANN_MLP_TrainParams()
  {
      term_crit = cvTermCriteria( CV_TERMCRIT_ITER + CV_TERMCRIT_EPS, 1000, 0.01 );
      train_method = RPROP;
      bp_dw_scale = bp_moment_scale = 0.1;
      rp_dw0 = 0.1; rp_dw_plus = 1.2; rp_dw_minus = 0.5;
      rp_dw_min = FLT_EPSILON; rp_dw_max = 50.;
  }
  ```

cv::CvANN_MLP: |-
  MLP model.
  
  Unlike many other models in ML that are constructed and trained at once,
  in the MLP model these steps are separated. First, a network with the
  specified topology is created using the non-default constructor or the
  method :ocvCvANN_MLP::create. All the weights are set to zeros. Then,
  the network is trained using a set of input and output vectors. The
  training procedure can be repeated more than once, that is, the weights
  can be adjusted based on the new training data.
cv::CvANN_MLP::CvANN_MLP: |-
  The constructors.
  
  The advanced constructor allows to create MLP with the specified
  topology. See :ocvCvANN_MLP::create for details.
cv::CvANN_MLP::create: |-
  Constructs MLP with the specified topology.
  
  The method creates an MLP network with the specified topology and
  assigns the same activation function to all the neurons.
cv::CvANN_MLP::train: |-
  Trains/updates MLP.
  
  This method applies the specified training algorithm to
  computing/adjusting the network weights. It returns the number of done
  iterations.
  
  The RPROP training algorithm is parallelized with the TBB library.
cv::CvANN_MLP::predict: |-
  Predicts responses for input samples.
  
  The method returns a dummy value which should be ignored.
cv::CvANN_MLP::get_layer_count: Returns the number of layers in the MLP.
cv::CvANN_MLP::get_layer_sizes: |-
  Returns numbers of neurons in each layer of the MLP.
  
  The method returns the integer vector specifying the number of neurons
  in each layer including the input and output layers of the MLP.
cv::CvANN_MLP::get_weights: Returns neurons weights of the particular layer.
cv::nonfree. Non-free functionality: |-
  The module contains algorithms that may be patented in some countries or
  have some other limitations on the use.
cv::Normal Bayes Classifier: |-
  This simple classification model assumes that feature vectors from each
  class are normally distributed (though, not necessarily independently
  distributed). So, the whole data distribution function is assumed to be
  a Gaussian mixture, one component per class. Using the training data the
  algorithm estimates mean vectors and covariance matrices for every
  class, and then it uses them for prediction.
cv::CvNormalBayesClassifier: Bayes classifier for normally distributed data.
cv::CvNormalBayesClassifier::CvNormalBayesClassifier: "Default and training constructors.\n\n\
  The constructors follow conventions of :ocvCvStatModel::CvStatModel. See\n\
  :ocvCvStatModel::train for parameters descriptions."
cv::CvNormalBayesClassifier::train: |
  Trains the model.
  
  The method trains the Normal Bayes classifier. It follows the
  conventions of the generic :ocvCvStatModel::train approach with the
  following limitations:
    Only `CV_ROW_SAMPLE` data layout is supported.
  
    Input variables are all ordered.
  
    Output variable is categorical , which means that elements of
  `responses` must be integer numbers, though the vector may have the
  `CV_32FC1` type.
  
    Missing measurements are not supported.

cv::CvNormalBayesClassifier::predict: |-
  Predicts the response for sample(s).
  
  The method estimates the most probable classes for input vectors. Input
  vectors (one or more) are stored as rows of the matrix `samples`. In
  case of multiple input vectors, there should be one output vector
  `results`. The predicted class for a single input vector is returned by
  the method.
  
  The function is parallelized with the TBB library.
cv::Object Categorization: |-
  This section describes approaches based on local 2D features and used to
  categorize objects.
cv::BOWTrainer: |
  Abstract base class for training the *bag of visual words* vocabulary
  from a set of descriptors. For details, see, for example, *Visual
  Categorization with Bags of Keypoints* by Gabriella Csurka, Christopher
  R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :
  
  ```c++
  class BOWTrainer
  {
  public:
      BOWTrainer(){}
      virtual ~BOWTrainer(){}
  
      void add( const Mat& descriptors );
      const vector<Mat>& getDescriptors() const;
      int descripotorsCount() const;
  
      virtual void clear();
  
      virtual Mat cluster() const = 0;
      virtual Mat cluster( const Mat& descriptors ) const = 0;
  
  protected:
      ...
  };
  ```

cv::BOWTrainer::add: |-
  Adds descriptors to a training set.
  
  The training set is clustered using `clustermethod` to construct the
  vocabulary.
cv::BOWTrainer::getDescriptors: Returns a training set of descriptors.
cv::BOWTrainer::descripotorsCount: Returns the count of all descriptors stored in the training set.
cv::BOWTrainer::cluster: |-
  Clusters train descriptors.
  
  The vocabulary consists of cluster centers. So, this method returns the
  vocabulary. In the first variant of the method, train descriptors stored
  in the object are clustered. In the second variant, input descriptors
  are clustered.
cv::BOWKMeansTrainer: |
  :ocvkmeans -based class to train visual vocabulary using the *bag of
  visual words* approach. :
  
  ```c++
  class BOWKMeansTrainer : public BOWTrainer
  {
  public:
      BOWKMeansTrainer( int clusterCount, const TermCriteria& termcrit=TermCriteria(),
                        int attempts=3, int flags=KMEANS_PP_CENTERS );
      virtual ~BOWKMeansTrainer(){}
  
      // Returns trained vocabulary (i.e. cluster centers).
      virtual Mat cluster() const;
      virtual Mat cluster( const Mat& descriptors ) const;
  
  protected:
      ...
  };
  ```

cv::BOWKMeansTrainer::BOWKMeansTrainer: The constructor.
cv::BOWImgDescriptorExtractor: |
  Class to compute an image descriptor using the *bag of visual words*.
  Such a computation consists of the following steps:
   Compute descriptors for a given image and its keypoints set.
  
   Find the nearest visual words from the vocabulary for each
  keypoint descriptor.
  
   Compute the bag-of-words image descriptor as is a normalized
  histogram of vocabulary words encountered in the image. The `i`-th
  bin of the histogram is a frequency of `i`-th word of the
  vocabulary in the given image.
  
  
  
  The class declaration is the following: :
  
  ```c++
  class BOWImgDescriptorExtractor
  {
  public:
      BOWImgDescriptorExtractor( const Ptr<DescriptorExtractor>& dextractor,
                                 const Ptr<DescriptorMatcher>& dmatcher );
      virtual ~BOWImgDescriptorExtractor(){}
  
      void setVocabulary( const Mat& vocabulary );
      const Mat& getVocabulary() const;
      void compute( const Mat& image, vector<KeyPoint>& keypoints,
                    Mat& imgDescriptor,
                    vector<vector<int> >* pointIdxsOfClusters=0,
                    Mat* descriptors=0 );
      int descriptorSize() const;
      int descriptorType() const;
  
  protected:
      ...
  };
  ```

cv::BOWImgDescriptorExtractor::BOWImgDescriptorExtractor: The constructor.
cv::BOWImgDescriptorExtractor::setVocabulary: Sets a visual vocabulary.
cv::BOWImgDescriptorExtractor::getVocabulary: Returns the set vocabulary.
cv::BOWImgDescriptorExtractor::compute: Computes an image descriptor using the set visual vocabulary.
cv::BOWImgDescriptorExtractor::descriptorSize: |-
  Returns an image descriptor size if the vocabulary is set. Otherwise, it
  returns 0.
cv::BOWImgDescriptorExtractor::descriptorType: Returns an image descriptor type.
cv::ocl::OclCascadeClassifier: |
  Cascade classifier class used for object detection. Supports HAAR
  cascade classifier in the form of cross link :
  
  ```c++
  class CV_EXPORTS OclCascadeClassifier : public CascadeClassifier
  {
  public:
        OclCascadeClassifier() {};
        ~OclCascadeClassifier() {};
         CvSeq *oclHaarDetectObjects(oclMat &gimg, CvMemStorage *storage,
                                    double scaleFactor,int minNeighbors,
                                    int flags, CvSize minSize = cvSize(0, 0),
                                    CvSize maxSize = cvSize(0, 0));
  };
  ```

cv::ocl::OclCascadeClassifier::oclHaarDetectObjects: |-
  Returns the detected objects by a list of rectangles
  
  Detects objects of different sizes in the input image,only tested for
  face detection now. The function returns the number of detected objects.
cv::ocl::MatchTemplateBuf: |-
  Class providing memory buffers for :ocvocl::matchTemplate function, plus
  it allows to adjust some specific parameters. :
  
  ```c++
  struct CV_EXPORTS MatchTemplateBuf
  {
      Size user_block_size;
      oclMat imagef, templf;
      std::vector<oclMat> images;
      std::vector<oclMat> image_sums;
      std::vector<oclMat> image_sqsums;
  };
  ```
  
  
  You can use field user_block_size to set specific block size for
  :ocvocl::matchTemplate function. If you leave its default value
  Size(0,0) then automatic estimation of block size will be used (which is
  optimized for speed). By varying user_block_size you can reduce memory
  requirements at the cost of speed.
cv::ocl::matchTemplate: |-
  Computes a proximity map for a raster template and an image where the
  template is searched for.
cv::Basic C Structures and Operations: |-
  The section describes the main data structures, used by the OpenCV 1.x
  API, and the basic functions to create and process the data structures.
cv::CvMat: |-
  Matrix elements are stored row by row. Element (i, j) (i - 0-based row
  index, j - 0-based column index) of a matrix can be retrieved or
  modified using `CV_MAT_ELEM` macro: :
  
  ```c++
  uchar pixval = CV_MAT_ELEM(grayimg, uchar, i, j)
  CV_MAT_ELEM(cameraMatrix, float, 0, 2) = image.width*0.5f;
  ```
  
  
  To access multiple-channel matrices, you can use
  `CV_MAT_ELEM(matrix, type, i, j*nchannels + channel_idx)`.
  
  `CvMat` is now obsolete; consider using :ocvMat instead.
cv::CvMatND: `CvMatND` is now obsolete; consider using :ocvMat instead.
cv::IplImage: |-
  The `IplImage` is taken from the Intel Image Processing Library, in
  which the format is native. OpenCV only supports a subset of possible
  `IplImage` formats, as outlined in the parameter list above.
  
  In addition to the above restrictions, OpenCV handles ROIs differently.
  OpenCV functions require that the image size or ROI size of all source
  and destination images match exactly. On the other hand, the Intel Image
  Processing Library processes the area of intersection between the source
  and destination images (or ROIs), allowing them to vary independently.
cv::CvArr: |-
  This is the "metatype" used *only* as a function parameter. It denotes
  that the function accepts arrays of multiple types, such as IplImage*,
  CvMat* or even CvSeq* sometimes. The particular array type is
  determined at runtime by analyzing the first 4 bytes of the header. In
  C++ interface the role of `CvArr` is played by `InputArray` and
  `OutputArray`.
cv::ClearND: |-
  Clears a specific array element.
  
  The function clears (sets to zero) a specific element of a dense array
  or deletes the element of a sparse array. If the sparse array element
  does not exists, the function does nothing.
cv::CloneImage: Makes a full copy of an image, including the header, data, and ROI.
cv::CloneMat: |-
  Creates a full matrix copy.
  
  Creates a full copy of a matrix and returns a pointer to the copy. Note
  that the matrix copy is compacted, that is, it will not have gaps
  between rows.
cv::CloneMatND: |-
  Creates full copy of a multi-dimensional array and returns a pointer to
  the copy.
cv::CloneSparseMat: |-
  Creates full copy of sparse array.
  
  The function creates a copy of the input array and returns pointer to
  the copy.
cv::ConvertScale: |-
  Converts one array to another with optional linear transformation.
  
  The function has several different purposes, and thus has several
  different names. It copies one array to another with optional scaling,
  which is performed first, and/or optional type conversion, performed
  after:
  
  $$\texttt{dst} (I) =  \texttt{scale} \texttt{src} (I) + ( \texttt{shift} _0, \texttt{shift} _1,...)$$
  
  All the channels of multi-channel arrays are processed independently.
  
  The type of conversion is done with rounding and saturation, that is if
  the result of scaling + conversion can not be represented exactly by a
  value of the destination array element type, it is set to the nearest
  representable value on the real axis.
cv::Copy: |-
  Copies one array to another.
  
  The function copies selected elements from an input array to an output
  array:
  
  $$\texttt{dst} (I)= \texttt{src} (I)  \quad \text{if} \quad \texttt{mask} (I)  \ne 0.$$
  
  If any of the passed arrays is of `IplImage` type, then its ROI and COI
  fields are used. Both arrays must have the same type, the same number of
  dimensions, and the same size. The function can also copy sparse arrays
  (mask is not supported in this case).
cv::CreateData: |-
  Allocates array data
  
  The function allocates image, matrix or multi-dimensional dense array
  data. Note that in the case of matrix types OpenCV allocation functions
  are used. In the case of IplImage they are used unless
  `CV_TURN_ON_IPL_COMPATIBILITY()` has been called before. In the latter
  case IPL functions are used to allocate the data.
cv::CreateImage: |
  Creates an image header and allocates the image data.
  
  This function call is equivalent to the following code: :
  
  ```c++
  header = cvCreateImageHeader(size, depth, channels);
  cvCreateData(header);
  ```

cv::CreateImageHeader: Creates an image header but does not allocate the image data.
cv::CreateMat: |
  Creates a matrix header and allocates the matrix data.
  
  The function call is equivalent to the following code: :
  
  ```c++
  CvMat* mat = cvCreateMatHeader(rows, cols, type);
  cvCreateData(mat);
  ```

cv::CreateMatHeader: |-
  Creates a matrix header but does not allocate the matrix data.
  
  The function allocates a new matrix header and returns a pointer to it.
  The matrix data can then be allocated using :ocvCreateData or set
  explicitly to user-allocated data via :ocvSetData.
cv::CreateMatND: |
  Creates the header and allocates the data for a multi-dimensional dense
  array.
  
  This function call is equivalent to the following code: :
  
  ```c++
  CvMatND* mat = cvCreateMatNDHeader(dims, sizes, type);
  cvCreateData(mat);
  ```

cv::CreateMatNDHeader: |-
  Creates a new matrix header but does not allocate the matrix data.
  
  The function allocates a header for a multi-dimensional dense array. The
  array data can further be allocated using :ocvCreateData or set
  explicitly to user-allocated data via :ocvSetData.
cv::CreateSparseMat: |-
  Creates sparse array.
  
  The function allocates a multi-dimensional sparse array. Initially the
  array contain no elements, that is :ocvPtrND and other related functions
  will return 0 for every index.
cv::CrossProduct: |-
  Calculates the cross product of two 3D vectors.
  
  The function calculates the cross product of two 3D vectors:
  
  $$\texttt{dst} =  \texttt{src1} \times \texttt{src2}$$
  
  or:
  
  $$\begin{array}{l} \texttt{dst} _1 =  \texttt{src1} _2  \texttt{src2} _3 -  \texttt{src1} _3  \texttt{src2} _2 \ \texttt{dst} _2 =  \texttt{src1} _3  \texttt{src2} _1 -  \texttt{src1} _1  \texttt{src2} _3 \ \texttt{dst} _3 =  \texttt{src1} _1  \texttt{src2} _2 -  \texttt{src1} _2  \texttt{src2} _1 \end{array}$$
cv::DotProduct: |-
  Calculates the dot product of two arrays in Euclidean metrics.
  
  The function calculates and returns the Euclidean dot product of two
  arrays.
  
  $$src1  \bullet src2 =  \sum _I ( \texttt{src1} (I)  \texttt{src2} (I))$$
  
  In the case of multiple channel arrays, the results for all channels are
  accumulated. In particular, `cvDotProduct(a,a)` where `a` is a complex
  vector, will return $||\texttt{a}||^2$. The function can process
  multi-dimensional arrays, row by row, layer by layer, and so on.
cv::Get?D: |-
  The functions return a specific array element. In the case of a sparse
  array the functions return 0 if the requested node does not exist (no
  new node is created by the functions).
cv::GetCol(s): |-
  Returns one of more array columns.
  
  The functions return the header, corresponding to a specified column
  span of the input array. That is, no data is copied. Therefore, any
  modifications of the submatrix will affect the original array. If you
  need to copy the columns, use :ocvCloneMat. `cvGetCol(arr, submat, col)`
  is a shortcut for `cvGetCols(arr, submat, col, col+1)`.
cv::GetDiag: |-
  Returns one of array diagonals.
  
  The function returns the header, corresponding to a specified diagonal
  of the input array.
cv::GetDims: |
  Return number of array dimensions
  
  The function returns the array dimensionality and the array of dimension
  sizes. In the case of `IplImage` or CvMat it always returns 2 regardless
  of number of image/matrix rows. For example, the following code
  calculates total number of array elements: :
  
  ```c++
  int sizes[CV_MAX_DIM];
  int i, total = 1;
  int dims = cvGetDims(arr, size);
  for(i = 0; i < dims; i++ )
      total *= sizes[i];
  ```

cv::GetDimSize: Returns array size along the specified dimension.
cv::GetElemType: |-
  Returns type of array elements.
  
  The function returns type of the array elements. In the case of
  `IplImage` the type is converted to `CvMat`-like representation. For
  example, if the image has been created as: :
  
  ```c++
  IplImage* img = cvCreateImage(cvSize(640, 480), IPL_DEPTH_8U, 3);
  ```
  
  
  The code `cvGetElemType(img)` will return `CV_8UC3`.
cv::GetImage: |-
  Returns image header for arbitrary array.
  
  The function returns the image header for the input array that can be a
  matrix (:ocvCvMat) or image (:ocvIplImage). In the case of an image the
  function simply returns the input pointer. In the case of `CvMat` it
  initializes an `image_header` structure with the parameters of the input
  matrix. Note that if we transform `IplImage` to `CvMat` using :ocvGetMat
  and then transform `CvMat` back to IplImage using this function, we will
  get different headers if the ROI is set in the original image.
cv::GetImageCOI: |-
  Returns the index of the channel of interest.
  
  Returns the channel of interest of in an IplImage. Returned values
  correspond to the `coi` in :ocvSetImageCOI.
cv::GetImageROI: |-
  Returns the image ROI.
  
  If there is no ROI set, `cvRect(0,0,image->width,image->height)` is
  returned.
cv::GetMat: |
  Returns matrix header for arbitrary array.
  
  The function returns a matrix header for the input array that can be a
  matrix - :ocvCvMat, an image - :ocvIplImage, or a multi-dimensional
  dense array - :ocvCvMatND (the third option is allowed only if
  `allowND != 0`) . In the case of matrix the function simply returns the
  input pointer. In the case of `IplImage*` or `CvMatND` it initializes
  the `header` structure with parameters of the current image ROI and
  returns `&header`. Because COI is not supported by `CvMat`, it is
  returned separately.
  
  The function provides an easy way to handle both types of arrays -
  `IplImage` and `CvMat` using the same code. Input array must have
  non-zero data pointer, otherwise the function will report an error.
  
  **note**
  
  If the input array is `IplImage` with planar data layout and COI set,
  the function returns the pointer to the selected plane and `COI == 0`.
  This feature allows user to process `IplImage` structures with planar
  data layout, even though OpenCV does not support such images.

cv::GetNextSparseNode: |
  Returns the next sparse matrix element
  
  The function moves iterator to the next sparse matrix element and
  returns pointer to it. In the current version there is no any particular
  order of the elements, because they are stored in the hash table. The
  sample below demonstrates how to iterate through the sparse matrix: :
  
  ```c++
  // print all the non-zero sparse matrix elements and compute their sum
  double sum = 0;
  int i, dims = cvGetDims(sparsemat);
  CvSparseMatIterator it;
  CvSparseNode* node = cvInitSparseMatIterator(sparsemat, &it);
  
  for(; node != 0; node = cvGetNextSparseNode(&it))
  {
      /* get pointer to the element indices */
      int* idx = CV_NODE_IDX(array, node);
      /* get value of the element (assume that the type is CV_32FC1) */
      float val = *(float*)CV_NODE_VAL(array, node);
      printf("M");
      for(i = 0; i < dims; i++ )
          printf("[%d]", idx[i]);
      printf("=%g\n", val);
  
      sum += val;
  }
  
  printf("nTotal sum = %g\n", sum);
  ```

cv::GetRawData: |
  Retrieves low-level information about the array.
  
  The function fills output variables with low-level information about the
  array data. All output parameters are optional, so some of the pointers
  may be set to `NULL`. If the array is `IplImage` with ROI set, the
  parameters of ROI are returned.
  
  The following example shows how to get access to array elements. It
  computes absolute values of the array elements :
  
  ```c++
  float* data;
  int step;
  CvSize size;
  
  cvGetRawData(array, (uchar**)&data, &step, &size);
  step /= sizeof(data[0]);
  
  for(int y = 0; y < size.height; y++, data += step )
      for(int x = 0; x < size.width; x++ )
          data[x] = (float)fabs(data[x]);
  ```

cv::GetReal?D: |-
  Return a specific element of single-channel 1D, 2D, 3D or nD array.
  
  Returns a specific element of a single-channel array. If the array has
  multiple channels, a runtime error is raised. Note that `Get?D`
  functions can be used safely for both single-channel and
  multiple-channel arrays though they are a bit slower.
  
  In the case of a sparse array the functions return 0 if the requested
  node does not exist (no new node is created by the functions).
cv::GetRow(s): |-
  Returns array row or row span.
  
  The functions return the header, corresponding to a specified row/row
  span of the input array. `cvGetRow(arr, submat, row)` is a shortcut for
  `cvGetRows(arr, submat, row, row+1)`.
cv::GetSize: |-
  Returns size of matrix or image ROI.
  
  The function returns number of rows (CvSize::height) and number of
  columns (CvSize::width) of the input matrix or image. In the case of
  image the size of ROI is returned.
cv::GetSubRect: |-
  Returns matrix header corresponding to the rectangular sub-array of
  input image or matrix.
  
  The function returns header, corresponding to a specified rectangle of
  the input array. In other words, it allows the user to treat a
  rectangular part of input array as a stand-alone array. ROI is taken
  into account by the function so the sub-array of ROI is actually
  extracted.
cv::DecRefData: "Decrements an array data reference counter.\n\n\
  The function decrements the data reference counter in a :ocvCvMat or\n\
  :ocvCvMatND if the reference counter pointer is not NULL. If the counter\n\
  reaches zero, the data is deallocated. In the current implementation the\n\
  reference counter is not NULL only if the data was allocated using the\n\
  :ocvCreateData function. The counter will be NULL in other cases such\n\
  as: external data was assigned to the header using :ocvSetData, header\n\
  is part of a larger matrix or image, or the header was converted from an\n\
  image or n-dimensional matrix header."
cv::IncRefData: |-
  Increments array data reference counter.
  
  The function increments :ocvCvMat or :ocvCvMatND data reference counter
  and returns the new counter value if the reference counter pointer is
  not NULL, otherwise it returns zero.
cv::InitImageHeader: |-
  Initializes an image header that was previously allocated.
  
  The returned `IplImage*` points to the initialized header.
cv::InitMatHeader: |
  Initializes a pre-allocated matrix header.
  
  This function is often used to process raw data with OpenCV matrix
  functions. For example, the following code computes the matrix product
  of two matrices, stored as ordinary arrays: :
  
  ```c++
  double a[] = { 1, 2, 3, 4,
                 5, 6, 7, 8,
                 9, 10, 11, 12 };
  
  double b[] = { 1, 5, 9,
                 2, 6, 10,
                 3, 7, 11,
                 4, 8, 12 };
  
  double c[9];
  CvMat Ma, Mb, Mc ;
  
  cvInitMatHeader(&Ma, 3, 4, CV_64FC1, a);
  cvInitMatHeader(&Mb, 4, 3, CV_64FC1, b);
  cvInitMatHeader(&Mc, 3, 3, CV_64FC1, c);
  
  cvMatMulAdd(&Ma, &Mb, 0, &Mc);
  // the c array now contains the product of a (3x4) and b (4x3)
  ```

cv::InitMatNDHeader: Initializes a pre-allocated multi-dimensional array header.
cv::InitSparseMatIterator: |-
  Initializes sparse array elements iterator.
  
  The function initializes iterator of sparse array elements and returns
  pointer to the first element, or NULL if the array is empty.
cv::Ptr?D: "Return pointer to a particular array element.\n\n\
  The functions return a pointer to a specific array element. Number of\n\
  array dimension should match to the number of indices passed to the\n\
  function except for `cvPtr1D` function that can be used for sequential\n\
  access to 1D, 2D or nD dense arrays.\n\n\
  The functions can be used for sparse arrays as well - if the requested\n\
  node does not exist they create it and set it to zero.\n\n\
  All these as well as other functions accessing array elements (\n\
  :ocvGetND , :ocvGetRealND , :ocvSet , :ocvSetND , :ocvSetRealND ) raise\n\
  an error in case if the element index is out of range."
cv::ReleaseData: "Releases array data.\n\n\
  The function releases the array data. In the case of :ocvCvMat or\n\
  :ocvCvMatND it simply calls cvDecRefData(), that is the function can not\n\
  deallocate external data. See also the note to :ocvCreateData ."
cv::ReleaseImage: |
  Deallocates the image header and the image data.
  
  This call is a shortened form of :
  
  ```c++
  if(*image )
  {
      cvReleaseData(*image);
      cvReleaseImageHeader(image);
  }
  ```

cv::ReleaseImageHeader: |-
  Deallocates an image header.
  
  This call is an analogue of :
  
  ```c++
  if(image )
  {
      iplDeallocate(*image, IPL_IMAGE_HEADER | IPL_IMAGE_ROI);
      *image = 0;
  }
  ```
  
  
  but it does not use IPL functions by default (see the
  `CV_TURN_ON_IPL_COMPATIBILITY` macro).
cv::ReleaseMat: |
  Deallocates a matrix.
  
  The function decrements the matrix data reference counter and
  deallocates matrix header. If the data reference counter is 0, it also
  deallocates the data. :
  
  ```c++
  if(*mat )
      cvDecRefData(*mat);
  cvFree((void**)mat);
  ```

cv::ReleaseMatND: |
  Deallocates a multi-dimensional array.
  
  The function decrements the array data reference counter and releases
  the array header. If the reference counter reaches 0, it also
  deallocates the data. :
  
  ```c++
  if(*mat )
      cvDecRefData(*mat);
  cvFree((void**)mat);
  ```

cv::ReleaseSparseMat: |-
  Deallocates sparse array.
  
  The function releases the sparse array and clears the array pointer upon
  exit.
cv::ResetImageROI: |
  Resets the image ROI to include the entire image and releases the ROI
  structure.
  
  This produces a similar result to the following, but in addition it
  releases the ROI structure. :
  
  ```c++
  cvSetImageROI(image, cvRect(0, 0, image->width, image->height ));
  cvSetImageCOI(image, 0);
  ```

cv::Reshape: |
  Changes shape of matrix/image without copying data.
  
  The function initializes the CvMat header so that it points to the same
  data as the original array but has a different shape - different number
  of channels, different number of rows, or both.
  
  The following example code creates one image buffer and two image
  headers, the first is for a 320x240x3 image and the second is for a
  960x240x1 image: :
  
  ```c++
  IplImage* color_img = cvCreateImage(cvSize(320,240), IPL_DEPTH_8U, 3);
  CvMat gray_mat_hdr;
  IplImage gray_img_hdr, *gray_img;
  cvReshape(color_img, &gray_mat_hdr, 1);
  gray_img = cvGetImage(&gray_mat_hdr, &gray_img_hdr);
  ```
  
  
  And the next example converts a 3x3 matrix to a single 1x9 vector:
  
  ```c++
  CvMat* mat = cvCreateMat(3, 3, CV_32F);
  CvMat row_header, *row;
  row = cvReshape(mat, &row_header, 0, 1);
  ```

cv::ReshapeMatND: |
  Changes the shape of a multi-dimensional array without copying the data.
  
  The function is an advanced version of :ocvReshape that can work with
  multi-dimensional arrays as well (though it can work with ordinary
  images and matrices) and change the number of dimensions.
  
  Below are the two samples from the :ocvReshape description rewritten
  using :ocvReshapeMatND : :
  
  ```c++
  IplImage* color_img = cvCreateImage(cvSize(320,240), IPL_DEPTH_8U, 3);
  IplImage gray_img_hdr, *gray_img;
  gray_img = (IplImage*)cvReshapeND(color_img, &gray_img_hdr, 1, 0, 0);
  
  ...
  
  /* second example is modified to convert 2x2x2 array to 8x1 vector */
  int size[] = { 2, 2, 2 };
  CvMatND* mat = cvCreateMatND(3, size, CV_32F);
  CvMat row_header, *row;
  row = (CvMat*)cvReshapeND(mat, &row_header, 0, 1, 0);
  ```

cv::Set: |-
  Sets every element of an array to a given value.
  
  The function copies the scalar `value` to every selected element of the
  destination array:
  
  $$\texttt{arr} (I)= \texttt{value} \quad \text{if} \quad \texttt{mask} (I)  \ne 0$$
  
  If array `arr` is of `IplImage` type, then is ROI used, but COI must not
  be set.
cv::Set?D: |-
  Change the particular array element.
  
  The functions assign the new value to a particular array element. In the
  case of a sparse array the functions create the node if it does not
  exist yet.
cv::SetData: "Assigns user data to the array header.\n\n\
  The function assigns user data to the array header. Header should be\n\
  initialized before using :ocvcvCreateMatHeader, :ocvcvCreateImageHeader,\n\
  :ocvcvCreateMatNDHeader, :ocvcvInitMatHeader, :ocvcvInitImageHeader or\n\
  :ocvcvInitMatNDHeader."
cv::SetImageCOI: "Sets the channel of interest in an IplImage.\n\n\
  If the ROI is set to `NULL` and the coi is *not* 0, the ROI is\n\
  allocated. Most OpenCV functions do *not* support the COI setting, so to\n\
  process an individual image/matrix channel one may copy (via :ocvCopy or\n\
  :ocvSplit) the channel to a separate image/matrix, process it and then\n\
  copy the result back (via :ocvCopy or :ocvMerge) if needed."
cv::SetImageROI: |-
  Sets an image Region Of Interest (ROI) for a given rectangle.
  
  If the original image ROI was `NULL` and the `rect` is not the whole
  image, the ROI structure is allocated.
  
  Most OpenCV functions support the use of ROI and treat the image
  rectangle as a separate image. For example, all of the pixel coordinates
  are counted from the top-left (or bottom-left) corner of the ROI, not
  the original image.
cv::SetReal?D: |-
  Change a specific array element.
  
  The functions assign a new value to a specific element of a
  single-channel array. If the array has multiple channels, a runtime
  error is raised. Note that the `Set*D` function can be used safely for
  both single-channel and multiple-channel arrays, though they are a bit
  slower.
  
  In the case of a sparse array the functions create the node if it does
  not yet exist.
cv::SetZero: |-
  Clears the array.
  
  The function clears the array. In the case of dense arrays (CvMat,
  CvMatND or IplImage), cvZero(array) is equivalent to
  cvSet(array,cvScalarAll(0),0). In the case of sparse arrays all the
  elements are removed.
cv::mGet: |-
  Returns the particular element of single-channel floating-point matrix.
  
  The function is a fast replacement for :ocvGetReal2D in the case of
  single-channel floating-point matrices. It is faster because it is
  inline, it does fewer checks for array type and array element type, and
  it checks for the row and column ranges only in debug mode.
cv::mSet: |-
  Sets a specific element of a single-channel floating-point matrix.
  
  The function is a fast replacement for :ocvSetReal2D in the case of
  single-channel floating-point matrices. It is faster because it is
  inline, it does fewer checks for array type and array element type, and
  it checks for the row and column ranges only in debug mode.
cv::SetIPLAllocators: |
  Makes OpenCV use IPL functions for allocating IplImage and IplROI
  structures.
  
  Normally, the function is not called directly. Instead, a simple macro
  `CV_TURN_ON_IPL_COMPATIBILITY()` is used that calls `cvSetIPLAllocators`
  and passes there pointers to IPL allocation functions. :
  
  ```c++
  ...
  CV_TURN_ON_IPL_COMPATIBILITY()
  ...
  ```

cv::RNG: "Initializes a random number generator state.\n\n\
  The function initializes a random number generator and returns the\n\
  state. The pointer to the state can be then passed to the :ocvRandInt,\n\
  :ocvRandReal and :ocvRandArr functions. In the current implementation a\n\
  multiply-with-carry generator is used.\n\n\
  Random number generator. It encapsulates the state (currently, a 64-bit\n\
  integer) and has methods to return scalar random values and to fill\n\
  arrays with random values. Currently it supports uniform and Gaussian\n\
  (normal) distributions. The generator uses Multiply-With-Carry\n\
  algorithm, introduced by G. Marsaglia (\n\
  <http://en.wikipedia.org/wiki/Multiply-with-carry> ).\n\
  Gaussian-distribution random numbers are generated using the Ziggurat\n\
  algorithm ( <http://en.wikipedia.org/wiki/Ziggurat_algorithm> ),\n\
  introduced by G. Marsaglia and W. W. Tsang."
cv::RandArr: |-
  Fills an array with random numbers and updates the RNG state.
  
  The function fills the destination array with uniformly or normally
  distributed random numbers.
cv::RandInt: |-
  Returns a 32-bit unsigned integer and updates RNG.
  
  The function returns a uniformly-distributed random 32-bit unsigned
  integer and updates the RNG state. It is similar to the rand() function
  from the C runtime library, except that OpenCV functions always
  generates a 32-bit random number, regardless of the platform.
cv::RandReal: |-
  Returns a floating-point random number and updates RNG.
  
  The function returns a uniformly-distributed random floating-point
  number between 0 and 1 (1 is not included).
cv::fromarray: |
  Create a CvMat from an object that supports the array interface.
  
  param object
  :   Any object that supports the array interface
  
  param allowND
  :   If true, will return a CvMatND
  
  If the object supports the [array
  interface](http://docs.scipy.org/doc/numpy/reference/arrays.interface.html)
  , return a :ocvCvMat or :ocvCvMatND, depending on `allowND` flag:
  
    If `allowND = False`, then the object's array must be either 2D or
  3D. If it is 2D, then the returned CvMat has a single channel. If
  it is 3D, then the returned CvMat will have N channels, where N is
  the last dimension of the array. In this case, N cannot be greater
  than OpenCV's channel limit, `CV_CN_MAX`.
  
  
    If`allowND = True`, then `fromarray` returns a single-channel
  :ocvCvMatND with the same shape as the original array.
  
  
  For example, [NumPy](http://numpy.scipy.org/) arrays support the array
  interface, so can be converted to OpenCV objects:
  
  
  ~~~~ {.sourceCode .python}
  
  import cv2.cv as cv, numpy
  a = numpy.ones((480, 640))
  mat = cv.fromarray(a)
  print cv.GetDims(mat), cv.CV_MAT_CN(cv.GetElemType(mat))
  (480, 640) 1
  a = numpy.ones((480, 640, 3))
  mat = cv.fromarray(a)
  print cv.GetDims(mat), cv.CV_MAT_CN(cv.GetElemType(mat))
  (480, 640) 3
  a = numpy.ones((480, 640, 3))
  mat = cv.fromarray(a, allowND = True)
  print cv.GetDims(mat), cv.CV_MAT_CN(cv.GetElemType(mat))
  (480, 640, 3) 1
  ~~~~
  
  
  
  **note**
  
  In the new Python wrappers (**cv2** module) the function is not
  needed, since cv2 can process Numpy arrays (and this is the only
  supported array type).

cv::XML/YAML Persistence (C API): |-
  The section describes the OpenCV 1.x API for reading and writing data
  structures to/from XML or YAML files. It is now recommended to use the
  new C++ interface for reading and writing data.
cv::CvFileStorage: |-
  The structure `CvFileStorage` is a "black box" representation of the
  file storage associated with a file on disk. Several functions that are
  described below take `CvFileStorage*` as inputs and allow the user to
  save or to load hierarchical collections that consist of scalar values,
  standard CXCore objects (such as matrices, sequences, graphs), and
  user-defined objects.
  
  OpenCV can read and write data in XML (<http://www.w3c.org/XML>) or YAML
  (<http://www.yaml.org>) formats. Below is an example of 3x3
  floating-point identity matrix `A`, stored in XML and YAML files using
  CXCore functions:
  
  XML: :
  
  ```c++
  <?xml version="1.0">
  <opencv_storage>
  <A type_id="opencv-matrix">
    <rows>3</rows>
    <cols>3</cols>
    <dt>f</dt>
    <data>1. 0. 0. 0. 1. 0. 0. 0. 1.</data>
  </A>
  </opencv_storage>
  ```
  
  
  YAML: :
  
  ```c++
  %YAML:1.0
  A: !!opencv-matrix
    rows: 3
    cols: 3
    dt: f
    data: [ 1., 0., 0., 0., 1., 0., 0., 0., 1.]
  ```
  
  
  As it can be seen from the examples, XML uses nested tags to represent
  hierarchy, while YAML uses indentation for that purpose (similar to the
  Python programming language).
  
  The same functions can read and write data in both formats; the
  particular format is determined by the extension of the opened file,
  ".xml" for XML files and ".yml" or ".yaml" for YAML.
cv::CvAttrList: |-
  List of attributes. :
  
  ```c++
  typedef struct CvAttrList
  {
      const char** attr; /* NULL-terminated array of (attribute_name,attribute_value) pairs */
      struct CvAttrList* next; /* pointer to next chunk of the attributes list */
  }
  CvAttrList;
  
  /* initializes CvAttrList structure */
  inline CvAttrList cvAttrList( const char** attr=NULL, CvAttrList* next=NULL );
  
  /* returns attribute value or 0 (NULL) if there is no such attribute */
  const char* cvAttrValue( const CvAttrList* attr, const char* attr_name );
  ```
  
  
  In the current implementation, attributes are used to pass extra
  parameters when writing user objects (see :ocvWrite). XML attributes
  inside tags are not supported, aside from the object type specification
  (`type_id` attribute).
cv::CvTypeInfo: |-
  Type information. :
  
  ```c++
  typedef int (CV_CDECL *CvIsInstanceFunc)( const void* structPtr );
  typedef void (CV_CDECL *CvReleaseFunc)( void** structDblPtr );
  typedef void* (CV_CDECL *CvReadFunc)( CvFileStorage* storage, CvFileNode* node );
  typedef void (CV_CDECL *CvWriteFunc)( CvFileStorage* storage,
                                        const char* name,
                                        const void* structPtr,
                                        CvAttrList attributes );
  typedef void* (CV_CDECL *CvCloneFunc)( const void* structPtr );
  
  typedef struct CvTypeInfo
  {
      int flags; /* not used */
      int header_size; /* sizeof(CvTypeInfo) */
      struct CvTypeInfo* prev; /* previous registered type in the list */
      struct CvTypeInfo* next; /* next registered type in the list */
      const char* type_name; /* type name, written to file storage */
  
      /* methods */
      CvIsInstanceFunc is_instance; /* checks if the passed object belongs to the type */
      CvReleaseFunc release; /* releases object (memory etc.) */
      CvReadFunc read; /* reads object from file storage */
      CvWriteFunc write; /* writes object to file storage */
      CvCloneFunc clone; /* creates a copy of the object */
  }
  CvTypeInfo;
  ```
  
  
  The structure contains information about one of the standard or
  user-defined types. Instances of the type may or may not contain a
  pointer to the corresponding :ocvCvTypeInfo structure. In any case,
  there is a way to find the type info structure for a given object using
  the :ocvTypeOf function. Alternatively, type info can be found by type
  name using :ocvFindType, which is used when an object is read from file
  storage. The user can register a new type with :ocvRegisterType that
  adds the type information structure into the beginning of the type list.
  Thus, it is possible to create specialized types from generic standard
  types and override the basic methods.
cv::Clone: |-
  Makes a clone of an object.
  
  The function finds the type of a given object and calls `clone` with the
  passed object. Of course, if you know the object type, for example,
  `struct_ptr` is `CvMat*`, it is faster to call the specific function,
  like :ocvCloneMat.
cv::EndWriteStruct: Finishes writing to a file node collection.
cv::FindType: |-
  Finds a type by its name.
  
  The function finds a registered type by its name. It returns NULL if
  there is no type with the specified name.
cv::FirstType: |-
  Returns the beginning of a type list.
  
  The function returns the first type in the list of registered types.
  Navigation through the list can be done via the `prev` and `next` fields
  of the :ocvCvTypeInfo structure.
cv::GetFileNode: "Finds a node in a map or file storage.\n\n\
  The function finds a file node. It is a faster version of\n\
  :ocvGetFileNodeByName (see :ocvGetHashedKey discussion). Also, the\n\
  function can insert a new node, if it is not in the map yet."
cv::GetFileNodeByName: "Finds a node in a map or file storage.\n\n\
  The function finds a file node by `name`. The node is searched either in\n\
  `map` or, if the pointer is NULL, among the top-level file storage\n\
  nodes. Using this function for maps and :ocvGetSeqElem (or sequence\n\
  reader) for sequences, it is possible to navigate through the file\n\
  storage. To speed up multiple queries for a certain key (e.g., in the\n\
  case of an array of structures) one may use a combination of\n\
  :ocvGetHashedKey and :ocvGetFileNode."
cv::GetFileNodeName: |-
  Returns the name of a file node.
  
  The function returns the name of a file node or NULL, if the file node
  does not have a name or if `node` is `NULL`.
cv::GetHashedKey: |-
  Returns a unique pointer for a given name.
  
  The function returns a unique pointer for each particular file node
  name. This pointer can be then passed to the :ocvGetFileNode function
  that is faster than :ocvGetFileNodeByName because it compares text
  strings by comparing pointers rather than the strings' content.
  
  Consider the following example where an array of points is encoded as a
  sequence of 2-entry maps: :
  
  ```c++
  points:
    - { x: 10, y: 10 }
    - { x: 20, y: 20 }
    - { x: 30, y: 30 }
    # ...
  ```
  
  
  Then, it is possible to get hashed "x" and "y" pointers to speed up
  decoding of the points. :
  
  ```c++
  #include "cxcore.h"
  
  int main( int argc, char** argv )
  {
      CvFileStorage* fs = cvOpenFileStorage( "points.yml", 0, CV_STORAGE_READ );
      CvStringHashNode* x_key = cvGetHashedNode( fs, "x", -1, 1 );
      CvStringHashNode* y_key = cvGetHashedNode( fs, "y", -1, 1 );
      CvFileNode* points = cvGetFileNodeByName( fs, 0, "points" );
  
      if( CV_NODE_IS_SEQ(points->tag) )
      {
          CvSeq* seq = points->data.seq;
          int i, total = seq->total;
          CvSeqReader reader;
          cvStartReadSeq( seq, &reader, 0 );
          for( i = 0; i < total; i++ )
          {
              CvFileNode* pt = (CvFileNode*)reader.ptr;
  #if 1 /* faster variant */
              CvFileNode* xnode = cvGetFileNode( fs, pt, x_key, 0 );
              CvFileNode* ynode = cvGetFileNode( fs, pt, y_key, 0 );
              assert( xnode && CV_NODE_IS_INT(xnode->tag) &&
                      ynode && CV_NODE_IS_INT(ynode->tag));
              int x = xnode->data.i; // or x = cvReadInt( xnode, 0 );
              int y = ynode->data.i; // or y = cvReadInt( ynode, 0 );
  #elif 1 /* slower variant; does not use x_key & y_key */
              CvFileNode* xnode = cvGetFileNodeByName( fs, pt, "x" );
              CvFileNode* ynode = cvGetFileNodeByName( fs, pt, "y" );
              assert( xnode && CV_NODE_IS_INT(xnode->tag) &&
                      ynode && CV_NODE_IS_INT(ynode->tag));
              int x = xnode->data.i; // or x = cvReadInt( xnode, 0 );
              int y = ynode->data.i; // or y = cvReadInt( ynode, 0 );
  #else /* the slowest yet the easiest to use variant */
              int x = cvReadIntByName( fs, pt, "x", 0 /* default value */ );
              int y = cvReadIntByName( fs, pt, "y", 0 /* default value */ );
  #endif
              CV_NEXT_SEQ_ELEM( seq->elem_size, reader );
              printf("
          }
      }
      cvReleaseFileStorage( &fs );
      return 0;
  }
  ```
  
  
  Please note that whatever method of accessing a map you are using, it is
  still much slower than using plain sequences; for example, in the above
  example, it is more efficient to encode the points as pairs of integers
  in a single numeric sequence.
cv::GetRootFileNode: |-
  Retrieves one of the top-level nodes of the file storage.
  
  The function returns one of the top-level file nodes. The top-level
  nodes do not have a name, they correspond to the streams that are stored
  one after another in the file storage. If the index is out of range, the
  function returns a NULL pointer, so all the top-level nodes can be
  iterated by subsequent calls to the function with
  `stream_index=0,1,...`, until the NULL pointer is returned. This
  function can be used as a base for recursive traversal of the file
  storage.
cv::Load: |-
  Loads an object from a file.
  
  The function loads an object from a file. It basically reads the
  specified file, find the first top-level node and calls :ocvRead for
  that node. If the file node does not have type information or the type
  information can not be found by the type name, the function returns
  NULL. After the object is loaded, the file storage is closed and all the
  temporary buffers are deleted. Thus, to load a dynamic structure, such
  as a sequence, contour, or graph, one should pass a valid memory storage
  destination to the function.
cv::OpenFileStorage: |-
  Opens file storage for reading or writing data.
  
  The function opens file storage for reading or writing data. In the
  latter case, a new file is created or an existing file is rewritten. The
  type of the read or written file is determined by the filename
  extension: `.xml` for `XML` and `.yml` or `.yaml` for `YAML`. The
  function returns a pointer to the :ocvCvFileStorage structure. If the
  file cannot be opened then the function returns `NULL`.
cv::Read: "Decodes an object and returns a pointer to it.\n\n\
  The function decodes a user object (creates an object in a native\n\
  representation from the file storage subtree) and returns it. The object\n\
  to be decoded must be an instance of a registered type that supports the\n\
  `read` method (see :ocvCvTypeInfo). The type of the object is determined\n\
  by the type name that is encoded in the file. If the object is a dynamic\n\
  structure, it is created either in memory storage and passed to\n\
  :ocvOpenFileStorage or, if a NULL pointer was passed, in temporary\n\
  memory storage, which is released when :ocvReleaseFileStorage is called.\n\
  Otherwise, if the object is not a dynamic structure, it is created in a\n\
  heap and should be released with a specialized function or by using the\n\
  generic :ocvRelease."
cv::ReadByName: "Finds an object by name and decodes it.\n\n\
  The function is a simple superposition of :ocvGetFileNodeByName and\n\
  :ocvRead."
cv::ReadInt: |-
  Retrieves an integer value from a file node.
  
  The function returns an integer that is represented by the file node. If
  the file node is NULL, the `default_value` is returned (thus, it is
  convenient to call the function right after :ocvGetFileNode without
  checking for a NULL pointer). If the file node has type `CV_NODE_INT`,
  then `node->data.i` is returned. If the file node has type
  `CV_NODE_REAL`, then `node->data.f` is converted to an integer and
  returned. Otherwise the error is reported.
cv::ReadIntByName: "Finds a file node and returns its value.\n\n\
  The function is a simple superposition of :ocvGetFileNodeByName and\n\
  :ocvReadInt."
cv::ReadRawData: |-
  Reads multiple numbers.
  
  The function reads elements from a file node that represents a sequence
  of scalars.
cv::ReadRawDataSlice: |-
  Initializes file node sequence reader.
  
  The function reads one or more elements from the file node, representing
  a sequence, to a user-specified array. The total number of read sequence
  elements is a product of `total` and the number of components in each
  array element. For example, if `dt=2if`, the function will read
  `total*3` sequence elements. As with any sequence, some parts of the
  file node sequence can be skipped or read repeatedly by repositioning
  the reader using :ocvSetSeqReaderPos.
cv::ReadReal: |-
  Retrieves a floating-point value from a file node.
  
  The function returns a floating-point value that is represented by the
  file node. If the file node is NULL, the `default_value` is returned
  (thus, it is convenient to call the function right after :ocvGetFileNode
  without checking for a NULL pointer). If the file node has type
  `CV_NODE_REAL` , then `node->data.f` is returned. If the file node has
  type `CV_NODE_INT` , then `` node-:math:`>`data.f `` is converted to
  floating-point and returned. Otherwise the result is not determined.
cv::ReadRealByName: "Finds a file node and returns its value.\n\n\
  The function is a simple superposition of :ocvGetFileNodeByName and\n\
  :ocvReadReal ."
cv::ReadString: |-
  Retrieves a text string from a file node.
  
  The function returns a text string that is represented by the file node.
  If the file node is NULL, the `default_value` is returned (thus, it is
  convenient to call the function right after :ocvGetFileNode without
  checking for a NULL pointer). If the file node has type `CV_NODE_STR` ,
  then `` node-:math:`>`data.str.ptr `` is returned. Otherwise the result
  is not determined.
cv::ReadStringByName: "Finds a file node by its name and returns its value.\n\n\
  The function is a simple superposition of :ocvGetFileNodeByName and\n\
  :ocvReadString ."
cv::RegisterType: |-
  Registers a new type.
  
  The function registers a new type, which is described by `info` . The
  function creates a copy of the structure, so the user should delete it
  after calling the function.
cv::Release: |-
  Releases an object.
  
  The function finds the type of a given object and calls `release` with
  the double pointer.
cv::ReleaseFileStorage: |-
  Releases file storage.
  
  The function closes the file associated with the storage and releases
  all the temporary structures. It must be called after all I/O operations
  with the storage are finished.
cv::Save: |-
  Saves an object to a file.
  
  The function saves an object to a file. It provides a simple interface
  to :ocvWrite .
cv::StartNextStream: |-
  Starts the next stream.
  
  The function finishes the currently written stream and starts the next
  stream. In the case of XML the file with multiple streams looks like
  this: :
  
  ```c++
  <opencv_storage>
  <!-- stream #1 data -->
  </opencv_storage>
  <opencv_storage>
  <!-- stream #2 data -->
  </opencv_storage>
  ...
  ```
  
  
  The YAML file will look like this: :
  
  ```c++
  %YAML:1.0
  # stream #1 data
  ...
  ---
  # stream #2 data
  ```
  
  
  This is useful for concatenating files or for resuming the writing
  process.
cv::StartReadRawData: |-
  Initializes the file node sequence reader.
  
  The function initializes the sequence reader to read data from a file
  node. The initialized reader can be then passed to :ocvReadRawDataSlice.
cv::StartWriteStruct: |-
  Starts writing a new structure.
  
  The function starts writing a compound structure (collection) that can
  be a sequence or a map. After all the structure fields, which can be
  scalars or structures, are written, :ocvEndWriteStruct should be called.
  The function can be used to group some objects or to implement the
  `write` function for a some user object (see :ocvCvTypeInfo).
cv::TypeOf: |-
  Returns the type of an object.
  
  The function finds the type of a given object. It iterates through the
  list of registered types and calls the `is_instance` function/method for
  every type info structure with that object until one of them returns
  non-zero or until the whole list has been traversed. In the latter case,
  the function returns NULL.
cv::UnregisterType: "Unregisters the type.\n\n\
  The function unregisters a type with a specified name. If the name is\n\
  unknown, it is possible to locate the type info by an instance of the\n\
  type using :ocvTypeOf or by iterating the type list, starting from\n\
  :ocvFirstType, and then calling `cvUnregisterType(info->typeName)`."
cv::Write: |
  Writes an object to file storage.
  
  The function writes an object to file storage. First, the appropriate
  type info is found using :ocvTypeOf. Then, the `write` method associated
  with the type info is called.
  
  Attributes are used to customize the writing procedure. The standard
  types support the following attributes (all the `dt` attributes have the
  same format as in :ocvWriteRawData):
  
  #.
  :   CvSeq
  
  ```c++
  > -   **header\_dt** description of user fields of the sequence
  >     header that follow CvSeq, or CvChain (if the sequence is a
  >     Freeman chain) or CvContour (if the sequence is a contour or
  >     point sequence)
  >
  > -   **dt** description of the sequence elements.
  >
  > -   **recursive** if the attribute is present and is not equal to
  >     "0" or "false", the whole tree of sequences (contours) is
  >     stored.
  >
  ```
  
  
  #.
  :   CvGraph
  
  ```c++
  > -   **header\_dt** description of user fields of the graph header
  >     that follows CvGraph;
  >
  > -   **vertex\_dt** description of user fields of graph vertices
  >
  > -   **edge\_dt** description of user fields of graph edges (note
  >     that the edge weight is always written, so there is no need to
  >     specify it explicitly)
  >
  ```
  
  
  Below is the code that creates the YAML file shown in the
  `CvFileStorage` description:
  
  ```c++
  #include "cxcore.h"
  
  int main( int argc, char** argv )
  {
      CvMat* mat = cvCreateMat( 3, 3, CV_32F );
      CvFileStorage* fs = cvOpenFileStorage( "example.yml", 0, CV_STORAGE_WRITE );
  
      cvSetIdentity( mat );
      cvWrite( fs, "A", mat, cvAttrList(0,0) );
  
      cvReleaseFileStorage( &fs );
      cvReleaseMat( &mat );
      return 0;
  }
  ```

cv::WriteComment: |-
  Writes a comment.
  
  The function writes a comment into file storage. The comments are
  skipped when the storage is read.
cv::WriteFileNode: |-
  Writes a file node to another file storage.
  
  The function writes a copy of a file node to file storage. Possible
  applications of the function are merging several file storages into one
  and conversion between XML and YAML formats.
cv::WriteInt: |-
  Writes an integer value.
  
  The function writes a single integer value (with or without a name) to
  the file storage.
cv::WriteRawData: |-
  Writes multiple numbers.
  
  The function writes an array, whose elements consist of single or
  multiple numbers. The function call can be replaced with a loop
  containing a few :ocvWriteInt and :ocvWriteReal calls, but a single call
  is more efficient. Note that because none of the elements have a name,
  they should be written to a sequence rather than a map.
cv::WriteReal: |
  Writes a floating-point value.
  
  The function writes a single floating-point value (with or without a
  name) to file storage. Special values are encoded as follows: NaN (Not A
  Number) as .NaN, infinity as +.Inf or -.Inf.
  
  The following example shows how to use the low-level writing functions
  to store custom structures, such as termination criteria, without
  registering a new type. :
  
  ```c++
  void write_termcriteria( CvFileStorage* fs, const char* struct_name,
                           CvTermCriteria* termcrit )
  {
      cvStartWriteStruct( fs, struct_name, CV_NODE_MAP, NULL, cvAttrList(0,0));
      cvWriteComment( fs, "termination criteria", 1 ); // just a description
      if( termcrit->type & CV_TERMCRIT_ITER )
          cvWriteInteger( fs, "max_iterations", termcrit->max_iter );
      if( termcrit->type & CV_TERMCRIT_EPS )
          cvWriteReal( fs, "accuracy", termcrit->epsilon );
      cvEndWriteStruct( fs );
  }
  ```

cv::WriteString: |-
  Writes a text string.
  
  The function writes a text string to file storage.
cv::OpenFABMAP: |-
  The openFABMAP package has been integrated into OpenCV from the
  openFABMAP <<http://code.google.com/p/openfabmap/>> project
  [ICRA2011]_. OpenFABMAP is an open and modifiable code-source which
  implements the Fast Appearance-based Mapping algorithm (FAB-MAP)
  developed by Mark Cummins and Paul Newman. The algorithms used in
  openFABMAP were developed using only the relevant FAB-MAP publications.
  
  FAB-MAP is an approach to appearance-based place recognition. FAB-MAP
  compares images of locations that have been visited and determines the
  probability of re-visiting a location, as well as providing a measure of
  the probability of being at a new, previously unvisited location. Camera
  images form the sole input to the system, from which visual bag-of-words
  models are formed through the extraction of appearance-based (e.g. SURF)
  features.
  
  openFABMAP requires training data (e.g. a collection of images from a
  similar but not identical environment) to construct a visual vocabulary
  for the visual bag-of-words model, along with a Chow-Liu tree
  representation of feature likelihood and for use in the Sampled new
  place method (see below).
cv::of2::FabMap: |
  The main FabMap class performs the comparison between visual
  bags-of-words extracted from one or more images. The FabMap class is
  instantiated as one of the four inherited FabMap classes (FabMap1,
  FabMapLUT, FabMapFBO, FabMap2). Each inherited class performs the
  comparison differently based on algorithm iterations as published (see
  each class below for specifics). A Chow-Liu tree, detector model
  parameters and some option flags are common to all Fabmap variants and
  are supplied on class creation. Training data (visual bag-of-words) is
  supplied to the class if using the SAMPLED new place method. Test data
  (visual bag-of-words) is supplied as images to which query bag-of-words
  are compared against. The common flags are listed below: :
  
  ```c++
  enum {
      MEAN_FIELD,
      SAMPLED,
      NAIVE_BAYES,
      CHOW_LIU,
      MOTION_MODEL
  };
  ```
  
   MEAN_FIELD: Use the Mean Field approximation to determine the new
  place likelihood (cannot be used for FabMap2).
  
   SAMPLED: Use the Sampled approximation to determine the new place
  likelihood. Requires training data (see below).
  
   NAIVE_BAYES: Assume a naive Bayes approximation to feature
  distribution (i.e. all features are independent). Note that a
  Chow-Liu tree is still required but only the absolute word
  probabilities are used, feature co-occurrance information is
  discarded.
  
   CHOW_LIU: Use the full Chow-Liu tree to approximate feature
  distribution.
  
   MOTION_MODEL: Update the location distribution using the previous
  distribution as a (weak) prior. Used for matching in sequences (i.e.
  successive video frames).

cv::Training Data: |-
  Training data is required to use the SAMPLED new place method. The
  SAMPLED method was shown to have improved performance over the
  alternative MEAN_FIELD method. Training data can be added singularly or
  as a batch.
cv::Test Data: |-
  Test Data is the database of images represented using bag-of-words
  models. When a compare function is called, each query point is compared
  to the test data.
cv::Image Comparison: |-
  Image matching is performed calling the compare function. Query
  bag-of-words image descriptors are provided and compared to test data
  added to the FabMap class. Alternatively test data can be provided with
  the call to compare to which the comparison is performed. Results are
  written to the 'matches' argument.
cv::FabMap classes: |-
  The original FAB-MAP algorithm without any computational improvements as
  published in [IJRR2008]_
  
  The original FAB-MAP algorithm implemented as a look-up table for speed
  enhancements [ICRA2011]_
  
  The accelerated FAB-MAP using a 'fast bail-out' approach as in
  [TRO2010]_
  
  The inverted index FAB-MAP as in [IJRR2010]_. This version of FAB-MAP
  is the fastest without any loss of accuracy.
cv::of2::IMatch: |
  FAB-MAP comparison results are stored in a vector of IMatch structs.
  Each IMatch structure provides the index of the provided query
  bag-of-words, the index of the test bag-of-words, the raw log-likelihood
  of the match (independent of other comparisons), and the match
  probability (normalised over other comparison likelihoods).
  
  ```c++
  struct IMatch {
  
      IMatch() :
          queryIdx(-1), imgIdx(-1), likelihood(-DBL_MAX), match(-DBL_MAX) {
      }
      IMatch(int _queryIdx, int _imgIdx, double _likelihood, double _match) :
          queryIdx(_queryIdx), imgIdx(_imgIdx), likelihood(_likelihood), match(
                  _match) {
      }
  
      int queryIdx;    //query index
      int imgIdx;      //test index
  
      double likelihood;  //raw loglikelihood
      double match;      //normalised probability
  
      bool operator<(const IMatch& m) const {
          return match < m.match;
      }
  
  };
  ```

cv::of2::ChowLiuTree: |-
  The Chow-Liu tree is a probabilistic model of the environment in terms
  of feature occurance and co-occurance. The Chow-Liu tree is a form of
  Bayesian network. FAB-MAP uses the model when calculating bag-of-words
  similarity by taking into account feature saliency. Training data is
  provided to the ChowLiuTree class in the form of bag-of-words image
  descriptors. The make function produces a cv::Mat that encodes the tree
  structure.
cv::of2::BOWMSCTrainer: |-
  BOWMSCTrainer is a custom clustering algorithm used to produce the
  feature vocabulary required to create bag-of-words representations. The
  algorithm is an implementation of [AVC2007]_. Arguments against using
  K-means for the FAB-MAP algorithm are discussed in [IJRR2010]_. The
  BOWMSCTrainer inherits from the cv::BOWTrainer class, overwriting the
  cluster function.
  
  Cluster using features added to the class
cv::abs: |
  Calculates an absolute value of each matrix element.
  
  `abs` is a meta-function that is expanded to one of :ocvabsdiff or
  :ocvconvertScaleAbs forms:
  
    `C = abs(A-B)` is equivalent to `absdiff(A, B, C)`
  
  
    `C = abs(A)` is equivalent to `absdiff(A, Scalar::all(0), C)`
  
  
    `C = Mat_<Vec<uchar,n> >(abs(A*alpha + beta))` is equivalent to
  `convertScaleAbs(A, C, alpha, beta)`
  
  
  The output matrix has the same size and the same type as the input one
  except for the last case, where `C` is `depth=CV_8U` .

cv::absdiff: |
  Calculates the per-element absolute difference between two arrays or
  between an array and a scalar.
  
  The function `absdiff` calculates:
  
  *
  :   Absolute difference between two arrays when they have the same
      size and type:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1}(I) -  \texttt{src2}(I)|)$$
  ```
  
  
  *
  :   Absolute difference between an array and a scalar when the second
      array is constructed from `Scalar` or has as many elements as the
      number of channels in `src1`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1}(I) -  \texttt{src2} |)$$
  ```
  
  
  *
  :   Absolute difference between a scalar and an array when the first
      array is constructed from `Scalar` or has as many elements as the
      number of channels in `src2`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} (| \texttt{src1} -  \texttt{src2}(I) |)$$
  
  where `I` is a multi-dimensional index of array elements. In case
  of multi-channel arrays, each channel is processed independently.
  ```
  
  
  **note**
  
  Saturation is not applied when the arrays have the depth `CV_32S`. You
  may even get a negative value in the case of overflow.

cv::add: |
  Calculates the per-element sum of two arrays or an array and a scalar.
  
  The function `add` calculates:
  
  *
  :   Sum of two arrays when both input arrays have the same size and
      the same number of channels:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  *
  :   Sum of an array and a scalar when `src2` is constructed from
      `Scalar` or has the same number of elements as `src1.channels()`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) +  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  *
  :   Sum of a scalar and an array when `src1` is constructed from
      `Scalar` or has the same number of elements as `src2.channels()`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} +  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0$$
  
  where `I` is a multi-dimensional index of array elements. In case
  of multi-channel arrays, each channel is processed independently.
  ```
  
  
  The first function in the list above can be replaced with matrix
  expressions: :
  
  
  ```c++
  dst = src1 + src2;
  dst += src1; // equivalent to add(dst, src1, dst);
  ```
  
  
  The input arrays and the output array can all have the same or different
  depths. For example, you can add a 16-bit unsigned array to a 8-bit
  signed array and store the sum as a 32-bit floating-point array. Depth
  of the output array is determined by the `dtype` parameter. In the
  second and third cases above, as well as in the first case, when
  `src1.depth() == src2.depth()`, `dtype` can be set to the default `-1`.
  In this case, the output array will have the same depth as the input
  array, be it `src1`, `src2` or both.
  
  **note**
  
  Saturation is not applied when the output array has the depth
  `CV_32S`. You may even get result of an incorrect sign in the case of
  overflow.

cv::addWeighted: |
  Calculates the weighted sum of two arrays.
  
  The function `addWeighted` calculates the weighted sum of two arrays as
  follows:
  
  $$\texttt{dst} (I)= \texttt{saturate} ( \texttt{src1} (I)* \texttt{alpha} +  \texttt{src2} (I)* \texttt{beta} +  \texttt{gamma} )$$
  
  where `I` is a multi-dimensional index of array elements. In case of
  multi-channel arrays, each channel is processed independently.
  
  The function can be replaced with a matrix expression: :
  
  ```c++
  dst = src1*alpha + src2*beta + gamma;
  ```
  
  
  **note**
  
  Saturation is not applied when the output array has the depth
  `CV_32S`. You may even get result of an incorrect sign in the case of
  overflow.

cv::bitwise_and: |
  Calculates the per-element bit-wise conjunction of two arrays or an
  array and a scalar.
  
  The function calculates the per-element bit-wise logical conjunction
  for:
  
  *
  :   Two arrays when `src1` and `src2` have the same size:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   An array and a scalar when `src2` is constructed from `Scalar` or
      has the same number of elements as `src1.channels()`:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1} (I)  \wedge \texttt{src2} \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   A scalar and an array when `src1` is constructed from `Scalar` or
      has the same number of elements as `src2.channels()`:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1}  \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  In case of floating-point arrays, their machine-specific bit
  representations (usually IEEE754-compliant) are used for the operation.
  In case of multi-channel arrays, each channel is processed
  independently. In the second and third cases above, the scalar is first
  converted to the array type.

cv::bitwise_not: |-
  Inverts every bit of an array.
  
  The function calculates per-element bit-wise inversion of the input
  array:
  
  $$\texttt{dst} (I) =  \neg \texttt{src} (I)$$
  
  In case of a floating-point input array, its machine-specific bit
  representation (usually IEEE754-compliant) is used for the operation. In
  case of multi-channel arrays, each channel is processed independently.
cv::bitwise_or: |
  Calculates the per-element bit-wise disjunction of two arrays or an
  array and a scalar.
  
  The function calculates the per-element bit-wise logical disjunction
  for:
  
  *
  :   Two arrays when `src1` and `src2` have the same size:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   An array and a scalar when `src2` is constructed from `Scalar` or
      has the same number of elements as `src1.channels()`:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1} (I)  \vee \texttt{src2} \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   A scalar and an array when `src1` is constructed from `Scalar` or
      has the same number of elements as `src2.channels()`:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1}  \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  In case of floating-point arrays, their machine-specific bit
  representations (usually IEEE754-compliant) are used for the operation.
  In case of multi-channel arrays, each channel is processed
  independently. In the second and third cases above, the scalar is first
  converted to the array type.

cv::bitwise_xor: |
  Calculates the per-element bit-wise "exclusive or" operation on two
  arrays or an array and a scalar.
  
  The function calculates the per-element bit-wise logical "exclusive-or"
  operation for:
  
  *
  :   Two arrays when `src1` and `src2` have the same size:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   An array and a scalar when `src2` is constructed from `Scalar` or
      has the same number of elements as `src1.channels()`:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1} (I)  \oplus \texttt{src2} \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  *
  :   A scalar and an array when `src1` is constructed from `Scalar` or
      has the same number of elements as `src2.channels()`:
  
  ```c++
  > $$\texttt{dst} (I) =  \texttt{src1}  \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0$$
  ```
  
  
  In case of floating-point arrays, their machine-specific bit
  representations (usually IEEE754-compliant) are used for the operation.
  In case of multi-channel arrays, each channel is processed
  independently. In the 2nd and 3rd cases above, the scalar is first
  converted to the array type.

cv::calcCovarMatrix: |-
  Calculates the covariance matrix of a set of vectors.
  
  The functions `calcCovarMatrix` calculate the covariance matrix and,
  optionally, the mean vector of the set of input vectors.
cv::cartToPolar: |-
  Calculates the magnitude and angle of 2D vectors.
  
  The function `cartToPolar` calculates either the magnitude, angle, or
  both for every 2D vector (x(I),y(I)):
  
  $$\begin{array}{l} \texttt{magnitude} (I)= \sqrt{\texttt{x}(I)^2+\texttt{y}(I)^2} , \ \texttt{angle} (I)= \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))[ \cdot180 / \pi ] \end{array}$$
  
  The angles are calculated with accuracy about 0.3 degrees. For the point
  (0,0), the angle is set to 0.
cv::checkRange: |-
  Checks every element of an input array for invalid values.
  
  The functions `checkRange` check that every array element is neither NaN
  nor infinite. When `minVal < -DBL_MAX` and `maxVal < DBL_MAX`, the
  functions also check that each value is between `minVal` and `maxVal`.
  In case of multi-channel arrays, each channel is processed
  independently. If some values are out of range, position of the first
  outlier is stored in `pos` (when `pos != NULL`). Then, the functions
  either return false (when `quiet=true`) or throw an exception.
cv::compare: |
  Performs the per-element comparison of two arrays or an array and scalar
  value.
  
  The function compares:
  
  *
  :   Elements of two arrays when `src1` and `src2` have the same size:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1} (I)  \,\texttt{cmpop}\, \texttt{src2} (I)$$
  ```
  
  
  *
  :   Elements of `src1` with a scalar `src2` when `src2` is constructed
      from `Scalar` or has a single element:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1}(I) \,\texttt{cmpop}\,  \texttt{src2}$$
  ```
  
  
  *
  :   `src1` with elements of `src2` when `src1` is constructed from
      `Scalar` or has a single element:
  
  ```c++
  $$\texttt{dst} (I) =  \texttt{src1}  \,\texttt{cmpop}\, \texttt{src2} (I)$$
  ```
  
  
  When the comparison result is true, the corresponding element of output
  array is set to 255. The comparison operations can be replaced with the
  equivalent matrix expressions: :
  
  
  ```c++
  Mat dst1 = src1 >= src2;
  Mat dst2 = src1 < 8;
  ...
  ```

cv::completeSymm: |-
  Copies the lower or the upper half of a square matrix to another half.
  
  The function `completeSymm` copies the lower half of a square matrix to
  its another half. The matrix diagonal remains unchanged:
  
  *
  :   $\texttt{mtx}_{ij}=\texttt{mtx}_{ji}$ for $i > j$ if
      `lowerToUpper=false`
  
  *
  :   $\texttt{mtx}_{ij}=\texttt{mtx}_{ji}$ for $i < j$ if
      `lowerToUpper=true`
cv::convertScaleAbs: "\n\n\
  Scales, calculates absolute values, and converts the result to 8-bit.\n\n\
  On each element of the input array, the function `convertScaleAbs`\n\
  performs three operations sequentially: scaling, taking an absolute\n\
  value, conversion to an unsigned 8-bit type:\n\n\
  $$\\texttt{dst} (I)= \\texttt{saturate_cast<uchar>} (| \\texttt{src} (I)* \\texttt{alpha} +  \\texttt{beta} |)$$\n\n\
  In case of multi-channel arrays, the function processes each channel\n\
  independently. When the output is not 8-bit, the operation can be\n\
  emulated by calling the `Mat::convertTo` method (or by using matrix\n\
  expressions) and then by calculating an absolute value of the result.\n\
  For example: :\n\n\
  ```c++\n\
  Mat_<float> A(30,30);\n\
  randu(A, Scalar(-100), Scalar(100));\n\
  Mat_<float> B = A*5 + 3;\n\
  B = abs(B);\n\
  // Mat_<float> B = abs(A*5+3) will also do the job,\n\
  // but it will allocate a temporary matrix\n\
  ```\n"
cv::countNonZero: |-
  Counts non-zero array elements.
  
  The function returns the number of non-zero elements in `src` :
  
  $$\sum _{I: \; \texttt{src} (I) \ne0 } 1$$
cv::cvarrToMat: "Converts `CvMat`, `IplImage` , or `CvMatND` to `Mat`.\n\n\
  The function `cvarrToMat` converts `CvMat`, `IplImage` , or `CvMatND`\n\
  header to :ocvMat header, and optionally duplicates the underlying data.\n\
  The constructed header is returned by the function.\n\n\
  When `copyData=false` , the conversion is done really fast (in O(1)\n\
  time) and the newly created matrix header will have `refcount=0` , which\n\
  means that no reference counting is done for the matrix data. In this\n\
  case, you have to preserve the data until the new header is destructed.\n\
  Otherwise, when `copyData=true` , the new buffer is allocated and\n\
  managed as if you created a new matrix from scratch and copied the data\n\
  there. That is, `cvarrToMat(arr, true)` is equivalent to\n\
  `cvarrToMat(arr, false).clone()` (assuming that COI is not set). The\n\
  function provides a uniform way of supporting `CvArr` paradigm in the\n\
  code that is migrated to use new-style data structures internally. The\n\
  reverse transformation, from `Mat` to `CvMat` or `IplImage` can be done\n\
  by a simple assignment: :\n\n\
  ```c++\n\
  CvMat* A = cvCreateMat(10, 10, CV_32F);\n\
  cvSetIdentity(A);\n\
  IplImage A1; cvGetImage(A, &A1);\n\
  Mat B = cvarrToMat(A);\n\
  Mat B1 = cvarrToMat(&A1);\n\
  IplImage C = B;\n\
  CvMat C1 = B1;\n\
  // now A, A1, B, B1, C and C1 are different headers\n\
  // for the same 10x10 floating-point array.\n\
  // note that you will need to use \"&\"\n\
  // to pass C & C1 to OpenCV functions, for example:\n\
  printf(\"%g\\n\", cvNorm(&C1, 0, CV_L2));\n\
  ```\n\n\n\
  Normally, the function is used to convert an old-style 2D array (\n\
  `CvMat` or `IplImage` ) to `Mat` . However, the function can also take\n\
  `CvMatND` as an input and create :ocvMat for it, if it is possible. And,\n\
  for `CvMatND A` , it is possible if and only if\n\
  `A.dim[i].size*A.dim.step[i] == A.dim.step[i-1]` for all or for all but\n\
  one `i, 0 < i < A.dims` . That is, the matrix data should be continuous\n\
  or it should be representable as a sequence of continuous matrices. By\n\
  using this function in this way, you can process `CvMatND` using an\n\
  arbitrary element-wise function.\n\n\
  The last parameter, `coiMode` , specifies how to deal with an image with\n\
  COI set. By default, it is 0 and the function reports an error when an\n\
  image with COI comes in. And `coiMode=1` means that no error is\n\
  signalled. You have to check COI presence and handle it manually. The\n\
  modern structures, such as :ocvMat and `MatND` do not support COI\n\
  natively. To process an individual channel of a new-style array, you\n\
  need either to organize a loop over the array (for example, using matrix\n\
  iterators) where the channel of interest will be processed, or extract\n\
  the COI using :ocvmixChannels (for new-style arrays) or\n\
  :ocvextractImageCOI (for old-style arrays), process this individual\n\
  channel, and insert it back to the output array if needed (using\n\
  :ocvmixChannels or :ocvinsertImageCOI , respectively)."
cv::dct: |+
  Performs a forward or inverse discrete Cosine transform of 1D or 2D
  array.
  
  The function `dct` performs a forward or inverse discrete Cosine
  transform (DCT) of a 1D or 2D floating-point array:
  
  *
  :   Forward Cosine transform of a 1D vector of `N` elements:
  
  ```c++
  $$Y = C^{(N)}  \cdot X$$
  
  where
  
  $$C^{(N)}_{jk}= \sqrt{\alpha_j/N} \cos \left ( \frac{\pi(2k+1)j}{2N} \right )$$
  
  and
  
  $\alpha_0=1$, $\alpha_j=2$ for *j \> 0*.
  ```
  
  
  *
  :   Inverse Cosine transform of a 1D vector of `N` elements:
  
  ```c++
  $$X =  \left (C^{(N)} \right )^{-1}  \cdot Y =  \left (C^{(N)} \right )^T  \cdot Y$$
  
  (since $C^{(N)}$ is an orthogonal matrix,
  $C^{(N)} \cdot \left(C^{(N)}\right)^T = I$ )
  ```
  
  
  *
  :   Forward 2D Cosine transform of `M x N` matrix:
  
  ```c++
  $$Y = C^{(N)}  \cdot X  \cdot \left (C^{(N)} \right )^T$$
  ```
  
  
  *
  :   Inverse 2D Cosine transform of `M x N` matrix:
  
  ```c++
  $$X =  \left (C^{(N)} \right )^T  \cdot X  \cdot C^{(N)}$$
  ```
  
  
  The function chooses the mode of operation by looking at the flags and
  size of the input array:
  
  *
  :   If `(flags & DCT_INVERSE) == 0` , the function does a forward 1D or
      2D transform. Otherwise, it is an inverse 1D or 2D transform.
  
  *
  :   If `(flags & DCT_ROWS) != 0` , the function performs a 1D transform
      of each row.
  
  *
  :   If the array is a single column or a single row, the function
      performs a 1D transform.
  
  *
  :   If none of the above is true, the function performs a 2D transform.
  
  **note**
  
  Currently `dct` supports even-size arrays (2, 4, 6 ...). For data
  analysis and approximation, you can pad the array when necessary.
  
  Also, the function performance depends very much, and not
  monotonically, on the array size (see :ocvgetOptimalDFTSize ). In the
  current implementation DCT of a vector of size `N` is calculated via
  DFT of a vector of size `N/2` . Thus, the optimal DCT size `N1 >= N`
  can be calculated as: :
  
  ```c++
  size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }
  N1 = getOptimalDCTSize(N);
  ```
  
cv::dft: |-
  Performs a forward or inverse Discrete Fourier transform of a 1D or 2D
  floating-point array.
  
  The function performs one of the following:
  
  *
  :   Forward the Fourier transform of a 1D vector of `N` elements:
  
  ```c++
  $$Y = F^{(N)}  \cdot X,$$
  
  where $F^{(N)}_{jk}=\exp(-2\pi i j k/N)$ and $i=\sqrt{-1}$
  ```
  
  
  *
  :   Inverse the Fourier transform of a 1D vector of `N` elements:
  
  ```c++
  $$\begin{array}{l} X'=  \left (F^{(N)} \right )^{-1}  \cdot Y =  \left (F^{(N)} \right )^*  \cdot y  \\ X = (1/N)  \cdot X, \end{array}$$
  
  where $F^*=\left(\textrm{Re}(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T$
  ```
  
  
  *
  :   Forward the 2D Fourier transform of a `M x N` matrix:
  
  ```c++
  $$Y = F^{(M)}  \cdot X  \cdot F^{(N)}$$
  ```
  
  
  *
  :   Inverse the 2D Fourier transform of a `M x N` matrix:
  
  ```c++
  $$\begin{array}{l} X'=  \left (F^{(M)} \right )^*  \cdot Y  \cdot \left (F^{(N)} \right )^* \\ X =  \frac{1}{M \cdot N} \cdot X' \end{array}$$
  ```
  
  
  In case of real (single-channel) data, the output spectrum of the
  forward Fourier transform or input spectrum of the inverse Fourier
  transform can be represented in a packed format called *CCS*
  (complex-conjugate-symmetrical). It was borrowed from IPL (Intel* Image
  Processing Library). Here is how 2D *CCS* spectrum looks:
  
  $$\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2} & Im Y_{0,2} &  \cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2}  \ Re Y_{1,0} & Re Y_{1,1} & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} &  \cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2}  \ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} &  \cdots & Re Y_{2,N/2-1} & Im Y_{2,N/2-1} & Im Y_{1,N/2}  \ \hdotsfor{9} \ Re Y_{M/2-1,0} &  Re Y_{M-3,1}  & Im Y_{M-3,1} &  \hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2}  \ Im Y_{M/2-1,0} &  Re Y_{M-2,1}  & Im Y_{M-2,1} &  \hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2}  \ Re Y_{M/2,0}  &  Re Y_{M-1,1} &  Im Y_{M-1,1} &  \hdotsfor{3} & Re Y_{M-1,N/2-1} & Im Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \end{bmatrix}$$
  
  In case of 1D transform of a real vector, the output looks like the
  first row of the matrix above.
  
  So, the function chooses an operation mode depending on the flags and
  size of the input array:
  
    If `DFT_ROWS` is set or the input array has a single row or single
  column, the function performs a 1D forward or inverse transform of
  each row of a matrix when `DFT_ROWS` is set. Otherwise, it
  performs a 2D transform.
  
  
    If the input array is real and `DFT_INVERSE` is not set, the
  function performs a forward 1D or 2D transform:
  
    When `DFT_COMPLEX_OUTPUT` is set, the output is a complex
  matrix of the same size as input.
  
  
    When `DFT_COMPLEX_OUTPUT` is not set, the output is a real
  matrix of the same size as input. In case of 2D transform,
  it uses the packed format as shown above. In case of a
  single 1D transform, it looks like the first row of the
  matrix above. In case of multiple 1D transforms (when using
  the `DCT_ROWS` flag), each row of the output matrix looks
  like the first row of the matrix above.
  
  
  
  
    If the input array is complex and either `DFT_INVERSE` or
  `DFT_REAL_OUTPUT` are not set, the output is a complex array of
  the same size as input. The function performs a forward or inverse
  1D or 2D transform of the whole input array or each row of the
  input array independently, depending on the flags `DFT_INVERSE`
  and `DFT_ROWS`.
  
  
    When `DFT_INVERSE` is set and the input array is real, or it is
  complex but `DFT_REAL_OUTPUT` is set, the output is a real array
  of the same size as input. The function performs a 1D or 2D
  inverse transformation of the whole input array or each individual
  row, depending on the flags `DFT_INVERSE` and `DFT_ROWS`.
  
  
  If `DFT_SCALE` is set, the scaling is done after the transformation.
  
  
  Unlike :ocvdct , the function supports arrays of arbitrary size. But
  only those arrays are processed efficiently, whose sizes can be
  factorized in a product of small prime numbers (2, 3, and 5 in the
  current implementation). Such an efficient DFT size can be calculated
  using the :ocvgetOptimalDFTSize method.
  
  The sample below illustrates how to calculate a DFT-based convolution of
  two 2D real arrays: :
  
  ```c++
  void convolveDFT(InputArray A, InputArray B, OutputArray C)
  {
      // reallocate the output array if needed
      C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());
      Size dftSize;
      // calculate the size of DFT transform
      dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);
      dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);
  
      // allocate temporary buffers and initialize them with 0's
      Mat tempA(dftSize, A.type(), Scalar::all(0));
      Mat tempB(dftSize, B.type(), Scalar::all(0));
  
      // copy A and B to the top-left corners of tempA and tempB, respectively
      Mat roiA(tempA, Rect(0,0,A.cols,A.rows));
      A.copyTo(roiA);
      Mat roiB(tempB, Rect(0,0,B.cols,B.rows));
      B.copyTo(roiB);
  
      // now transform the padded A & B in-place;
      // use "nonzeroRows" hint for faster processing
      dft(tempA, tempA, 0, A.rows);
      dft(tempB, tempB, 0, B.rows);
  
      // multiply the spectrums;
      // the function handles packed spectrum representations well
      mulSpectrums(tempA, tempB, tempA);
  
      // transform the product back from the frequency domain.
      // Even though all the result rows will be non-zero,
      // you need only the first C.rows of them, and thus you
      // pass nonzeroRows == C.rows
      dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);
  
      // now copy the result back to C.
      tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);
  
      // all the temporary buffers will be deallocated automatically
  }
  ```
  
  
  To optimize this sample, consider the following approaches:
  
  *
  :   Since `nonzeroRows != 0` is passed to the forward transform calls
      and since `A` and `B` are copied to the top-left corners of `tempA`
      and `tempB`, respectively, it is not necessary to clear the whole
      `tempA` and `tempB`. It is only necessary to clear the
      `tempA.cols - A.cols` ( `tempB.cols - B.cols`) rightmost columns of
      the matrices.
  
  *
  :   This DFT-based convolution does not have to be applied to the whole
      big arrays, especially if `B` is significantly smaller than `A` or
      vice versa. Instead, you can calculate convolution by parts. To do
      this, you need to split the output array `C` into multiple tiles.
      For each tile, estimate which parts of `A` and `B` are required to
      calculate convolution in this tile. If the tiles in `C` are too
      small, the speed will decrease a lot because of repeated work. In
      the ultimate case, when each tile in `C` is a single pixel, the
      algorithm becomes equivalent to the naive convolution algorithm. If
      the tiles are too big, the temporary arrays `tempA` and `tempB`
      become too big and there is also a slowdown because of bad cache
      locality. So, there is an optimal tile size somewhere in the middle.
  
  *
  :   If different tiles in `C` can be calculated in parallel and, thus,
      the convolution is done by parts, the loop can be threaded.
  
  All of the above improvements have been implemented in :ocvmatchTemplate
  and :ocvfilter2D . Therefore, by using them, you can get the performance
  even better than with the above theoretically optimal implementation.
  Though, those two functions actually calculate cross-correlation, not
  convolution, so you need to "flip" the second convolution operand `B`
  vertically and horizontally using :ocvflip .
cv::divide: |
  Performs per-element division of two arrays or a scalar by an array.
  
  The functions `divide` divide one array by another:
  
  $$\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}$$
  
  or a scalar by an array when there is no `src1` :
  
  $$\texttt{dst(I) = saturate(scale/src2(I))}$$
  
  When `src2(I)` is zero, `dst(I)` will also be zero. Different channels
  of multi-channel arrays are processed independently.
  
  **note**
  
  Saturation is not applied when the output array has the depth
  `CV_32S`. You may even get result of an incorrect sign in the case of
  overflow.

cv::determinant: "Returns the determinant of a square floating-point matrix.\n\n\
  The function `determinant` calculates and returns the determinant of the\n\
  specified matrix. For small matrices ( `mtx.cols=mtx.rows<=3` ), the\n\
  direct method is used. For larger matrices, the function uses LU\n\
  factorization with partial pivoting.\n\n\
  For symmetric positively-determined matrices, it is also possible to use\n\
  :ocveigen decomposition to calculate the determinant."
cv::eigen: |
  Calculates eigenvalues and eigenvectors of a symmetric matrix.
  
  The functions `eigen` calculate just eigenvalues, or eigenvalues and
  eigenvectors of the symmetric matrix `src` : :
  
  ```c++
  src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()
  ```
  
  
  **note**
  
  in the new and the old interfaces different ordering of eigenvalues
  and eigenvectors parameters is used.

cv::exp: |-
  Calculates the exponent of every array element.
  
  The function `exp` calculates the exponent of every element of the input
  array:
  
  $$\texttt{dst} [I] = e^{ src(I) }$$
  
  The maximum relative error is about `7e-6` for single-precision input
  and less than `1e-10` for double-precision input. Currently, the
  function converts denormalized values to zeros on output. Special values
  (NaN, Inf) are not handled.
cv::extractImageCOI: "Extracts the selected image channel.\n\n\
  The function `extractImageCOI` is used to extract an image COI from an\n\
  old-style array and put the result to the new-style C++ matrix. As\n\
  usual, the output matrix is reallocated using `Mat::create` if needed.\n\n\
  To extract a channel from a new-style matrix, use :ocvmixChannels or\n\
  :ocvsplit ."
cv::insertImageCOI: |-
  Copies the selected image channel from a new-style C++ matrix to the
  old-style C array.
  
  The function `insertImageCOI` is used to extract an image COI from a
  new-style C++ matrix and put the result to the old-style array.
  
  The sample below illustrates how to use the function: :
  
  ```c++
  Mat temp(240, 320, CV_8UC1, Scalar(255));
  IplImage* img = cvCreateImage(cvSize(320,240), IPL_DEPTH_8U, 3);
  insertImageCOI(temp, img, 1); //insert to the first channel
  cvNamedWindow("window",1);
  cvShowImage("window", img); //you should see green image, because channel number 1 is green (BGR)
  cvWaitKey(0);
  cvDestroyAllWindows();
  cvReleaseImage(&img);
  ```
  
  
  To insert a channel to a new-style matrix, use :ocvmerge .
cv::flip: |-
  Flips a 2D array around vertical, horizontal, or both axes.
  
  The function `flip` flips the array in one of three different ways (row
  and column indices are 0-based):
  
  $$\texttt{dst} _{ij} =
  \left{
  \begin{array}{l l}
  \texttt{src} _{\texttt{src.rows}-i-1,j} & if\;  \texttt{flipCode} = 0 \
  \texttt{src} _{i, \texttt{src.cols} -j-1} & if\;  \texttt{flipCode} > 0 \
  \texttt{src} _{ \texttt{src.rows} -i-1, \texttt{src.cols} -j-1} & if\; \texttt{flipCode} < 0 \
  \end{array}
  \right.$$
  
  The example scenarios of using the function are the following:
  
  *
  :   Vertical flipping of the image (`flipCode == 0`) to switch between
      top-left and bottom-left image origin. This is a typical operation
      in video processing on Microsoft Windows* OS.
  
  *
  :   Horizontal flipping of the image with the subsequent horizontal
      shift and absolute difference calculation to check for a
      vertical-axis symmetry (`flipCode > 0`).
  
  *
  :   Simultaneous horizontal and vertical flipping of the image with
      the subsequent shift and absolute difference calculation to check
      for a central symmetry (`flipCode < 0`).
  
  *
  :   Reversing the order of point arrays (`flipCode > 0` or
      `flipCode == 0`).
cv::gemm: "\n\n\
  Performs generalized matrix multiplication.\n\n\
  The function performs generalized matrix multiplication similar to the\n\
  `gemm` functions in BLAS level 3. For example,\n\
  `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)`\n\
  corresponds to\n\n\
  $$\\texttt{dst} =  \\texttt{alpha} \\cdot \\texttt{src1} ^T  \\cdot \\texttt{src2} +  \\texttt{beta} \\cdot \\texttt{src3} ^T$$\n\n\
  The function can be replaced with a matrix expression. For example, the\n\
  above call can be replaced with: :\n\n\
  ```c++\n\
  dst = alpha*src1.t()*src2 + beta*src3.t();\n\
  ```\n"
cv::getOptimalDFTSize: |-
  Returns the optimal DFT size for a given vector size.
  
  DFT performance is not a monotonic function of a vector size. Therefore,
  when you calculate convolution of two arrays or perform the spectral
  analysis of an array, it usually makes sense to pad the input data with
  zeros to get a bit larger array that can be transformed much faster than
  the original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32,
  ...) are the fastest to process. Though, the arrays whose size is a
  product of 2's, 3's, and 5's (for example, 300 = 5*5*3*2*2) are also
  processed quite efficiently.
  
  The function `getOptimalDFTSize` returns the minimum number `N` that is
  greater than or equal to `vecsize` so that the DFT of a vector of size
  `N` can be processed efficiently. In the current implementation `N` = 2
  ^p^ * 3 ^q^ * 5 ^r^ for some integer `p`, `q`, `r`.
  
  The function returns a negative number if `vecsize` is too large (very
  close to `INT_MAX` ).
  
  While the function cannot be used directly to estimate the optimal
  vector size for DCT transform (since the current DCT implementation
  supports only even-size vectors), it can be easily processed as
  `getOptimalDFTSize((vecsize+1)/2)*2`.
cv::idct: |-
  Calculates the inverse Discrete Cosine Transform of a 1D or 2D array.
  
  `idct(src, dst, flags)` is equivalent to
  `dct(src, dst, flags | DCT_INVERSE)`.
cv::idft: |
  Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.
  
  `idft(src, dst, flags)` is equivalent to
  `dft(src, dst, flags | DFT_INVERSE)` .
  
  See :ocvdft for details.
  
  **note**
  
  None of `dft` and `idft` scales the result by default. So, you should
  pass `DFT_SCALE` to one of `dft` or `idft` explicitly to make these
  transforms mutually inverse.

cv::inRange: |-
  Checks if array elements lie between the elements of two other arrays.
  
  The function checks the range as follows:
  
    For every element of a single-channel input array:
  
  $$\texttt{dst} (I)= \texttt{lowerb} (I)_0  \leq \texttt{src} (I)_0 \leq  \texttt{upperb} (I)_0$$
  
  
    For two-channel arrays:
  
  $$\texttt{dst} (I)= \texttt{lowerb} (I)_0  \leq \texttt{src} (I)_0 \leq  \texttt{upperb} (I)_0  \land \texttt{lowerb} (I)_1  \leq \texttt{src} (I)_1 \leq  \texttt{upperb} (I)_1$$
  
  
    and so forth.
  
  
  That is, `dst` (I) is set to 255 (all `1` -bits) if `src` (I) is within
  the specified 1D, 2D, 3D, ... box and 0 otherwise.
  
  
  When the lower and/or upper boundary parameters are scalars, the indexes
  `(I)` at `lowerb` and `upperb` in the above formulas should be omitted.
cv::invert: |-
  Finds the inverse or pseudo-inverse of a matrix.
  
  The function `invert` inverts the matrix `src` and stores the result in
  `dst` . When the matrix `src` is singular or non-square, the function
  calculates the pseudo-inverse matrix (the `dst` matrix) so that
  `norm(src*dst - I)` is minimal, where I is an identity matrix.
  
  In case of the `DECOMP_LU` method, the function returns non-zero value
  if the inverse has been successfully calculated and 0 if `src` is
  singular.
  
  In case of the `DECOMP_SVD` method, the function returns the inverse
  condition number of `src` (the ratio of the smallest singular value to
  the largest singular value) and 0 if `src` is singular. The SVD method
  calculates a pseudo-inverse matrix if `src` is singular.
  
  Similarly to `DECOMP_LU` , the method `DECOMP_CHOLESKY` works only with
  non-singular square matrices that should also be symmetrical and
  positively defined. In this case, the function stores the inverted
  matrix in `dst` and returns non-zero. Otherwise, it returns 0.
cv::log: |-
  Calculates the natural logarithm of every array element.
  
  The function `log` calculates the natural logarithm of the absolute
  value of every element of the input array:
  
  $$\texttt{dst} (I) =  \fork{\log |\texttt{src}(I)|}{if $\texttt{src}(I) \ne 0$ }{\texttt{C}}{otherwise}$$
  
  where `C` is a large negative number (about -700 in the current
  implementation). The maximum relative error is about `7e-6` for
  single-precision input and less than `1e-10` for double-precision input.
  Special values (NaN, Inf) are not handled.
cv::LUT: |-
  Performs a look-up table transform of an array.
  
  The function `LUT` fills the output array with values from the look-up
  table. Indices of the entries are taken from the input array. That is,
  the function processes each element of `src` as follows:
  
  $$\texttt{dst} (I)  \leftarrow \texttt{lut(src(I) + d)}$$
  
  where
  
  $$d =  \fork{0}{if \texttt{src} has depth \texttt{CV_8U}}{128}{if \texttt{src} has depth \texttt{CV_8S}}$$
cv::magnitude: |-
  Calculates the magnitude of 2D vectors.
  
  The function `magnitude` calculates the magnitude of 2D vectors formed
  from the corresponding elements of `x` and `y` arrays:
  
  $$\texttt{dst} (I) =  \sqrt{\texttt{x}(I)^2 + \texttt{y}(I)^2}$$
cv::Mahalanobis: |-
  Calculates the Mahalanobis distance between two vectors.
  
  The function `Mahalanobis` calculates and returns the weighted distance
  between two vectors:
  
  $$d( \texttt{vec1} , \texttt{vec2} )= \sqrt{\sum_{i,j}{\texttt{icovar(i,j)}\cdot(\texttt{vec1}(I)-\texttt{vec2}(I))\cdot(\texttt{vec1(j)}-\texttt{vec2(j)})} }$$
  
  The covariance matrix may be calculated using the :ocvcalcCovarMatrix
  function and then inverted using the :ocvinvert function (preferably
  using the `DECOMP_SVD` method, as the most accurate).
cv::max: |-
  Calculates per-element maximum of two arrays or an array and a scalar.
  
  The functions `max` calculate the per-element maximum of two arrays:
  
  $$\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{src2} (I))$$
  
  or array and a scalar:
  
  $$\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{value} )$$
  
  In the second variant, when the input array is multi-channel, each
  channel is compared with `value` independently.
  
  The first 3 variants of the function listed above are actually a part of
  MatrixExpressions . They return an expression object that can be further
  either transformed/ assigned to a matrix, or passed to a function, and
  so on.
cv::mean: |-
  Calculates an average (mean) of array elements.
  
  The function `mean` calculates the mean value `M` of array elements,
  independently for each channel, and return it:
  
  $$\begin{array}{l} N =  \sum _{I: \; \texttt{mask} (I) \ne 0} 1 \ M_c =  \left ( \sum _{I: \; \texttt{mask} (I) \ne 0}{ \texttt{mtx} (I)_c} \right )/N \end{array}$$
  
  When all the mask elements are 0's, the functions return
  `Scalar::all(0)` .
cv::meanStdDev: |
  Calculates a mean and standard deviation of array elements.
  
  The function `meanStdDev` calculates the mean and the standard deviation
  `M` of array elements independently for each channel and returns it via
  the output parameters:
  
  $$\begin{array}{l} N =  \sum _{I, \texttt{mask} (I)  \ne 0} 1 \ \texttt{mean} _c =  \frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \texttt{src} (I)_c}{N} \ \texttt{stddev} _c =  \sqrt{\frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \left ( \texttt{src} (I)_c -  \texttt{mean} _c \right )^2}{N}} \end{array}$$
  
  When all the mask elements are 0's, the functions return
  `mean=stddev=Scalar::all(0)` .
  
  **note**
  
  The calculated standard deviation is only the diagonal of the complete
  normalized covariance matrix. If the full matrix is needed, you can
  reshape the multi-channel array `M x N` to the single-channel array
  `M*N x mtx.channels()` (only possible when the matrix is continuous)
  and then pass the matrix to :ocvcalcCovarMatrix .

cv::merge: |-
  Creates one multichannel array out of several single-channel ones.
  
  The functions `merge` merge several arrays to make a single
  multi-channel array. That is, each element of the output array will be a
  concatenation of the elements of the input arrays, where elements of
  i-th input array are treated as `mv[i].channels()`-element vectors.
  
  The function :ocvsplit does the reverse operation. If you need to
  shuffle channels in some other advanced way, use :ocvmixChannels .
cv::min: |-
  Calculates per-element minimum of two arrays or an array and a scalar.
  
  The functions `min` calculate the per-element minimum of two arrays:
  
  $$\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{src2} (I))$$
  
  or array and a scalar:
  
  $$\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{value} )$$
  
  In the second variant, when the input array is multi-channel, each
  channel is compared with `value` independently.
  
  The first three variants of the function listed above are actually a
  part of MatrixExpressions . They return the expression object that can
  be further either transformed/assigned to a matrix, or passed to a
  function, and so on.
cv::minMaxIdx: Finds the global minimum and maximum in an array
cv::minMaxLoc: "Finds the global minimum and maximum in an array.\n\n\
  The functions `minMaxLoc` find the minimum and maximum element values\n\
  and their positions. The extremums are searched across the whole array\n\
  or, if `mask` is not an empty array, in the specified array region.\n\n\
  The functions do not work with multi-channel arrays. If you need to find\n\
  minimum or maximum elements across all the channels, use\n\
  :ocvMat::reshape first to reinterpret the array as single-channel. Or\n\
  you may extract the particular channel using either :ocvextractImageCOI\n\
  , or :ocvmixChannels , or :ocvsplit ."
cv::mixChannels: "Copies specified channels from input arrays to the specified channels of\n\
  output arrays.\n\n\
  The functions `mixChannels` provide an advanced mechanism for shuffling\n\
  image channels.\n\n\
  :ocvsplit and :ocvmerge and some forms of :ocvcvtColor are partial cases\n\
  of `mixChannels` .\n\n\
  In the example below, the code splits a 4-channel RGBA image into a\n\
  3-channel BGR (with R and B channels swapped) and a separate\n\
  alpha-channel image: :\n\n\
  ```c++\n\
  Mat rgba( 100, 100, CV_8UC4, Scalar(1,2,3,4) );\n\
  Mat bgr( rgba.rows, rgba.cols, CV_8UC3 );\n\
  Mat alpha( rgba.rows, rgba.cols, CV_8UC1 );\n\n\
  // forming an array of matrices is a quite efficient operation,\n\
  // because the matrix data is not copied, only the headers\n\
  Mat out[] = { bgr, alpha };\n\
  // rgba[0] -> bgr[2], rgba[1] -> bgr[1],\n\
  // rgba[2] -> bgr[0], rgba[3] -> alpha[0]\n\
  int from_to[] = { 0,2, 1,1, 2,0, 3,3 };\n\
  mixChannels( &rgba, 1, out, 2, from_to, 4 );\n\
  ```\n\n\n\
  **note**\n\n\
  Unlike many other new-style C++ functions in OpenCV (see the\n\
  introduction section and :ocvMat::create ), `mixChannels` requires the\n\
  output arrays to be pre-allocated before calling the function.\n"
cv::mulSpectrums: |-
  Performs the per-element multiplication of two Fourier spectrums.
  
  The function `mulSpectrums` performs the per-element multiplication of
  the two CCS-packed or complex matrices that are results of a real or
  complex Fourier transform.
  
  The function, together with :ocvdft and :ocvidft , may be used to
  calculate convolution (pass `conjB=false` ) or correlation (pass
  `conjB=true` ) of two arrays rapidly. When the arrays are complex, they
  are simply multiplied (per element) with an optional conjugation of the
  second-array elements. When the arrays are real, they are assumed to be
  CCS-packed (see :ocvdft for details).
cv::multiply: |
  Calculates the per-element scaled product of two arrays.
  
  The function `multiply` calculates the per-element product of two
  arrays:
  
  $$\texttt{dst} (I)= \texttt{saturate} ( \texttt{scale} \cdot \texttt{src1} (I)  \cdot \texttt{src2} (I))$$
  
  There is also a MatrixExpressions -friendly variant of the first
  function. See :ocvMat::mul .
  
  For a not-per-element matrix product, see :ocvgemm .
  
  **note**
  
  Saturation is not applied when the output array has the depth
  `CV_32S`. You may even get result of an incorrect sign in the case of
  overflow.

cv::mulTransposed: |-
  Calculates the product of a matrix and its transposition.
  
  The function `mulTransposed` calculates the product of `src` and its
  transposition:
  
  $$\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )$$
  
  if `aTa=true` , and
  
  $$\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T$$
  
  otherwise. The function is used to calculate the covariance matrix. With
  zero delta, it can be used as a faster substitute for general matrix
  product `A*B` when `B=A'`
cv::norm: |-
  Calculates an absolute array norm, an absolute difference norm, or a
  relative difference norm.
  
  The functions `norm` calculate an absolute norm of `src1` (when there is
  no `src2` ):
  
  $$norm =  \forkthree{|\texttt{src1}|_{L_{\infty}} =  \max _I | \texttt{src1} (I)|}{if  $\texttt{normType} = \texttt{NORM_INF}$ }
  { | \texttt{src1} | _{L_1} =  \sum _I | \texttt{src1} (I)|}{if  $\texttt{normType} = \texttt{NORM_L1}$ }
  { | \texttt{src1} | _{L_2} =  \sqrt{\sum_I \texttt{src1}(I)^2} }{if  $\texttt{normType} = \texttt{NORM_L2}$ }$$
  
  or an absolute or relative difference norm if `src2` is there:
  
  $$norm =  \forkthree{|\texttt{src1}-\texttt{src2}|_{L_{\infty}} =  \max _I | \texttt{src1} (I) -  \texttt{src2} (I)|}{if  $\texttt{normType} = \texttt{NORM_INF}$ }
  { | \texttt{src1} - \texttt{src2} | _{L_1} =  \sum _I | \texttt{src1} (I) -  \texttt{src2} (I)|}{if  $\texttt{normType} = \texttt{NORM_L1}$ }
  { | \texttt{src1} - \texttt{src2} | _{L_2} =  \sqrt{\sum_I (\texttt{src1}(I) - \texttt{src2}(I))^2} }{if  $\texttt{normType} = \texttt{NORM_L2}$ }$$
  
  or
  
  $$norm =  \forkthree{\frac{|\texttt{src1}-\texttt{src2}|_{L_{\infty}}    }{|\texttt{src2}|_{L_{\infty}} }}{if  $\texttt{normType} = \texttt{NORM_RELATIVE_INF}$ }
  { \frac{|\texttt{src1}-\texttt{src2}|_{L_1} }{|\texttt{src2}|_{L_1}} }{if  $\texttt{normType} = \texttt{NORM_RELATIVE_L1}$ }
  { \frac{|\texttt{src1}-\texttt{src2}|_{L_2} }{|\texttt{src2}|_{L_2}} }{if  $\texttt{normType} = \texttt{NORM_RELATIVE_L2}$ }$$
  
  The functions `norm` return the calculated norm.
  
  When the `mask` parameter is specified and it is not empty, the norm is
  calculated only over the region specified by the mask.
  
  A multi-channel input arrays are treated as a single-channel, that is,
  the results for all channels are combined.
cv::normalize: "Normalizes the norm or value range of an array.\n\n\
  The functions `normalize` scale and shift the input array elements so\n\
  that\n\n\
  $$| \\texttt{dst} | _{L_p}= \\texttt{alpha}$$\n\n\
  (where p=Inf, 1 or 2) when `normType=NORM_INF`, `NORM_L1`, or `NORM_L2`,\n\
  respectively; or so that\n\n\
  $$\\min _I  \\texttt{dst} (I)= \\texttt{alpha} , \\, \\, \\max _I  \\texttt{dst} (I)= \\texttt{beta}$$\n\n\
  when `normType=NORM_MINMAX` (for dense arrays only). The optional mask\n\
  specifies a sub-array to be normalized. This means that the norm or\n\
  min-n-max are calculated over the sub-array, and then this sub-array is\n\
  modified to be normalized. If you want to only use the mask to calculate\n\
  the norm or min-max but modify the whole array, you can use :ocvnorm and\n\
  :ocvMat::convertTo.\n\n\
  In case of sparse matrices, only the non-zero values are analyzed and\n\
  transformed. Because of this, the range transformation for sparse\n\
  matrices is not allowed since it can shift the zero level."
cv::PCA: |
  Principal Component Analysis class.
  
  The class is used to calculate a special basis for a set of vectors. The
  basis will consist of eigenvectors of the covariance matrix calculated
  from the input set of vectors. The class `PCA` can also transform
  vectors to/from the new coordinate space defined by the basis. Usually,
  in this new coordinate system, each vector from the original set (and
  any linear combination of such vectors) can be quite accurately
  approximated by taking its first few components, corresponding to the
  eigenvectors of the largest eigenvalues of the covariance matrix.
  Geometrically it means that you calculate a projection of the vector to
  a subspace formed by a few eigenvectors corresponding to the dominant
  eigenvalues of the covariance matrix. And usually such a projection is
  very close to the original vector. So, you can represent the original
  vector from a high-dimensional space with a much shorter vector
  consisting of the projected vector's coordinates in the subspace. Such a
  transformation is also known as Karhunen-Loeve Transform, or KLT. See
  <http://en.wikipedia.org/wiki/Principal_component_analysis> .
  
  The sample below is the function that takes two matrices. The first
  function stores a set of vectors (a row per vector) that is used to
  calculate PCA. The second function stores another "test" set of vectors
  (a row per vector). First, these vectors are compressed with PCA, then
  reconstructed back, and then the reconstruction error norm is computed
  and printed for each vector. :
  
  ```c++
  PCA compressPCA(InputArray pcaset, int maxComponents,
                  const Mat& testset, OutputArray compressed)
  {
      PCA pca(pcaset, // pass the data
              Mat(), // there is no pre-computed mean vector,
                     // so let the PCA engine to compute it
              CV_PCA_DATA_AS_ROW, // indicate that the vectors
                                  // are stored as matrix rows
                                  // (use CV_PCA_DATA_AS_COL if the vectors are
                                  // the matrix columns)
              maxComponents // specify how many principal components to retain
              );
      // if there is no test data, just return the computed basis, ready-to-use
      if( !testset.data )
          return pca;
      CV_Assert( testset.cols == pcaset.cols );
  
      compressed.create(testset.rows, maxComponents, testset.type());
  
      Mat reconstructed;
      for( int i = 0; i < testset.rows; i++ )
      {
          Mat vec = testset.row(i), coeffs = compressed.row(i);
          // compress the vector, the result will be stored
          // in the i-th row of the output matrix
          pca.project(vec, coeffs);
          // and then reconstruct it
          pca.backProject(coeffs, reconstructed);
          // and measure the error
          printf("%d. diff = %g\n", i, norm(vec, reconstructed, NORM_L2));
      }
      return pca;
  }
  ```

cv::PCA::PCA: |-
  PCA constructors
  
  The default constructor initializes an empty PCA structure. The other
  constructors initialize the structure and call :ocvPCA::operator() .
cv::PCA::operator (): |-
  Performs Principal Component Analysis of the supplied dataset.
  
  The operator performs PCA of the supplied dataset. It is safe to reuse
  the same PCA structure for multiple datasets. That is, if the structure
  has been previously used with another dataset, the existing internal
  data is reclaimed and the new `eigenvalues`, `eigenvectors` , and `mean`
  are allocated and computed.
  
  The computed eigenvalues are sorted from the largest to the smallest and
  the corresponding eigenvectors are stored as `PCA::eigenvectors` rows.
cv::PCA::project: |-
  Projects vector(s) to the principal component subspace.
  
  The methods project one or more vectors to the principal component
  subspace, where each vector projection is represented by coefficients in
  the principal component basis. The first form of the method returns the
  matrix that the second form writes to the result. So the first form can
  be used as a part of expression while the second form can be more
  efficient in a processing loop.
cv::PCA::backProject: |-
  Reconstructs vectors from their PC projections.
  
  The methods are inverse operations to :ocvPCA::project. They take PC
  coordinates of projected vectors and reconstruct the original vectors.
  Unless all the principal components have been retained, the
  reconstructed vectors are different from the originals. But typically,
  the difference is small if the number of components is large enough (but
  still much smaller than the original vector dimensionality). As a
  result, PCA is used.
cv::perspectiveTransform: "Performs the perspective matrix transformation of vectors.\n\n\
  The function `perspectiveTransform` transforms every element of `src` by\n\
  treating it as a 2D or 3D vector, in the following way:\n\n\
  $$(x, y, z)  \\rightarrow (x'/w, y'/w, z'/w)$$\n\n\
  where\n\n\
  $$(x', y', z', w') =  \\texttt{mat} \\cdot \\begin{bmatrix} x & y & z & 1  \\end{bmatrix}$$\n\n\
  and\n\n\
  $$w =  \\fork{w'}{if $w' \\ne 0$}{\\infty}{otherwise}$$\n\n\
  Here a 3D vector transformation is shown. In case of a 2D vector\n\
  transformation, the `z` component is omitted.\n\n\
  **note**\n\n\
  The function transforms a sparse set of 2D or 3D vectors. If you want\n\
  to transform an image using perspective transformation, use\n\
  :ocvwarpPerspective . If you have an inverse problem, that is, you\n\
  want to compute the most probable perspective transformation out of\n\
  several pairs of corresponding points, you can use\n\
  :ocvgetPerspectiveTransform or :ocvfindHomography .\n"
cv::phase: |-
  Calculates the rotation angle of 2D vectors.
  
  The function `phase` calculates the rotation angle of each 2D vector
  that is formed from the corresponding elements of `x` and `y` :
  
  $$\texttt{angle} (I) =  \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))$$
  
  The angle estimation accuracy is about 0.3 degrees. When `x(I)=y(I)=0` ,
  the corresponding `angle(I)` is set to 0.
cv::polarToCart: |-
  Calculates x and y coordinates of 2D vectors from their magnitude and
  angle.
  
  The function `polarToCart` calculates the Cartesian coordinates of each
  2D vector represented by the corresponding elements of `magnitude` and
  `angle` :
  
  $$\begin{array}{l} \texttt{x} (I) =  \texttt{magnitude} (I) \cos ( \texttt{angle} (I)) \ \texttt{y} (I) =  \texttt{magnitude} (I) \sin ( \texttt{angle} (I)) \ \end{array}$$
  
  The relative accuracy of the estimated coordinates is about `1e-6`.
cv::pow: |-
  Raises every array element to a power.
  
  The function `pow` raises every element of the input array to `power` :
  
  $$\texttt{dst} (I) =  \fork{\texttt{src}(I)^power}{if \texttt{power} is integer}{|\texttt{src}(I)|^power}{otherwise}$$
  
  So, for a non-integer power exponent, the absolute values of input array
  elements are used. However, it is possible to get true values for
  negative values using some extra operations. In the example below,
  computing the 5th root of array `src` shows: :
  
  ```c++
  Mat mask = src < 0;
  pow(src, 1./5, dst);
  subtract(Scalar::all(0), dst, dst, mask);
  ```
  
  
  For some values of `power` , such as integer values, 0.5 and -0.5,
  specialized faster algorithms are used.
  
  Special values (NaN, Inf) are not handled.
cv::RNG::RNG: |-
  The constructors
  
  These are the RNG constructors. The first form sets the state to some
  pre-defined value, equal to `2**32-1` in the current implementation. The
  second form sets the state to the specified value. If you passed
  `state=0` , the constructor uses the above default value instead to
  avoid the singular random number sequence, consisting of all zeros.
cv::RNG::next: |-
  Returns the next random number.
  
  The method updates the state using the MWC algorithm and returns the
  next 32-bit random number.
cv::RNG::operator T: |-
  Returns the next random number of the specified type.
  
  Each of the methods updates the state using the MWC algorithm and
  returns the next random number of the specified type. In case of integer
  types, the returned number is from the available value range for the
  specified type. In case of floating-point types, the returned value is
  from `[0,1)` range.
cv::RNG::operator (): |-
  Returns the next random number.
  
  The methods transform the state using the MWC algorithm and return the
  next random number. The first form is equivalent to :ocvRNG::next . The
  second form returns the random number modulo `N` , which means that the
  result is in the range `[0, N)` .
cv::RNG::uniform: |-
  Returns the next random number sampled from the uniform distribution.
  
  The methods transform the state using the MWC algorithm and return the
  next uniformly-distributed random number of the specified type, deduced
  from the input parameter type, from the range `[a, b)` . There is a
  nuance illustrated by the following sample: :
  
  ```c++
  RNG rng;
  
  // always produces 0
  double a = rng.uniform(0, 1);
  
  // produces double from [0, 1)
  double a1 = rng.uniform((double)0, (double)1);
  
  // produces float from [0, 1)
  double b = rng.uniform(0.f, 1.f);
  
  // produces double from [0, 1)
  double c = rng.uniform(0., 1.);
  
  // may cause compiler error because of ambiguity:
  //  RNG::uniform(0, (int)0.999999)? or RNG::uniform((double)0, 0.99999)?
  double d = rng.uniform(0, 0.999999);
  ```
  
  
  The compiler does not take into account the type of the variable to
  which you assign the result of `RNG::uniform` . The only thing that
  matters to the compiler is the type of `a` and `b` parameters. So, if
  you want a floating-point random number, but the range boundaries are
  integer numbers, either put dots in the end, if they are constants, or
  use explicit type cast operators, as in the `a1` initialization above.
cv::RNG::gaussian: |-
  Returns the next random number sampled from the Gaussian distribution.
  
  The method transforms the state using the MWC algorithm and returns the
  next random number from the Gaussian distribution `N(0,sigma)` . That
  is, the mean value of the returned random numbers is zero and the
  standard deviation is the specified `sigma` .
cv::RNG::fill: |-
  Fills arrays with random numbers.
  
  Each of the methods fills the matrix with the random values from the
  specified distribution. As the new numbers are generated, the RNG state
  is updated accordingly. In case of multiple-channel images, every
  channel is filled independently, which means that RNG cannot generate
  samples from the multi-dimensional Gaussian distribution with
  non-diagonal covariance matrix directly. To do that, the method
  generates samples from multi-dimensional standard Gaussian distribution
  with zero mean and identity covariation matrix, and then transforms them
  using :ocvtransform to get samples from the specified Gaussian
  distribution.
cv::randu: |-
  Generates a single uniformly-distributed random number or an array of
  random numbers.
  
  The template functions `randu` generate and return the next
  uniformly-distributed random value of the specified type. `randu<int>()`
  is an equivalent to `(int)theRNG();` , and so on. See :ocvRNG
  description.
  
  The second non-template variant of the function fills the matrix `dst`
  with uniformly-distributed random numbers from the specified range:
  
  $$\texttt{low} _c  \leq \texttt{dst} (I)_c <  \texttt{high} _c$$
cv::randn: |-
  Fills the array with normally distributed random numbers.
  
  The function `randn` fills the matrix `dst` with normally distributed
  random numbers with the specified mean vector and the standard deviation
  matrix. The generated random numbers are clipped to fit the value range
  of the output array data type.
cv::randShuffle: |-
  Shuffles the array elements randomly.
  
  The function `randShuffle` shuffles the specified 1D array by randomly
  choosing pairs of elements and swapping them. The number of such swap
  operations will be `dst.rows*dst.cols*iterFactor` .
cv::reduce: |-
  Reduces a matrix to a vector.
  
  The function `reduce` reduces the matrix to a vector by treating the
  matrix rows/columns as a set of 1D vectors and performing the specified
  operation on the vectors until a single row/column is obtained. For
  example, the function can be used to compute horizontal and vertical
  projections of a raster image. In case of `CV_REDUCE_SUM` and
  `CV_REDUCE_AVG` , the output may have a larger element bit-depth to
  preserve accuracy. And multi-channel arrays are also supported in these
  two reduction modes.
cv::repeat: |-
  Fills the output array with repeated copies of the input array.
  
  The functions :ocvrepeat duplicate the input array one or more times
  along each of the two axes:
  
  $$\texttt{dst} _{ij}= \texttt{src} _{i\mod src.rows, \; j\mod src.cols }$$
  
  The second variant of the function is more convenient to use with
  MatrixExpressions .
cv::scaleAdd: "Calculates the sum of a scaled array and another array.\n\n\
  The function `scaleAdd` is one of the classical primitive linear algebra\n\
  operations, known as `DAXPY` or `SAXPY` in\n\
  [BLAS](http://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms).\n\
  It calculates the sum of a scaled array and another array:\n\n\
  $$\\texttt{dst} (I)= \\texttt{scale} \\cdot \\texttt{src1} (I) +  \\texttt{src2} (I)$$\n\n\
  The function can also be emulated with a matrix expression, for example:\n\
  :\n\n\
  ```c++\n\
  Mat A(3, 3, CV_64F);\n\
  ...\n\
  A.row(0) = A.row(1)*2 + A.row(2);\n\
  ```\n"
cv::setIdentity: |
  Initializes a scaled identity matrix.
  
  The function :ocvsetIdentity initializes a scaled identity matrix:
  
  $$\texttt{mtx} (i,j)= \fork{\texttt{value}}{ if $i=j$}{0}{otherwise}$$
  
  The function can also be emulated using the matrix initializers and the
  matrix expressions: :
  
  ```c++
  Mat A = Mat::eye(4, 3, CV_32F)*5;
  // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]
  ```

cv::solve: |
  Solves one or more linear systems or least-squares problems.
  
  The function `solve` solves a linear system or least-squares problem
  (the latter is possible with SVD or QR methods, or by specifying the
  flag `DECOMP_NORMAL` ):
  
  $$\texttt{dst} =  \arg \min _X | \texttt{src1} \cdot \texttt{X} -  \texttt{src2} |$$
  
  If `DECOMP_LU` or `DECOMP_CHOLESKY` method is used, the function returns
  1 if `src1` (or $\texttt{src1}^T\texttt{src1}$ ) is non-singular.
  Otherwise, it returns 0. In the latter case, `dst` is not valid. Other
  methods find a pseudo-solution in case of a singular left-hand side
  part.
  
  **note**
  
  If you want to find a unity-norm solution of an under-defined singular
  system $\texttt{src1}\cdot\texttt{dst}=0$ , the function `solve` will
  not do the work. Use :ocvSVD::solveZ instead.

cv::solveCubic: |-
  Finds the real roots of a cubic equation.
  
  The function `solveCubic` finds the real roots of a cubic equation:
    if `coeffs` is a 4-element vector:
  
  
  $$\texttt{coeffs} [0] x^3 +  \texttt{coeffs} [1] x^2 +  \texttt{coeffs} [2] x +  \texttt{coeffs} [3] = 0$$
    if `coeffs` is a 3-element vector:
  
  
  $$x^3 +  \texttt{coeffs} [0] x^2 +  \texttt{coeffs} [1] x +  \texttt{coeffs} [2] = 0$$
  
  The roots are stored in the `roots` array.
cv::solvePoly: |-
  Finds the real or complex roots of a polynomial equation.
  
  The function `solvePoly` finds real and complex roots of a polynomial
  equation:
  
  $$\texttt{coeffs} [n] x^{n} +  \texttt{coeffs} [n-1] x^{n-1} + ... +  \texttt{coeffs} [1] x +  \texttt{coeffs} [0] = 0$$
cv::sort: |-
  Sorts each row or each column of a matrix.
  
  The function `sort` sorts each matrix row or each matrix column in
  ascending or descending order. So you should pass two operation flags to
  get desired behaviour. If you want to sort matrix rows or columns
  lexicographically, you can use STL `std::sort` generic function with the
  proper comparison predicate.
cv::sortIdx: |
  Sorts each row or each column of a matrix.
  
  The function `sortIdx` sorts each matrix row or each matrix column in
  the ascending or descending order. So you should pass two operation
  flags to get desired behaviour. Instead of reordering the elements
  themselves, it stores the indices of sorted elements in the output
  array. For example: :
  
  ```c++
  Mat A = Mat::eye(3,3,CV_32F), B;
  sortIdx(A, B, CV_SORT_EVERY_ROW + CV_SORT_ASCENDING);
  // B will probably contain
  // (because of equal elements in A some permutations are possible):
  // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]
  ```

cv::split: |-
  Divides a multi-channel array into several single-channel arrays.
  
  The functions `split` split a multi-channel array into separate
  single-channel arrays:
  
  $$\texttt{mv} [c](I) =  \texttt{src} (I)_c$$
  
  If you need to extract a single channel or do some other sophisticated
  channel permutation, use :ocvmixChannels .
cv::sqrt: |-
  Calculates a square root of array elements.
  
  The functions `sqrt` calculate a square root of each input array
  element. In case of multi-channel arrays, each channel is processed
  independently. The accuracy is approximately the same as of the built-in
  `std::sqrt` .
cv::subtract: |
  Calculates the per-element difference between two arrays or array and a
  scalar.
  
  The function `subtract` calculates:
  
  *
  :   Difference between two arrays, when both input arrays have the
      same size and the same number of channels:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  *
  :   Difference between an array and a scalar, when `src2` is
      constructed from `Scalar` or has the same number of elements as
      `src1.channels()`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1}(I) -  \texttt{src2} ) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  *
  :   Difference between a scalar and an array, when `src1` is
      constructed from `Scalar` or has the same number of elements as
      `src2.channels()`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src1} -  \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  *
  :   The reverse difference between a scalar and an array in the case
      of `SubRS`:
  
  ```c++
  $$\texttt{dst}(I) =  \texttt{saturate} ( \texttt{src2} -  \texttt{src1}(I) ) \quad \texttt{if mask}(I) \ne0$$
  ```
  
  
  where `I` is a multi-dimensional index of array elements. In case of
  multi-channel arrays, each channel is processed independently.
  
  
  The first function in the list above can be replaced with matrix
  expressions: :
  
  ```c++
  dst = src1 - src2;
  dst -= src1; // equivalent to subtract(dst, src1, dst);
  ```
  
  
  The input arrays and the output array can all have the same or different
  depths. For example, you can subtract to 8-bit unsigned arrays and store
  the difference in a 16-bit signed array. Depth of the output array is
  determined by `dtype` parameter. In the second and third cases above, as
  well as in the first case, when `src1.depth() == src2.depth()`, `dtype`
  can be set to the default `-1`. In this case the output array will have
  the same depth as the input array, be it `src1`, `src2` or both.
  
  **note**
  
  Saturation is not applied when the output array has the depth
  `CV_32S`. You may even get result of an incorrect sign in the case of
  overflow.

cv::SVD: |-
  Class for computing Singular Value Decomposition of a floating-point
  matrix. The Singular Value Decomposition is used to solve least-square
  problems, under-determined linear systems, invert matrices, compute
  condition numbers, and so on.
  
  For a faster operation, you can pass `flags=SVD::MODIFY_A|...` to modify
  the decomposed matrix when it is not necessary to preserve it. If you
  want to compute a condition number of a matrix or an absolute value of
  its determinant, you do not need `u` and `vt` . You can pass
  `flags=SVD::NO_UV|...` . Another flag `FULL_UV` indicates that full-size
  `u` and `vt` must be computed, which is not necessary most of the time.
cv::SVD::SVD: "The constructors.\n\n\
  The first constructor initializes an empty `SVD` structure. The second\n\
  constructor initializes an empty `SVD` structure and then calls\n\
  :ocvSVD::operator() ."
cv::SVD::operator (): |-
  Performs SVD of a matrix.
  
  The operator performs the singular value decomposition of the supplied
  matrix. The `u`,`vt` , and the vector of singular values `w` are stored
  in the structure. The same `SVD` structure can be reused many times with
  different matrices. Each time, if needed, the previous `u`,`vt` , and
  `w` are reclaimed and the new matrices are created, which is all handled
  by :ocvMat::create .
cv::SVD::compute: |
  Performs SVD of a matrix
  
  The methods/functions perform SVD of matrix. Unlike `SVD::SVD`
  constructor and `SVD::operator()`, they store the results to the
  user-provided matrices. :
  
  ```c++
  Mat A, w, u, vt;
  SVD::compute(A, w, u, vt);
  ```

cv::SVD::solveZ: |-
  Solves an under-determined singular linear system.
  
  The method finds a unit-length solution `x` of a singular linear system
  `A*x = 0`. Depending on the rank of `A`, there can be no solutions, a
  single solution or an infinite number of solutions. In general, the
  algorithm solves the following problem:
  
  $$dst =  \arg \min _{x:  | x | =1}  | src  \cdot x  |$$
cv::SVD::backSubst: "Performs a singular value back substitution.\n\n\
  The method calculates a back substitution for the specified right-hand\n\
  side:\n\n\
  $$\\texttt{x} =  \\texttt{vt} ^T  \\cdot diag( \\texttt{w} )^{-1}  \\cdot \\texttt{u} ^T  \\cdot \\texttt{rhs} \\sim \\texttt{A} ^{-1}  \\cdot \\texttt{rhs}$$\n\n\
  Using this technique you can either get a very accurate solution of the\n\
  convenient linear system, or the best (in the least-squares terms)\n\
  pseudo-solution of an overdetermined linear system.\n\n\
  **note**\n\n\
  Explicit SVD with the further back substitution only makes sense if\n\
  you need to solve many linear systems with the same left-hand side\n\
  (for example, `src` ). If all you need is to solve a single system\n\
  (possibly with multiple `rhs` immediately available), simply call\n\
  :ocvsolve add pass `DECOMP_SVD` there. It does absolutely the same\n\
  thing.\n"
cv::sum: |-
  Calculates the sum of array elements.
  
  The functions `sum` calculate and return the sum of array elements,
  independently for each channel.
cv::theRNG: |-
  Returns the default random number generator.
  
  The function `theRNG` returns the default random number generator. For
  each thread, there is a separate random number generator, so you can use
  the function safely in multi-thread environments. If you just need to
  get a single random number using this generator or initialize an array,
  you can use :ocvrandu or :ocvrandn instead. But if you are going to
  generate many random numbers inside a loop, it is much faster to use
  this function to retrieve the generator and then use
  `RNG::operator _Tp()` .
cv::trace: |-
  Returns the trace of a matrix.
  
  The function `trace` returns the sum of the diagonal elements of the
  matrix `mtx` .
  
  $$\mathrm{tr} ( \texttt{mtx} ) =  \sum _i  \texttt{mtx} (i,i)$$
cv::transform: |-
  Performs the matrix transformation of every array element.
  
  The function `transform` performs the matrix transformation of every
  element of the array `src` and stores the results in `dst` :
  
  $$\texttt{dst} (I) =  \texttt{m} \cdot \texttt{src} (I)$$
  
  (when `m.cols=src.channels()` ), or
  
  $$\texttt{dst} (I) =  \texttt{m} \cdot [ \texttt{src} (I); 1]$$
  
  (when `m.cols=src.channels()+1` )
  
  Every element of the `N` -channel array `src` is interpreted as `N`
  -element vector that is transformed using the `M x N` or `M x (N+1)`
  matrix `m` to `M`-element vector - the corresponding element of the
  output array `dst` .
  
  The function may be used for geometrical transformation of `N`
  -dimensional points, arbitrary linear color space transformation (such
  as various kinds of RGB to YUV transforms), shuffling the image
  channels, and so forth.
cv::transpose: |
  Transposes a matrix.
  
  The function :ocvtranspose transposes the matrix `src` :
  
  $$\texttt{dst} (i,j) =  \texttt{src} (j,i)$$
  
  **note**
  
  No complex conjugation is done in case of a complex matrix. It it
  should be done separately if needed.

cv::borderInterpolate: |-
  Computes the source location of an extrapolated pixel.
  
  The function computes and returns the coordinate of a donor pixel
  corresponding to the specified extrapolated pixel when using the
  specified extrapolation border mode. For example, if you use
  `BORDER_WRAP` mode in the horizontal direction, `BORDER_REFLECT_101` in
  the vertical direction and want to compute value of the "virtual" pixel
  `Point(-5, 100)` in a floating-point image `img` , it looks like: :
  
  ```c++
  float val = img.at<float>(borderInterpolate(100, img.rows, BORDER_REFLECT_101),
                            borderInterpolate(-5, img.cols, BORDER_WRAP));
  ```
  
  
  Normally, the function is not called directly. It is used inside
  :ocvFilterEngine and :ocvcopyMakeBorder to compute tables for quick
  extrapolation.
cv::copyMakeBorder: |
  Forms a border around an image.
  
  The function copies the source image into the middle of the destination
  image. The areas to the left, to the right, above and below the copied
  source image will be filled with extrapolated pixels. This is not what
  :ocvFilterEngine or filtering functions based on it do (they extrapolate
  pixels on-fly), but what other more complex functions, including your
  own, may do to simplify image boundary handling.
  
  The function supports the mode when `src` is already in the middle of
  `dst` . In this case, the function does not copy `src` itself but simply
  constructs the border, for example: :
  
  ```c++
  // let border be the same in all directions
  int border=2;
  // constructs a larger image to fit both the image and the border
  Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());
  // select the middle part of it w/o copying data
  Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));
  // convert image from RGB to grayscale
  cvtColor(rgb, gray, COLOR_RGB2GRAY);
  // form a border in-place
  copyMakeBorder(gray, gray_buf, border, border,
                 border, border, BORDER_REPLICATE);
  // now do some custom filtering ...
  ...
  ```
  
  
  **note**
  
  When the source image is a part (ROI) of a bigger image, the function
  will try to use the pixels outside of the ROI to form a border. To
  disable this feature and always do extrapolation, as if `src` was not
  a ROI, use `borderType | BORDER_ISOLATED`.

cv::ocl::oclMat::convertTo: |-
  Returns void
  
  The method converts source pixel values to the target datatype. saturate
  cast is applied in the end to avoid possible overflows. Supports
  CV_8UC1, CV_8UC4, CV_32SC1, CV_32SC4, CV_32FC1, CV_32FC4.
cv::ocl::oclMat::copyTo: |-
  Returns void
  
  Copies the matrix to another one. Supports CV_8UC1, CV_8UC4,
  CV_32SC1, CV_32SC4, CV_32FC1, CV_32FC4
cv::ocl::oclMat::setTo: |-
  Returns oclMat
  
  Sets all or some of the array elements to the specified value. This is
  the advanced variant of Mat::operator=(const Scalar s) operator.
  Supports CV_8UC1, CV_8UC4, CV_32SC1, CV_32SC4, CV_32FC1, CV_32FC4.
cv::ocl::absdiff: |-
  Returns void
  
  Computes per-element absolute difference between two arrays or between
  array and a scalar. Supports all data types except CV_8S.
cv::ocl::add: |-
  Returns void
  
  Computes per-element additon between two arrays or between array and a
  scalar. Supports all data types except CV_8S.
cv::ocl::subtract: |-
  Returns void
  
  Computes per-element subtract between two arrays or between array and a
  scalar. Supports all data types except CV_8S.
cv::ocl::multiply: |-
  Returns void
  
  Computes per-element multiply between two arrays or between array and a
  scalar. Supports all data types except CV_8S.
cv::ocl::divide: |-
  Returns void
  
  Computes per-element divide between two arrays or between array and a
  scalar. Supports all data types except CV_8S.
cv::ocl::bitwise_and: |-
  Returns void
  
  Computes per-element bitwise_and between two arrays or between array
  and a scalar. Supports all data types except CV_8S.
cv::ocl::bitwise_or: |-
  Returns void
  
  Computes per-element bitwise_or between two arrays or between array and
  a scalar. Supports all data types except CV_8S.
cv::ocl::bitwise_xor: |-
  Returns void
  
  Computes per-element bitwise_xor between two arrays or between array
  and a scalar. Supports all data types except CV_8S.
cv::ocl::bitwise_not: |-
  Returns void
  
  The functions bitwise not compute per-element bit-wise inversion of the
  source array:. Supports all data types except CV_8S.
cv::ocl::cartToPolar: |-
  Returns void
  
  Calculates the magnitude and angle of 2d vectors. Supports only CV_32F
  and CV_64F data types.
cv::ocl::polarToCart: |-
  Returns void
  
  The function polarToCart computes the cartesian coordinates of each 2D
  vector represented by the corresponding elements of magnitude and angle.
  Supports only CV_32F and CV_64F data types.
cv::ocl::compare: |-
  Returns void
  
  Performs per-element comparison of two arrays or an array and scalar
  value. Supports all the 1 channel data types except CV_8S.
cv::ocl::exp: |-
  Returns void
  
  The function exp calculates the exponent of every element of the input
  array. Supports only CV_32FC1 data type.
cv::ocl::log: |-
  Returns void
  
  The function log calculates the log of every element of the input array.
  Supports only CV_32FC1 data type.
cv::ocl::LUT: |-
  Returns void
  
  Performs a look-up table transform of an array. Supports only CV_8UC1
  and CV_8UC4 data type.
cv::ocl::magnitude: |-
  Returns void
  
  The function magnitude calculates magnitude of 2D vectors formed from
  the corresponding elements of x and y arrays. Supports only CV_32F and
  CV_64F data type.
cv::ocl::flip: |-
  Returns void
  
  The function flip flips the array in one of three different ways (row
  and column indices are 0-based). Supports all data types.
cv::ocl::meanStdDev: |-
  Returns void
  
  The functions meanStdDev compute the mean and the standard deviation M
  of array elements, independently for each channel, and return it via the
  output parameters. Supports all data types except CV_32F,CV_64F
cv::ocl::merge: |-
  Returns void
  
  Composes a multi-channel array from several single-channel arrays.
  Supports all data types.
cv::ocl::split: |-
  Returns void
  
  The functions split split multi-channel array into separate
  single-channel arrays. Supports all data types.
cv::ocl::norm: |-
  Returns the calculated norm
  
  Calculates absolute array norm, absolute difference norm, or relative
  difference norm. Supports only CV_8UC1 data type.
cv::ocl::phase: |-
  Returns void
  
  The function phase computes the rotation angle of each 2D vector that is
  formed from the corresponding elements of x and y. Supports only
  CV_32FC1 and CV_64FC1 data type.
cv::ocl::pow: |-
  Returns void
  
  The function pow raises every element of the input array to p. Supports
  only CV_32FC1 and CV_64FC1 data type.
cv::ocl::transpose: |-
  Returns void
  
  Transposes a matrix. Supports 8UC1, 8UC4, 8SC4, 16UC2, 16SC2, 32SC1 and
  32FC1 data types.
cv::ocl::dft: |-
  Performs a forward or inverse discrete Fourier transform (1D or 2D) of
  the floating point matrix.
  
  Use to handle real matrices ( `CV32FC1` ) and complex matrices in the
  interleaved format ( `CV32FC2` ).
  
  The dft_size must be powers of 2, 3 and 5. Real to complex dft output
  is not the same with cpu version. real to complex and complex to real
  does not support DFT_ROWS
cv::ocl::gemm: Performs generalized matrix multiplication.
cv::gpu::BroxOpticalFlow: |
  Class computing the optical flow for two images using Brox et al Optical
  Flow algorithm ([Brox2004]_). :
  
  ```c++
  class BroxOpticalFlow
  {
  public:
      BroxOpticalFlow(float alpha_, float gamma_, float scale_factor_, int inner_iterations_, int outer_iterations_, int solver_iterations_);
  
      //! Compute optical flow
      //! frame0 - source frame (supports only CV_32FC1 type)
      //! frame1 - frame to track (with the same size and type as frame0)
      //! u      - flow horizontal component (along x axis)
      //! v      - flow vertical component (along y axis)
      void operator ()(const GpuMat& frame0, const GpuMat& frame1, GpuMat& u, GpuMat& v, Stream& stream = Stream::Null());
  
      //! flow smoothness
      float alpha;
  
      //! gradient constancy importance
      float gamma;
  
      //! pyramid scale factor
      float scale_factor;
  
      //! number of lagged non-linearity iterations (inner loop)
      int inner_iterations;
  
      //! number of warping iterations (number of pyramid levels)
      int outer_iterations;
  
      //! number of linear system solver iterations
      int solver_iterations;
  
      GpuMat buf;
  };
  ```

cv::gpu::FarnebackOpticalFlow: "Class computing a dense optical flow using the Gunnar Farneback\xE2\x80\x99s\n\
  algorithm. :\n\n\
  ```c++\n\
  class CV_EXPORTS FarnebackOpticalFlow\n\
  {\n\
  public:\n    FarnebackOpticalFlow()\n    {\n        numLevels = 5;\n        pyrScale = 0.5;\n        fastPyramids = false;\n        winSize = 13;\n        numIters = 10;\n        polyN = 5;\n        polySigma = 1.1;\n        flags = 0;\n    }\n\n    int numLevels;\n    double pyrScale;\n    bool fastPyramids;\n    int winSize;\n    int numIters;\n    int polyN;\n    double polySigma;\n    int flags;\n\n    void operator ()(const GpuMat &frame0, const GpuMat &frame1, GpuMat &flowx, GpuMat &flowy, Stream &s = Stream::Null());\n\n    void releaseMemory();\n\n\
  private:\n    /* hidden */\n\
  };\n\
  ```\n"
cv::gpu::FarnebackOpticalFlow::operator (): "Computes a dense optical flow using the Gunnar Farneback\xE2\x80\x99s algorithm."
cv::gpu::FarnebackOpticalFlow::releaseMemory: Releases unused auxiliary memory buffers.
cv::gpu::PyrLKOpticalFlow: |-
  Class used for calculating an optical flow. :
  
  ```c++
  class PyrLKOpticalFlow
  {
  public:
      PyrLKOpticalFlow();
  
      void sparse(const GpuMat& prevImg, const GpuMat& nextImg, const GpuMat& prevPts, GpuMat& nextPts,
          GpuMat& status, GpuMat* err = 0);
  
      void dense(const GpuMat& prevImg, const GpuMat& nextImg, GpuMat& u, GpuMat& v, GpuMat* err = 0);
  
      Size winSize;
      int maxLevel;
      int iters;
      bool useInitialFlow;
  
      void releaseMemory();
  };
  ```
  
  
  The class can calculate an optical flow for a sparse feature set or
  dense optical flow using the iterative Lucas-Kanade method with
  pyramids.
cv::gpu::PyrLKOpticalFlow::sparse: Calculate an optical flow for a sparse feature set.
cv::gpu::PyrLKOpticalFlow::dense: Calculate dense optical flow.
cv::gpu::PyrLKOpticalFlow::releaseMemory: Releases inner buffers memory.
cv::gpu::interpolateFrames: |-
  Interpolates frames (images) using provided optical flow (displacement
  field).
cv::CvSubdiv2D: |-
  Planar subdivision.
  
  ```c++
  #define CV_SUBDIV2D_FIELDS()    \
      CV_GRAPH_FIELDS()           \
      int  quad_edges;            \
      int  is_geometry_valid;     \
      CvSubdiv2DEdge recent_edge; \
      CvPoint2D32f  topleft;      \
      CvPoint2D32f  bottomright;
  
  typedef struct CvSubdiv2D
  {
      CV_SUBDIV2D_FIELDS()
  }
  CvSubdiv2D;
  ```
  
  
  Planar subdivision is the subdivision of a plane into a set of
  non-overlapped regions (facets) that cover the whole plane. The above
  structure describes a subdivision built on a 2D point set, where the
  points are linked together and form a planar graph, which, together with
  a few edges connecting the exterior subdivision points (namely, convex
  hull points) with infinity, subdivides a plane into facets by its edges.
  
  For every subdivision, there is a dual subdivision in which facets and
  points (subdivision vertices) swap their roles. This means that a facet
  is treated as a vertex (called a virtual point below) of the dual
  subdivision and the original subdivision vertices become facets. In the
  figure below, the original subdivision is marked with solid lines and
  dual subdivision -with dotted lines.
  
  ![image](pics/subdiv.png)
  
  OpenCV subdivides a plane into triangles using the Delaunay's algorithm.
  Subdivision is built iteratively starting from a dummy triangle that
  includes all the subdivision points for sure. In this case, the dual
  subdivision is a Voronoi diagram of the input 2D point set. The
  subdivisions can be used for the 3D piece-wise transformation of a
  plane, morphing, fast location of points on the plane, building special
  graphs (such as NNG,RNG), and so forth.
cv::CvQuadEdge2D: |-
  Quad-edge of a planar subdivision.
  
  ```c++
  /* one of edges within quad-edge, lower 2 bits is index (0..3)
     and upper bits are quad-edge pointer */
  typedef long CvSubdiv2DEdge;
  
  /* quad-edge structure fields */
  #define CV_QUADEDGE2D_FIELDS()     \
      int flags;                     \
      struct CvSubdiv2DPoint* pt[4]; \
      CvSubdiv2DEdge  next[4];
  
  typedef struct CvQuadEdge2D
  {
      CV_QUADEDGE2D_FIELDS()
  }
  CvQuadEdge2D;
  ```
  
  
  Quad-edge is a basic element of a subdivision containing four edges (e,
  eRot, reversed e, and reversed eRot):
  
  ![image](pics/quadedge.png)
cv::CvSubdiv2DPoint: |
  Point of an original or dual subdivision.
  
  ```c++
  #define CV_SUBDIV2D_POINT_FIELDS()\
      int            flags;      \
      CvSubdiv2DEdge first;      \
      CvPoint2D32f   pt;         \
      int id;
  
  #define CV_SUBDIV2D_VIRTUAL_POINT_FLAG (1 << 30)
  
  typedef struct CvSubdiv2DPoint
  {
      CV_SUBDIV2D_POINT_FIELDS()
  }
  CvSubdiv2DPoint;
  ```
  
    id
  :   This integer can be used to index auxiliary data associated with
      each vertex of the planar subdivision.

cv::CalcSubdivVoronoi2D: |-
  Calculates the coordinates of the Voronoi diagram cells.
  
  The function calculates the coordinates of virtual points. All virtual
  points corresponding to a vertex of the original subdivision form (when
  connected together) a boundary of the Voronoi cell at that point.
cv::ClearSubdivVoronoi2D: |-
  Removes all virtual points.
  
  The function removes all of the virtual points. It is called internally
  in :ocvCalcSubdivVoronoi2D if the subdivision was modified after the
  previous call to the function.
cv::CreateSubdivDelaunay2D: |-
  Creates an empty Delaunay triangulation.
  
  The function creates an empty Delaunay subdivision where 2D points can
  be added using the function :ocvSubdivDelaunay2DInsert . All of the
  points to be added must be within the specified rectangle, otherwise a
  runtime error is raised.
  
  Note that the triangulation is a single large triangle that covers the
  given rectangle. Hence the three vertices of this triangle are outside
  the rectangle `rect` .
cv::FindNearestPoint2D: "Finds the subdivision vertex closest to the given point.\n\n\
  The function is another function that locates the input point within the\n\
  subdivision. It finds the subdivision vertex that is the closest to the\n\
  input point. It is not necessarily one of vertices of the facet\n\
  containing the input point, though the facet (located using\n\
  :ocvSubdiv2DLocate ) is used as a starting point. The function returns a\n\
  pointer to the found subdivision vertex."
cv::Subdiv2DEdgeDst: |-
  Returns the edge destination.
  
  The function returns the edge destination. The returned pointer may be
  NULL if the edge is from a dual subdivision and the virtual point
  coordinates are not calculated yet. The virtual points can be calculated
  using the function :ocvCalcSubdivVoronoi2D.
cv::Subdiv2DGetEdge: |-
  Returns one of the edges related to the given edge.
  
  ![image](pics/quadedge.png)
  
  The function returns one of the edges related to the input edge.
cv::Subdiv2DNextEdge: |-
  Returns next edge around the edge origin.
  
  The function returns the next edge around the edge origin: `eOnext` on
  the picture above if `e` is the input edge).
cv::Subdiv2DLocate: |-
  Returns the location of a point within a Delaunay triangulation.
  
  The function locates the input point within the subdivision. There are
  five cases:
  
  *
  :   The point falls into some facet. The function returns
      `CV_PTLOC_INSIDE` and `*edge` will contain one of edges of the
      facet.
  
  *
  :   The point falls onto the edge. The function returns
      `CV_PTLOC_ON_EDGE` and `*edge` will contain this edge.
  
  *
  :   The point coincides with one of the subdivision vertices. The
      function returns `CV_PTLOC_VERTEX` and `*vertex` will contain a
      pointer to the vertex.
  
  *
  :   The point is outside the subdivision reference rectangle. The
      function returns `CV_PTLOC_OUTSIDE_RECT` and no pointers are filled.
  
  *
  :   One of input arguments is invalid. A runtime error is raised or, if
      silent or "parent" error processing mode is selected,
      `CV_PTLOC_ERROR` is returnd.
cv::Subdiv2DRotateEdge: |-
  Returns another edge of the same quad-edge.
  
  The function returns one of the edges of the same quad-edge as the input
  edge.
cv::SubdivDelaunay2DInsert: |-
  Inserts a single point into a Delaunay triangulation.
  
  The function inserts a single point into a subdivision and modifies the
  subdivision topology appropriately. If a point with the same coordinates
  exists already, no new point is added. The function returns a pointer to
  the allocated point. No virtual point coordinates are calculated at this
  stage.
cv::Qt New Functions: |
  ![image](pics/qtgui.png)
  
  This figure explains new functionality implemented with Qt* GUI. The
  new GUI provides a statusbar, a toolbar, and a control panel. The
  control panel can have trackbars and buttonbars attached to it. If you
  cannot see the control panel, press Ctrl+P or right-click any Qt window
  and select **Display properties window**.
  
  *
  :   To attach a trackbar, the window name parameter must be NULL.
  
  *
  :   To attach a buttonbar, a button must be created. If the last bar
      attached to the control panel is a buttonbar, the new button is
      added to the right of the last button. If the last bar attached to
      the control panel is a trackbar, or the control panel is empty, a
      new buttonbar is created. Then, a new button is attached to it.
  
  See below the example used to generate the figure: :
  
  ```c++
  int main(int argc, char *argv[])
      int value = 50;
      int value2 = 0;
  
      cvNamedWindow("main1",CV_WINDOW_NORMAL);
      cvNamedWindow("main2",CV_WINDOW_AUTOSIZE | CV_GUI_NORMAL);
  
      cvCreateTrackbar( "track1", "main1", &value, 255,  NULL);//OK tested
      char* nameb1 = "button1";
      char* nameb2 = "button2";
      cvCreateButton(nameb1,callbackButton,nameb1,CV_CHECKBOX,1);
  
      cvCreateButton(nameb2,callbackButton,nameb2,CV_CHECKBOX,0);
      cvCreateTrackbar( "track2", NULL, &value2, 255, NULL);
      cvCreateButton("button5",callbackButton1,NULL,CV_RADIOBOX,0);
      cvCreateButton("button6",callbackButton2,NULL,CV_RADIOBOX,1);
  
      cvSetMouseCallback( "main2",on_mouse,NULL );
  
      IplImage* img1 = cvLoadImage("files/flower.jpg");
      IplImage* img2 = cvCreateImage(cvGetSize(img1),8,3);
      CvCapture* video = cvCaptureFromFile("files/hockey.avi");
      IplImage* img3 = cvCreateImage(cvGetSize(cvQueryFrame(video)),8,3);
  
      while(cvWaitKey(33) != 27)
      {
          cvAddS(img1,cvScalarAll(value),img2);
          cvAddS(cvQueryFrame(video),cvScalarAll(value2),img3);
          cvShowImage("main1",img2);
          cvShowImage("main2",img3);
      }
  
      cvDestroyAllWindows();
      cvReleaseImage(&img1);
      cvReleaseImage(&img2);
      cvReleaseImage(&img3);
      cvReleaseCapture(&video);
      return 0;
  }
  ```

cv::setWindowProperty: |-
  Changes parameters of a window dynamically.
  
  The function `setWindowProperty` enables changing properties of a
  window.
cv::getWindowProperty: |-
  Provides parameters of a window.
  
  See :ocvsetWindowProperty to know the meaning of the returned values.
  
  The function `getWindowProperty` returns properties of a window.
cv::fontQt: |
  Creates the font to draw a text on an image.
  
  The function `fontQt` creates a `CvFont` object. This `CvFont` is not
  compatible with `putText` .
  
  A basic usage of this function is the following: :
  
  ```c++
  CvFont font = fontQt(''Times'');
  addText( img1, ``Hello World !'', Point(50,50), font);
  ```

cv::addText: |-
  Creates the font to draw a text on an image.
  
  The function `addText` draws *text* on an image *img* using a specific
  font *font* (see example :ocvfontQt )
cv::displayOverlay: |-
  Displays a text on a window image as an overlay for a specified
  duration.
  
  The function `displayOverlay` displays useful information/tips on top of
  the window for a certain amount of time *delayms*. The function does not
  modify the image, displayed in the window, that is, after the specified
  delay the original content of the window is restored.
cv::displayStatusBar: |-
  Displays a text on the window statusbar during the specified period of
  time.
  
  The function `displayOverlay` displays useful information/tips on top of
  the window for a certain amount of time *delayms* . This information is
  displayed on the window statusbar (the window must be created with the
  `CV_GUI_EXPANDED` flags).
cv::setOpenGlDrawCallback: |
  Sets a callback function to be called to draw on top of displayed image.
  
  The function `setOpenGlDrawCallback` can be used to draw 3D data on the
  window. See the example of callback function below: :
  
  ```c++
  void on_opengl(void* param)
  {
      glLoadIdentity();
  
      glTranslated(0.0, 0.0, -1.0);
  
      glRotatef( 55, 1, 0, 0 );
      glRotatef( 45, 0, 1, 0 );
      glRotatef( 0, 0, 0, 1 );
  
      static const int coords[6][4][3] = {
          { { +1, -1, -1 }, { -1, -1, -1 }, { -1, +1, -1 }, { +1, +1, -1 } },
          { { +1, +1, -1 }, { -1, +1, -1 }, { -1, +1, +1 }, { +1, +1, +1 } },
          { { +1, -1, +1 }, { +1, -1, -1 }, { +1, +1, -1 }, { +1, +1, +1 } },
          { { -1, -1, -1 }, { -1, -1, +1 }, { -1, +1, +1 }, { -1, +1, -1 } },
          { { +1, -1, +1 }, { -1, -1, +1 }, { -1, -1, -1 }, { +1, -1, -1 } },
          { { -1, -1, +1 }, { +1, -1, +1 }, { +1, +1, +1 }, { -1, +1, +1 } }
      };
  
      for (int i = 0; i < 6; ++i) {
                  glColor3ub( i*20, 100+i*10, i*42 );
                  glBegin(GL_QUADS);
                  for (int j = 0; j < 4; ++j) {
                          glVertex3d(0.2 * coords[i][j][0], 0.2 * coords[i][j][1], 0.2 * coords[i][j][2]);
                  }
                  glEnd();
      }
  }
  ```

cv::saveWindowParameters: |-
  Saves parameters of the specified window.
  
  The function `saveWindowParameters` saves size, location, flags,
  trackbars value, zoom and panning location of the window `window_name` .
cv::loadWindowParameters: |-
  Loads parameters of the specified window.
  
  The function `loadWindowParameters` loads size, location, flags,
  trackbars value, zoom and panning location of the window `window_name` .
cv::createButton: |
  Attaches a button to the control panel.
  
  The function `createButton` attaches a button to the control panel. Each
  button is added to a buttonbar to the right of the last button. A new
  buttonbar is created if nothing was attached to the control panel
  before, or if the last element attached to the control panel was a
  trackbar.
  
  See below various examples of the `createButton` function call: :
  
  ```c++
  createButton(NULL,callbackButton);//create a push button "button 0", that will call callbackButton.
  createButton("button2",callbackButton,NULL,CV_CHECKBOX,0);
  createButton("button3",callbackButton,&value);
  createButton("button5",callbackButton1,NULL,CV_RADIOBOX);
  createButton("button6",callbackButton2,NULL,CV_PUSH_BUTTON,1);
  ```

cv::Random Trees: |
  Random trees have been introduced by Leo Breiman and Adele Cutler:
  <http://www.stat.berkeley.edu/users/breiman/RandomForests/> . The
  algorithm can deal with both classification and regression problems.
  Random trees is a collection (ensemble) of tree predictors that is
  called *forest* further in this section (the term has been also
  introduced by L. Breiman). The classification works as follows: the
  random trees classifier takes the input feature vector, classifies it
  with every tree in the forest, and outputs the class label that received
  the majority of "votes". In case of a regression, the classifier
  response is the average of the responses over all the trees in the
  forest.
  
  All the trees are trained with the same parameters but on different
  training sets. These sets are generated from the original training set
  using the bootstrap procedure: for each training set, you randomly
  select the same number of vectors as in the original set ( `=N` ). The
  vectors are chosen with replacement. That is, some vectors will occur
  more than once and some will be absent. At each node of each trained
  tree, not all the variables are used to find the best split, but a
  random subset of them. With each node a new subset is generated.
  However, its size is fixed for all the nodes and all the trees. It is a
  training parameter set to $\sqrt{number_of_variables}$ by default.
  None of the built trees are pruned.
  
  In random trees there is no need for any accuracy estimation procedures,
  such as cross-validation or bootstrap, or a separate test set to get an
  estimate of the training error. The error is estimated internally during
  the training. When the training set for the current tree is drawn by
  sampling with replacement, some vectors are left out (so-called *oob
  (out-of-bag) data* ). The size of oob data is about `N/3` . The
  classification error is estimated by using this oob-data as follows:
  
  #.
  :   Get a prediction for each vector, which is oob relative to the i-th
      tree, using the very i-th tree.
  
  #.
  :   After all the trees have been trained, for each vector that has ever
      been oob, find the class-*winner* for it (the class that has got the
      majority of votes in the trees where the vector was oob) and compare
      it to the ground-truth response.
  
  #.
  :   Compute the classification error estimate as a ratio of the number
      of misclassified oob vectors to all the vectors in the original
      data. In case of regression, the oob-error is computed as the
      squared error for oob vectors difference divided by the total number
      of vectors.
  
  For the random trees usage example, please, see letter_recog.cpp sample
  in OpenCV distribution.
  
  **References:**
  
    *Machine Learning*, Wald I, July 2002.
  <http://stat-www.berkeley.edu/users/breiman/wald2002-1.pdf>
  
  
    *Looking Inside the Black Box*, Wald II, July 2002.
  <http://stat-www.berkeley.edu/users/breiman/wald2002-2.pdf>
  
  
    *Software for the Masses*, Wald III, July 2002.
  <http://stat-www.berkeley.edu/users/breiman/wald2002-3.pdf>
  
  
    And other articles from the web site
  <http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm>

cv::CvRTParams: "\n\n\
  The set of training parameters for the forest is a superset of the\n\
  training parameters for a single tree. However, random trees do not need\n\
  all the functionality/features of decision trees. Most noticeably, the\n\
  trees are not pruned, so the cross-validation parameters are not used."
"cv::CvRTParams::CvRTParams:": |
  The constructors.
  
  For meaning of other parameters see :ocvCvDTreeParams::CvDTreeParams.
  
  The default constructor sets all parameters to default values which are
  different from default values of :ocvCvDTreeParams:
  
  ```c++
  CvRTParams::CvRTParams() : CvDTreeParams( 5, 10, 0, false, 10, 0, false, false, 0 ),
      calc_var_importance(false), nactive_vars(0)
  {
      term_crit = cvTermCriteria( CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 50, 0.1 );
  }
  ```

cv::CvRTrees::train: "Trains the Random Trees model.\n\n\
  The method :ocvCvRTrees::train is very similar to the method\n\
  :ocvCvDTree::train and follows the generic method :ocvCvStatModel::train\n\
  conventions. All the parameters specific to the algorithm training are\n\
  passed as a :ocvCvRTParams instance. The estimate of the training error\n\
  (`oob-error`) is stored in the protected class member `oob_error`.\n\n\
  The function is parallelized with the TBB library."
cv::CvRTrees::predict: "Predicts the output for an input sample.\n\n\
  The input parameters of the prediction method are the same as in\n\
  :ocvCvDTree::predict but the return value type is different. This method\n\
  returns the cumulative result from all the trees in the forest (the\n\
  class that receives the majority of voices, or the mean of the\n\
  regression function estimates)."
cv::CvRTrees::predict_prob: |-
  Returns a fuzzy-predicted class label.
  
  The function works for binary classification problems only. It returns
  the number between 0 and 1. This number represents probability or
  confidence of the sample belonging to the second class. It is calculated
  as the proportion of decision trees that classified the sample to the
  second class.
cv::CvRTrees::getVarImportance: |-
  Returns the variable importance array.
  
  The method returns the variable importance vector, computed at the
  training stage when `CvRTParams::calc_var_importance` is set to true. If
  this flag was set to false, the `NULL` pointer is returned. This differs
  from the decision trees where variable importance can be computed
  anytime after the training.
cv::CvRTrees::get_proximity: |-
  Retrieves the proximity measure between two training samples.
  
  The method returns proximity measure between any two samples. This is a
  ratio of those trees in the ensemble, in which the samples fall into the
  same leaf node, to the total number of the trees.
cv::CvRTrees::calc_error: |-
  Returns error of the random forest.
  
  The method is identical to :ocvCvDTree::calc_error but uses the random
  forest as predictor.
cv::CvRTrees::get_train_error: |-
  Returns the train error.
  
  The method works for classification problems only. It returns the
  proportion of incorrectly classified train samples.
cv::CvRTrees::get_rng: Returns the state of the used random number generator.
cv::CvRTrees::get_tree_count: Returns the number of trees in the constructed random forest.
cv::CvRTrees::get_tree: Returns the specific decision tree in the constructed random forest.
cv::imdecode: |
  Reads an image from a buffer in memory.
  
  The function reads an image from the specified buffer in the memory. If
  the buffer is too short or contains invalid data, the empty matrix/image
  is returned.
  
  See :ocvimread for the list of supported formats and flags description.
  
  **note**
  
  In the case of color images, the decoded images will have the channels
  stored in `B G R` order.

cv::imencode: |
  Encodes an image into a memory buffer.
  
  The function compresses the image and stores it in the memory buffer
  that is resized to fit the result. See :ocvimwrite for the list of
  supported formats and flags description.
  
  **note**
  
  `cvEncodeImage` returns single-row matrix of type `CV_8UC1` that
  contains encoded image as array of bytes.

cv::imread: |
  Loads an image from a file.
  
  The function `imread` loads an image from the specified file and returns
  it. If the image cannot be read (because of missing file, improper
  permissions, unsupported or invalid format), the function returns an
  empty matrix ( `Mat::data==NULL` ). Currently, the following file
  formats are supported:
  
    Windows bitmaps - `*.bmp, *.dib` (always supported)
  
  
    JPEG files - `*.jpeg, *.jpg, *.jpe` (see the *Notes* section)
  
  
    JPEG 2000 files - `*.jp2` (see the *Notes* section)
  
  
    Portable Network Graphics - `*.png` (see the *Notes* section)
  
  
    WebP - `*.webp` (see the *Notes* section)
  
  
    Portable image format - `*.pbm, *.pgm, *.ppm` (always supported)
  
  
    Sun rasters - `*.sr, *.ras` (always supported)
  
  
    TIFF files - `*.tiff, *.tif` (see the *Notes* section)
  
  
  **note**
  
    The function determines the type of an image by the content, not
  by the file extension.
  
  
    On Microsoft Windows* OS and MacOSX*, the codecs shipped with an
  OpenCV image (libjpeg, libpng, libtiff, and libjasper) are used by
  default. So, OpenCV can always read JPEGs, PNGs, and TIFFs. On
  MacOSX, there is also an option to use native MacOSX image
  readers. But beware that currently these native image loaders give
  images with different pixel values because of the color management
  embedded into MacOSX.
  
  
    On Linux*, BSD flavors and other Unix-like open-source operating
  systems, OpenCV looks for codecs supplied with an OS image.
  Install the relevant packages (do not forget the development
  files, for example, "libjpeg-dev", in Debian* and Ubuntu*) to
  get the codec support or turn on the `OPENCV_BUILD_3RDPARTY_LIBS`
  flag in CMake.
  
  
  **note**
  
  In the case of color images, the decoded images will have the channels
  stored in `B G R` order.

cv::imwrite: |
  Saves an image to a specified file.
  
  The function `imwrite` saves the image to the specified file. The image
  format is chosen based on the `filename` extension (see :ocvimread for
  the list of extensions). Only 8-bit (or 16-bit unsigned (`CV_16U`) in
  case of PNG, JPEG 2000, and TIFF) single-channel or 3-channel (with
  'BGR' channel order) images can be saved using this function. If the
  format, depth or channel order is different, use :ocvMat::convertTo ,
  and :ocvcvtColor to convert it before saving. Or, use the universal XML
  I/O functions to save the image to XML or YAML format.
  
  It is possible to store PNG images with an alpha channel using this
  function. To do this, create 8-bit (or 16-bit) 4-channel image BGRA,
  where the alpha channel goes last. Fully transparent pixels should have
  alpha set to 0, fully opaque pixels should have alpha set to 255/65535.
  The sample below shows how to create such a BGRA image and store to PNG
  file. It also demonstrates how to set custom compression parameters :
  
  ```c++
  #include <vector>
  #include <stdio.h>
  #include <opencv2/opencv.hpp>
  
  using namespace cv;
  using namespace std;
  
  void createAlphaMat(Mat &mat)
  {
      for (int i = 0; i < mat.rows; ++i) {
          for (int j = 0; j < mat.cols; ++j) {
              Vec4b& rgba = mat.at<Vec4b>(i, j);
              rgba[0] = UCHAR_MAX;
              rgba[1] = saturate_cast<uchar>((float (mat.cols - j)) / ((float)mat.cols) * UCHAR_MAX);
              rgba[2] = saturate_cast<uchar>((float (mat.rows - i)) / ((float)mat.rows) * UCHAR_MAX);
              rgba[3] = saturate_cast<uchar>(0.5 * (rgba[1] + rgba[2]));
          }
      }
  }
  
  int main(int argv, char **argc)
  {
      // Create mat with alpha channel
      Mat mat(480, 640, CV_8UC4);
      createAlphaMat(mat);
  
      vector<int> compression_params;
      compression_params.push_back(CV_IMWRITE_PNG_COMPRESSION);
      compression_params.push_back(9);
  
      try {
          imwrite("alpha.png", mat, compression_params);
      }
      catch (runtime_error& ex) {
          fprintf(stderr, "Exception converting image to PNG format: %s\n", ex.what());
          return 1;
      }
  
      fprintf(stdout, "Saved PNG file with alpha data.\n");
      return 0;
  }
  ```

cv::VideoCapture: |
  Class for video capturing from video files, image sequences or cameras.
  The class provides C++ API for capturing video from cameras or for
  reading video files and image sequences. Here is how the class can be
  used: :
  
  ```c++
  #include "opencv2/opencv.hpp"
  
  using namespace cv;
  
  int main(int, char**)
  {
      VideoCapture cap(0); // open the default camera
      if(!cap.isOpened())  // check if we succeeded
          return -1;
  
      Mat edges;
      namedWindow("edges",1);
      for(;;)
      {
          Mat frame;
          cap >> frame; // get a new frame from camera
          cvtColor(frame, edges, COLOR_BGR2GRAY);
          GaussianBlur(edges, edges, Size(7,7), 1.5, 1.5);
          Canny(edges, edges, 0, 30, 3);
          imshow("edges", edges);
          if(waitKey(30) >= 0) break;
      }
      // the camera will be deinitialized automatically in VideoCapture destructor
      return 0;
  }
  ```
  
  
  **note**
  
  In C API the black-box structure `CvCapture` is used instead of
  `VideoCapture`.

cv::VideoCapture::VideoCapture: |
  VideoCapture constructors.
  
  **note**
  
  In C API, when you finished working with video, release `CvCapture`
  structure with `cvReleaseCapture()`, or use `Ptr<CvCapture>` that
  calls `cvReleaseCapture()` automatically in the destructor.

cv::VideoCapture::open: |-
  Open video file or a capturing device for video capturing
  
  The methods first call :ocvVideoCapture::release to close the already
  opened file or camera.
cv::VideoCapture::isOpened: |-
  Returns true if video capturing has been initialized already.
  
  If the previous call to `VideoCapture` constructor or
  `VideoCapture::open` succeeded, the method returns true.
cv::VideoCapture::release: "Closes video file or capturing device.\n\n\
  The methods are automatically called by subsequent\n\
  :ocvVideoCapture::open and by `VideoCapture` destructor.\n\n\
  The C function also deallocates memory and clears `*capture` pointer."
cv::VideoCapture::grab: "Grabs the next frame from video file or capturing device.\n\n\
  The methods/functions grab the next frame from video file or camera and\n\
  return true (non-zero) in the case of success.\n\n\
  The primary use of the function is in multi-camera environments,\n\
  especially when the cameras do not have hardware synchronization. That\n\
  is, you call `VideoCapture::grab()` for each camera and after that call\n\
  the slower method `VideoCapture::retrieve()` to decode and get frame\n\
  from each camera. This way the overhead on demosaicing or motion jpeg\n\
  decompression etc. is eliminated and the retrieved frames from different\n\
  cameras will be closer in time.\n\n\
  Also, when a connected camera is multi-head (for example, a stereo\n\
  camera or a Kinect device), the correct way of retrieving data from it\n\
  is to call VideoCapture::grab first and then call\n\
  :ocvVideoCapture::retrieve one or more times with different values of\n\
  the `channel` parameter. See\n\
  <http://code.opencv.org/projects/opencv/repository/revisions/master/entry/samples/cpp/kinect_maps.cpp>"
cv::VideoCapture::retrieve: "Decodes and returns the grabbed video frame.\n\n\
  The methods/functions decode and return the just grabbed frame. If no\n\
  frames has been grabbed (camera has been disconnected, or there are no\n\
  more frames in video file), the methods return false and the functions\n\
  return NULL pointer.\n\n\
  **note**\n\n\
  OpenCV 1.x functions `cvRetrieveFrame` and `cv.RetrieveFrame` return\n\
  image stored inside the video capturing structure. It is not allowed\n\
  to modify or release the image! You can copy the frame using\n\
  :ocvcvCloneImage and then do whatever you want with the copy.\n"
cv::VideoCapture::read: "Grabs, decodes and returns the next video frame.\n\n\
  The methods/functions combine :ocvVideoCapture::grab and\n\
  :ocvVideoCapture::retrieve in one call. This is the most convenient\n\
  method for reading video files or capturing data from decode and return\n\
  the just grabbed frame. If no frames has been grabbed (camera has been\n\
  disconnected, or there are no more frames in video file), the methods\n\
  return false and the functions return NULL pointer.\n\n\
  **note**\n\n\
  OpenCV 1.x functions `cvRetrieveFrame` and `cv.RetrieveFrame` return\n\
  image stored inside the video capturing structure. It is not allowed\n\
  to modify or release the image! You can copy the frame using\n\
  :ocvcvCloneImage and then do whatever you want with the copy.\n"
cv::VideoCapture::get: |-
  Returns the specified `VideoCapture` property
  
  **Note**: When querying a property that is not supported by the backend
  used by the `VideoCapture` class, value 0 is returned.
cv::VideoCapture::set: Sets a property in the `VideoCapture`.
cv::VideoWriter: Video writer class.
cv::VideoWriter::VideoWriter: |-
  VideoWriter constructors
  
  The constructors/functions initialize video writers. On Linux FFMPEG is
  used to write videos; on Windows FFMPEG or VFW is used; on MacOSX QTKit
  is used.
cv::ReleaseVideoWriter: |-
  Releases the AVI writer.
  
  The function should be called after you finished using `CvVideoWriter`
  opened with :ocvCreateVideoWriter.
cv::VideoWriter::open: |-
  Initializes or reinitializes video writer.
  
  The method opens video writer. Parameters are the same as in the
  constructor :ocvVideoWriter::VideoWriter.
cv::VideoWriter::isOpened: Returns true if video writer has been successfully initialized.
cv::VideoWriter::write: |-
  Writes the next video frame
  
  The functions/methods write the specified image to video file. It must
  have the same size as has been specified when opening the video writer.
cv::gpu::norm: Returns the norm of a matrix (or difference of two matrices).
cv::gpu::sum: Returns the sum of matrix elements.
cv::gpu::absSum: Returns the sum of absolute values for matrix elements.
cv::gpu::sqrSum: Returns the squared sum of matrix elements.
cv::gpu::minMax: |-
  Finds global minimum and maximum matrix elements and returns their
  values.
  
  The function does not work with `CV_64F` images on GPUs with the compute
  capability < 1.3.
cv::gpu::minMaxLoc: |-
  Finds global minimum and maximum matrix elements and returns their
  values with locations.
cv::gpu::countNonZero: |-
  Counts non-zero matrix elements.
  
  The function does not work with `CV_64F` images on GPUs with the compute
  capability < 1.3.
cv::gpu::reduce: |-
  Reduces a matrix to a vector.
  
  The function `reduce` reduces the matrix to a vector by treating the
  matrix rows/columns as a set of 1D vectors and performing the specified
  operation on the vectors until a single row/column is obtained. For
  example, the function can be used to compute horizontal and vertical
  projections of a raster image. In case of `CV_REDUCE_SUM` and
  `CV_REDUCE_AVG` , the output may have a larger element bit-depth to
  preserve accuracy. And multi-channel arrays are also supported in these
  two reduction modes.
cv::gpu::meanStdDev: Computes a mean value and a standard deviation of matrix elements.
cv::gpu::rectStdDev: Computes a standard deviation of integral images.
cv::gpu::normalize: Normalizes the norm or value range of an array.
cv::gpu::integral: Computes an integral image.
cv::gpu::sqrIntegral: Computes a squared integral image.
cv::detail::SeamFinder: |
  Base class for a seam estimator. :
  
  ```c++
  class CV_EXPORTS SeamFinder
  {
  public:
      virtual ~SeamFinder() {}
      virtual void find(const std::vector<Mat> &src, const std::vector<Point> &corners,
                        std::vector<Mat> &masks) = 0;
  };
  ```

cv::detail::SeamFinder::find: Estimates seams.
cv::detail::NoSeamFinder: |
  Stub seam estimator which does nothing. :
  
  ```c++
  class CV_EXPORTS NoSeamFinder : public SeamFinder
  {
  public:
      void find(const std::vector<Mat>&, const std::vector<Point>&, std::vector<Mat>&) {}
  };
  ```

cv::detail::PairwiseSeamFinder: |
  Base class for all pairwise seam estimators. :
  
  ```c++
  class CV_EXPORTS PairwiseSeamFinder : public SeamFinder
  {
  public:
      virtual void find(const std::vector<Mat> &src, const std::vector<Point> &corners,
                        std::vector<Mat> &masks);
  
  protected:
      void run();
      virtual void findInPair(size_t first, size_t second, Rect roi) = 0;
  
      std::vector<Mat> images_;
      std::vector<Size> sizes_;
      std::vector<Point> corners_;
      std::vector<Mat> masks_;
  };
  ```

cv::detail::PairwiseSeamFinder::findInPair: Resolves masks intersection of two specified images in the given ROI.
cv::detail::VoronoiSeamFinder: |
  Voronoi diagram-based seam estimator. :
  
  ```c++
  class CV_EXPORTS VoronoiSeamFinder : public PairwiseSeamFinder
  {
  public:
      virtual void find(const std::vector<Size> &size, const std::vector<Point> &corners,
                        std::vector<Mat> &masks);
  private:
      void findInPair(size_t first, size_t second, Rect roi);
  };
  ```

cv::detail::GraphCutSeamFinderBase: |
  Base class for all minimum graph-cut-based seam estimators. :
  
  ```c++
  class CV_EXPORTS GraphCutSeamFinderBase
  {
  public:
      enum { COST_COLOR, COST_COLOR_GRAD };
  };
  ```

cv::detail::GraphCutSeamFinder: |
  Minimum graph cut-based seam estimator. See details in [V03]_. :
  
  ```c++
  class CV_EXPORTS GraphCutSeamFinder : public GraphCutSeamFinderBase, public SeamFinder
  {
  public:
      GraphCutSeamFinder(int cost_type = COST_COLOR_GRAD, float terminal_cost = 10000.f,
                         float bad_region_penalty = 1000.f);
  
      void find(const std::vector<Mat> &src, const std::vector<Point> &corners,
                std::vector<Mat> &masks);
  
  private:
      /* hidden */
  };
  ```

cv::softcascade::SCascade: |
  Implementation of soft (stageless) cascaded detector. :
  
  ```c++
  class CV_EXPORTS SCascade : public Algorithm
  {
      struct CV_EXPORTS Detection
      {
            ushort x;
            ushort y;
            ushort w;
            ushort h;
            float confidence;
            int kind;
  
            enum {PEDESTRIAN = 0};
      };
  
      SCascade(const double minScale = 0.4, const double maxScale = 5., const int scales = 55, const int rejfactor = 1);
      virtual ~SCascade();
      virtual bool load(const FileNode& fn);
      virtual void detect(InputArray image, InputArray rois, OutputArray objects, Stream& stream = Stream::Null()) const;
      virtual void genRoi(InputArray roi, OutputArray mask, Stream& stream = Stream::Null()) const;
  };
  ```

cv::softcascade::SCascade::~SCascade: Destructor for SCascade.
cv::softcascade::SCascade::load: Load cascade from FileNode.
cv::softcascade::SCascade::detect: |-
  Apply cascade to an input frame and return the vector of Decection
  objcts.
cv::Soft Cascade Classifier for Object Detection: |-
  Cascade detectors have been shown to operate extremely rapidly, with
  high accuracy, and have important applications in different spheres. The
  initial goal for this cascade implementation was the fast and accurate
  pedestrian detector but it also useful in general. Soft cascade is
  trained with AdaBoost. But instead of training sequence of stages, the
  soft cascade is trained as a one long stage of T weak classifiers. Soft
  cascade is formulated as follows:
  
  $$\texttt{H}(x) = \sum _{\texttt{t}=1..\texttt{T}} {\texttt{s}_t(x)}$$
  
  where $\texttt{s}_t(x) = \alpha_t\texttt{h}_t(x)$ are the set of
  thresholded weak classifiers selected during AdaBoost training scaled by
  the associated weights. Let
  
  $$\texttt{H}_t(x) = \sum _{\texttt{i}=1..\texttt{t}} {\texttt{s}_i(x)}$$
  
  be the partial sum of sample responses before $t$-the weak classifier
  will be applied. The function $\texttt{H}_t(x)$ of $t$ for sample $x$
  named *sample trace*. After each weak classifier evaluation, the sample
  trace at the point $t$ is compared with the rejection threshold $r_t$.
  The sequence of $r_t$ named *rejection trace*.
  
  The sample has been rejected if it fall rejection threshold. So
  stageless cascade allows to reject not-object sample as soon as
  possible. Another meaning of the sample trace is a confidence with that
  sample recognized as desired object. At each $t$ that confidence depend
  on all previous weak classifier. This feature of soft cascade is
  resulted in more accurate detection. The original formulation of soft
  cascade can be found in [BJ05]_.
cv::softcascade::Detector: |
  Implementation of soft (stageless) cascaded detector. :
  
  ```c++
  class Detector : public Algorithm
  {
  public:
  
      enum { NO_REJECT = 1, DOLLAR = 2, /*PASCAL = 4,*/ DEFAULT = NO_REJECT};
  
      Detector(double minScale = 0.4, double maxScale = 5., int scales = 55, int rejCriteria = 1);
      virtual ~Detector();
      cv::AlgorithmInfo* info() const;
      virtual bool load(const FileNode& fileNode);
      virtual void read(const FileNode& fileNode);
      virtual void detect(InputArray image, InputArray rois, std::vector<Detection>& objects) const;
      virtual void detect(InputArray image, InputArray rois, OutputArray rects, OutputArray confs) const;
  
  }
  ```

cv::softcascade::Detector::Detector: An empty cascade will be created.
cv::softcascade::Detector::~Detector: Destructor for Detector.
cv::softcascade::Detector::load: Load cascade from FileNode.
cv::softcascade::Detector::detect: |-
  Apply cascade to an input frame and return the vector of Detection
  objects.
cv::softcascade::ChannelFeatureBuilder: |
  Public interface for of soft (stageless) cascaded detector. :
  
  ```c++
  class ChannelFeatureBuilder : public Algorithm
  {
  public:
      virtual ~ChannelFeatureBuilder();
  
      virtual void operator()(InputArray src, OutputArray channels) const = 0;
  
      static cv::Ptr<ChannelFeatureBuilder> create();
  };
  ```

cv::softcascade::ChannelFeatureBuilder:~ChannelFeatureBuilder: Destructor for ChannelFeatureBuilder.
cv::softcascade::ChannelFeatureBuilder::operator(): Create channel feature integrals for input image.
cv::softcascade::Octave: |
  Public interface for soft cascade training algorithm. :
  
  ```c++
  class Octave : public Algorithm
  {
  public:
  
      enum {
          // Direct backward pruning. (Cha Zhang and Paul Viola)
          DBP = 1,
          // Multiple instance pruning. (Cha Zhang and Paul Viola)
          MIP = 2,
          // Originally proposed by L. Bourdev and J. Brandt
          HEURISTIC = 4 };
  
      virtual ~Octave();
      static cv::Ptr<Octave> create(cv::Rect boundingBox, int npositives, int nnegatives, int logScale, int shrinkage);
  
      virtual bool train(const Dataset* dataset, const FeaturePool* pool, int weaks, int treeDepth) = 0;
      virtual void setRejectThresholds(OutputArray thresholds) = 0;
      virtual void write( cv::FileStorage &fs, const FeaturePool* pool, InputArray thresholds) const = 0;
      virtual void write( CvFileStorage* fs, String name) const = 0;
  
  };
  ```

cv::softcascade::Octave::~Octave: Destructor for Octave.
cv::softcascade::FeaturePool: |
  Public interface for feature pool. This is a hight level abstraction for
  training random feature pool. :
  
  ```c++
  class FeaturePool
  {
  public:
  
      virtual int size() const = 0;
      virtual float apply(int fi, int si, const Mat& channels) const = 0;
      virtual void write( cv::FileStorage& fs, int index) const = 0;
      virtual ~FeaturePool();
  
  };
  ```

cv::softcascade::FeaturePool::size: Returns size of feature pool.
cv::softcascade::FeaturePool::~FeaturePool: FeaturePool destructor.
cv::softcascade::FeaturePool::write: Write specified feature from feature pool to file storage.
cv::softcascade::FeaturePool::apply: Compute feature on integral channel image.
cv::CvStatModel: |-
  Base class for statistical models in ML. :
  
  ```c++
  class CvStatModel
  {
  public:
      /* CvStatModel(); */
      /* CvStatModel( const Mat& train_data ... ); */
  
      virtual ~CvStatModel();
  
      virtual void clear()=0;
  
      /* virtual bool train( const Mat& train_data, [int tflag,] ..., const
          Mat& responses, ...,
       [const Mat& var_idx,] ..., [const Mat& sample_idx,] ...
       [const Mat& var_type,] ..., [const Mat& missing_mask,]
          <misc_training_alg_params> ... )=0;
        */
  
      /* virtual float predict( const Mat& sample ... ) const=0; */
  
      virtual void save( const char* filename, const char* name=0 )=0;
      virtual void load( const char* filename, const char* name=0 )=0;
  
      virtual void write( CvFileStorage* storage, const char* name )=0;
      virtual void read( CvFileStorage* storage, CvFileNode* node )=0;
  };
  ```
  
  
  In this declaration, some methods are commented off. These are methods
  for which there is no unified API (with the exception of the default
  constructor). However, there are many similarities in the syntax and
  semantics that are briefly described below in this section, as if they
  are part of the base class.
cv::CvStatModel::CvStatModel: "The default constructor.\n\n\
  Each statistical model class in ML has a default constructor without\n\
  parameters. This constructor is useful for a two-stage model\n\
  construction, when the default constructor is followed by\n\
  :ocvCvStatModel::train or :ocvCvStatModel::load."
cv::CvStatModel::CvStatModel(...): |-
  The training constructor.
  
  Most ML classes provide a single-step constructor and train
  constructors. This constructor is equivalent to the default constructor,
  followed by the :ocvCvStatModel::train method with the parameters that
  are passed to the constructor.
cv::CvStatModel::~CvStatModel: |-
  The virtual destructor.
  
  The destructor of the base class is declared as virtual. So, it is safe
  to write the following code: :
  
  ```c++
  CvStatModel* model;
  if( use_svm )
      model = new CvSVM(... /* SVM params */);
  else
      model = new CvDTree(... /* Decision tree params */);
  ...
  delete model;
  ```
  
  
  Normally, the destructor of each derived class does nothing. But in this
  instance, it calls the overridden method :ocvCvStatModel::clear that
  deallocates all the memory.
cv::CvStatModel::clear: "Deallocates memory and resets the model state.\n\n\
  The method `clear` does the same job as the destructor: it deallocates\n\
  all the memory occupied by the class members. But the object itself is\n\
  not destructed and can be reused further. This method is called from the\n\
  destructor, from the :ocvCvStatModel::train methods of the derived\n\
  classes, from the methods :ocvCvStatModel::load,\n\
  :ocvCvStatModel::read(), or even explicitly by the user."
cv::CvStatModel::save: |-
  Saves the model to a file.
  
  The method `save` saves the complete model state to the specified XML or
  YAML file with the specified name or default name (which depends on a
  particular class). *Data persistence* functionality from `CxCore` is
  used.
cv::CvStatModel::load: |-
  Loads the model from a file.
  
  The method `load` loads the complete model state with the specified name
  (or default model-dependent name) from the specified XML or YAML file.
  The previous model state is cleared by :ocvCvStatModel::clear.
cv::CvStatModel::write: |-
  Writes the model to the file storage.
  
  The method `write` stores the complete model state in the file storage
  with the specified name or default name (which depends on the particular
  class). The method is called by :ocvCvStatModel::save.
cv::CvStatModel::read: |-
  Reads the model from the file storage.
  
  The method `read` restores the complete model state from the specified
  node of the file storage. Use the function :ocvGetFileNodeByName to
  locate the node.
  
  The previous model state is cleared by :ocvCvStatModel::clear.
cv::CvStatModel::train: |-
  Trains the model.
  
  The method trains the statistical model using a set of input feature
  vectors and the corresponding output values (responses). Both input and
  output vectors/values are passed as matrices. By default, the input
  feature vectors are stored as `train_data` rows, that is, all the
  components (features) of a training vector are stored continuously.
  However, some algorithms can handle the transposed representation when
  all values of each particular feature (component/input variable) over
  the whole input set are stored continuously. If both layouts are
  supported, the method includes the `tflag` parameter that specifies the
  orientation as follows:
  
    `tflag=CV_ROW_SAMPLE` The feature vectors are stored as rows.
  
  
    `tflag=CV_COL_SAMPLE` The feature vectors are stored as columns.
  
  
  The `train_data` must have the `CV_32FC1` (32-bit floating-point,
  single-channel) format. Responses are usually stored in a 1D vector (a
  row or a column) of `CV_32SC1` (only in the classification problem) or
  `CV_32FC1` format, one value per input vector. Although, some
  algorithms, like various flavors of neural nets, take vector responses.
  
  For classification problems, the responses are discrete class labels.
  For regression problems, the responses are values of the function to be
  approximated. Some algorithms can deal only with classification
  problems, some - only with regression problems, and some can deal with
  both problems. In the latter case, the type of output variable is either
  passed as a separate parameter or as the last element of the `var_type`
  vector:
  
    `CV_VAR_CATEGORICAL` The output values are discrete class labels.
  
  
    `CV_VAR_ORDERED(=CV_VAR_NUMERICAL)` The output values are ordered.
  This means that two different values can be compared as numbers, and
  this is a regression problem.
  
  
  Types of input variables can be also specified using `var_type`. Most
  algorithms can handle only ordered input variables.
  
  Many ML models may be trained on a selected feature subset, and/or on a
  selected sample subset of the training set. To make it easier for you,
  the method `train` usually includes the `var_idx` and `sample_idx`
  parameters. The former parameter identifies variables (features) of
  interest, and the latter one identifies samples of interest. Both
  vectors are either integer (`CV_32SC1`) vectors (lists of 0-based
  indices) or 8-bit (`CV_8UC1`) masks of active variables/samples. You may
  pass `NULL` pointers instead of either of the arguments, meaning that
  all of the variables/samples are used for training.
  
  Additionally, some algorithms can handle missing measurements, that is,
  when certain features of certain training samples have unknown values
  (for example, they forgot to measure a temperature of patient A on
  Monday). The parameter `missing_mask`, an 8-bit matrix of the same size
  as `train_data`, is used to mark the missed values (non-zero elements of
  the mask).
  
  Usually, the previous model state is cleared by :ocvCvStatModel::clear
  before running the training procedure. However, some algorithms may
  optionally update the model state with the new training data, instead of
  resetting it.
cv::CvStatModel::predict: |-
  Predicts the response for a sample.
  
  The method is used to predict the response for a new sample. In case of
  a classification, the method returns the class label. In case of a
  regression, the method returns the output function value. The input
  sample must have as many components as the `train_data` passed to
  `train` contains. If the `var_idx` parameter is passed to `train`, it is
  remembered and then is used to extract only the necessary components
  from the input sample in the method `predict`.
  
  The suffix `const` means that prediction does not affect the internal
  model state, so the method can be safely called from within different
  threads.
cv::gpu::StereoBM: |-
  Class computing stereo correspondence (disparity map) using the block
  matching algorithm. :
cv::gpu::createStereoBM: Creates StereoBM object.
cv::gpu::StereoBeliefPropagation: |-
  Class computing stereo correspondence using the belief propagation
  algorithm. :
  
  ```c++
  class CV_EXPORTS StereoBeliefPropagation : public cv::StereoMatcher
  {
  public:
      using cv::StereoMatcher::compute;
  
      virtual void compute(InputArray left, InputArray right, OutputArray disparity, Stream& stream) = 0;
  
      //! version for user specified data term
      virtual void compute(InputArray data, OutputArray disparity, Stream& stream = Stream::Null()) = 0;
  
      //! number of BP iterations on each level
      virtual int getNumIters() const = 0;
      virtual void setNumIters(int iters) = 0;
  
      //! number of levels
      virtual int getNumLevels() const = 0;
      virtual void setNumLevels(int levels) = 0;
  
      //! truncation of data cost
      virtual double getMaxDataTerm() const = 0;
      virtual void setMaxDataTerm(double max_data_term) = 0;
  
      //! data weight
      virtual double getDataWeight() const = 0;
      virtual void setDataWeight(double data_weight) = 0;
  
      //! truncation of discontinuity cost
      virtual double getMaxDiscTerm() const = 0;
      virtual void setMaxDiscTerm(double max_disc_term) = 0;
  
      //! discontinuity single jump
      virtual double getDiscSingleJump() const = 0;
      virtual void setDiscSingleJump(double disc_single_jump) = 0;
  
      virtual int getMsgType() const = 0;
      virtual void setMsgType(int msg_type) = 0;
  
      static void estimateRecommendedParams(int width, int height, int& ndisp, int& iters, int& levels);
  };
  ```
  
  
  The class implements algorithm described in [Felzenszwalb2006]_ . It
  can compute own data cost (using a truncated linear model) or use a
  user-provided data cost.
  
  **note**
  
  `StereoBeliefPropagation` requires a lot of memory for message
  storage:
  
  $$width _ step  \cdot height  \cdot ndisp  \cdot 4  \cdot (1 + 0.25)$$
  
  and for data cost storage:
  
  $$width_step \cdot height \cdot ndisp \cdot (1 + 0.25 + 0.0625 +  \dotsm + \frac{1}{4^{levels}})$$
  
  `width_step` is the number of bytes in a line including padding.
  
  
  `StereoBeliefPropagation` uses a truncated linear model for the data
  cost and discontinuity terms:
  
  $$DataCost = data _ weight  \cdot \min ( \lvert Img_Left(x,y)-Img_Right(x-d,y)  \rvert , max _ data _ term)$$
  
  $$DiscTerm =  \min (disc _ single _ jump  \cdot \lvert f_1-f_2  \rvert , max _ disc _ term)$$
  
  For more details, see [Felzenszwalb2006]_.
  
  By default, `StereoBeliefPropagation` uses floating-point arithmetics
  and the `CV_32FC1` type for messages. But it can also use fixed-point
  arithmetics and the `CV_16SC1` message type for better performance. To
  avoid an overflow in this case, the parameters must satisfy the
  following requirement:
  
  $$10  \cdot 2^{levels-1}  \cdot max _ data _ term < SHRT _ MAX$$
cv::gpu::createStereoBeliefPropagation: Creates StereoBeliefPropagation object.
cv::gpu::StereoBeliefPropagation::estimateRecommendedParams: |-
  Uses a heuristic method to compute the recommended parameters ( `ndisp`,
  `iters` and `levels` ) for the specified image size ( `width` and
  `height` ).
cv::gpu::StereoBeliefPropagation::compute: |-
  Enables the stereo correspondence operator that finds the disparity for
  the specified data cost.
cv::gpu::StereoConstantSpaceBP: |-
  Class computing stereo correspondence using the constant space belief
  propagation algorithm. :
  
  ```c++
  class CV_EXPORTS StereoConstantSpaceBP : public gpu::StereoBeliefPropagation
  {
  public:
      //! number of active disparity on the first level
      virtual int getNrPlane() const = 0;
      virtual void setNrPlane(int nr_plane) = 0;
  
      virtual bool getUseLocalInitDataCost() const = 0;
      virtual void setUseLocalInitDataCost(bool use_local_init_data_cost) = 0;
  
      static void estimateRecommendedParams(int width, int height, int& ndisp, int& iters, int& levels, int& nr_plane);
  };
  ```
  
  
  The class implements algorithm described in [Yang2010]_.
  `StereoConstantSpaceBP` supports both local minimum and global minimum
  data cost initialization algorithms. For more details, see the paper
  mentioned above. By default, a local algorithm is used. To enable a
  global algorithm, set `use_local_init_data_cost` to `false` .
  
  `StereoConstantSpaceBP` uses a truncated linear model for the data cost
  and discontinuity terms:
  
  $$DataCost = data _ weight  \cdot \min ( \lvert I_2-I_1  \rvert , max _ data _ term)$$
  
  $$DiscTerm =  \min (disc _ single _ jump  \cdot \lvert f_1-f_2  \rvert , max _ disc _ term)$$
  
  For more details, see [Yang2010]_.
  
  By default, `StereoConstantSpaceBP` uses floating-point arithmetics and
  the `CV_32FC1` type for messages. But it can also use fixed-point
  arithmetics and the `CV_16SC1` message type for better performance. To
  avoid an overflow in this case, the parameters must satisfy the
  following requirement:
  
  $$10  \cdot 2^{levels-1}  \cdot max _ data _ term < SHRT _ MAX$$
cv::gpu::createStereoConstantSpaceBP: Creates StereoConstantSpaceBP object.
cv::gpu::StereoConstantSpaceBP::estimateRecommendedParams: |-
  Uses a heuristic method to compute parameters (ndisp, iters, levelsand
  nrplane) for the specified image size (widthand height).
cv::gpu::DisparityBilateralFilter: |-
  Class refining a disparity map using joint bilateral filtering. :
  
  ```c++
  class CV_EXPORTS DisparityBilateralFilter : public cv::Algorithm
  {
  public:
      //! the disparity map refinement operator. Refine disparity map using joint bilateral filtering given a single color image.
      //! disparity must have CV_8U or CV_16S type, image must have CV_8UC1 or CV_8UC3 type.
      virtual void apply(InputArray disparity, InputArray image, OutputArray dst, Stream& stream = Stream::Null()) = 0;
  
      virtual int getNumDisparities() const = 0;
      virtual void setNumDisparities(int numDisparities) = 0;
  
      virtual int getRadius() const = 0;
      virtual void setRadius(int radius) = 0;
  
      virtual int getNumIters() const = 0;
      virtual void setNumIters(int iters) = 0;
  
      //! truncation of data continuity
      virtual double getEdgeThreshold() const = 0;
      virtual void setEdgeThreshold(double edge_threshold) = 0;
  
      //! truncation of disparity continuity
      virtual double getMaxDiscThreshold() const = 0;
      virtual void setMaxDiscThreshold(double max_disc_threshold) = 0;
  
      //! filter range sigma
      virtual double getSigmaRange() const = 0;
      virtual void setSigmaRange(double sigma_range) = 0;
  };
  ```
  
  
  The class implements [Yang2010]_ algorithm.
cv::gpu::createDisparityBilateralFilter: Creates DisparityBilateralFilter object.
cv::gpu::DisparityBilateralFilter::apply: Refines a disparity map using joint bilateral filtering.
cv::gpu::reprojectImageTo3D: Reprojects a disparity image to 3D space.
cv::gpu::drawColorDisp: |-
  Colors a disparity image.
  
  This function draws a colored disparity map by converting disparity
  values from `[0..ndisp)` interval first to `HSV` color space (where
  different disparity values correspond to different hues) and then
  converting the pixels to `RGB` for visualization.
cv::moments: |
  Calculates all of the moments up to the third order of a polygon or
  rasterized shape.
  
  The function computes moments, up to the 3rd order, of a vector shape or
  a rasterized shape. The results are returned in the structure `Moments`
  defined as: :
  
  ```c++
  class Moments
  {
  public:
      Moments();
      Moments(double m00, double m10, double m01, double m20, double m11,
              double m02, double m30, double m21, double m12, double m03 );
      Moments( const CvMoments& moments );
      operator CvMoments() const;
  
      // spatial moments
      double  m00, m10, m01, m20, m11, m02, m30, m21, m12, m03;
      // central moments
      double  mu20, mu11, mu02, mu30, mu21, mu12, mu03;
      // central normalized moments
      double  nu20, nu11, nu02, nu30, nu21, nu12, nu03;
  }
  ```
  
  
  In case of a raster image, the spatial moments
  $\texttt{Moments::m}_{ji}$ are computed as:
  
  $$\texttt{m} _{ji}= \sum _{x,y}  \left ( \texttt{array} (x,y)  \cdot x^j  \cdot y^i \right )$$
  
  The central moments $\texttt{Moments::mu}_{ji}$ are computed as:
  
  $$\texttt{mu} _{ji}= \sum _{x,y}  \left ( \texttt{array} (x,y)  \cdot (x -  \bar{x} )^j  \cdot (y -  \bar{y} )^i \right )$$
  
  where $(\bar{x}, \bar{y})$ is the mass center:
  
  $$\bar{x} = \frac{\texttt{m}_{10}}{\texttt{m}_{00}} , \; \bar{y} = \frac{\texttt{m}_{01}}{\texttt{m}_{00}}$$
  
  The normalized central moments $\texttt{Moments::nu}_{ij}$ are computed
  as:
  
  $$\texttt{nu} _{ji}= \frac{\texttt{mu}_{ji}}{\texttt{m}_{00}^{(i+j)/2+1}} .$$
  
  **note**
  
  $\texttt{mu}_{00}=\texttt{m}_{00}$, $\texttt{nu}_{00}=1$
  $\texttt{nu}_{10}=\texttt{mu}_{10}=\texttt{mu}_{01}=\texttt{mu}_{10}=0$
  , hence the values are not stored.
  
  
  The moments of a contour are defined in the same way but computed using
  the Green's formula (see <http://en.wikipedia.org/wiki/Green_theorem>).
  So, due to a limited raster resolution, the moments computed for a
  contour are slightly different from the moments computed for the same
  rasterized contour.
  
  **note**
  
  Since the contour moments are computed using Green formula, you may
  get seemingly odd results for contours with self-intersections, e.g. a
  zero area (`m00`) for butterfly-shaped contours.

cv::HuMoments: |-
  Calculates seven Hu invariants.
  
  The function calculates seven Hu invariants (introduced in [Hu62]_; see
  also <http://en.wikipedia.org/wiki/Image_moment>) defined as:
  
  $$\begin{array}{l} hu[0]= \eta _{20}+ \eta _{02} \ hu[1]=( \eta _{20}- \eta _{02})^{2}+4 \eta _{11}^{2} \ hu[2]=( \eta _{30}-3 \eta _{12})^{2}+ (3 \eta _{21}- \eta _{03})^{2} \ hu[3]=( \eta _{30}+ \eta _{12})^{2}+ ( \eta _{21}+ \eta _{03})^{2} \ hu[4]=( \eta _{30}-3 \eta _{12})( \eta _{30}+ \eta _{12})[( \eta _{30}+ \eta _{12})^{2}-3( \eta _{21}+ \eta _{03})^{2}]+(3 \eta _{21}- \eta _{03})( \eta _{21}+ \eta _{03})[3( \eta _{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _{03})^{2}] \ hu[5]=( \eta _{20}- \eta _{02})[( \eta _{30}+ \eta _{12})^{2}- ( \eta _{21}+ \eta _{03})^{2}]+4 \eta _{11}( \eta _{30}+ \eta _{12})( \eta _{21}+ \eta _{03}) \ hu[6]=(3 \eta _{21}- \eta _{03})( \eta _{21}+ \eta _{03})[3( \eta _{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _{03})^{2}]-( \eta _{30}-3 \eta _{12})( \eta _{21}+ \eta _{03})[3( \eta _{30}+ \eta _{12})^{2}-( \eta _{21}+ \eta _{03})^{2}] \ \end{array}$$
  
  where $\eta_{ji}$ stands for $\texttt{Moments::nu}_{ji}$ .
  
  These values are proved to be invariants to the image scale, rotation,
  and reflection except the seventh one, whose sign is changed by
  reflection. This invariance is proved with the assumption of infinite
  image resolution. In case of raster images, the computed Hu invariants
  for the original and transformed images are a bit different.
cv::connectedComponents: |-
  computes the connected components labeled image of boolean image `image`
  with 4 or 8 way connectivity - returns N, the total number of labels [0,
  N-1] where 0 represents the background label. ltype specifies the output
  label image type, an important consideration based on the total number
  of labels or alternatively the total number of pixels in the source
  image.
cv::findContours: |
  Finds contours in a binary image.
  
  The function retrieves contours from the binary image using the
  algorithm [Suzuki85]_. The contours are a useful tool for shape
  analysis and object detection and recognition. See `squares.c` in the
  OpenCV sample directory.
  
  **note**
  
  Source `image` is modified by this function. Also, the function does
  not take into account 1-pixel border of the image (it's filled with
  0's and used for neighbor analysis in the algorithm), therefore the
  contours touching the image border will be clipped.
  
  **note**
  
  If you use the new Python interface then the `CV_` prefix has to be
  omitted in contour retrieval mode and contour approximation method
  parameters (for example, use `cv2.RETR_LIST` and
  `cv2.CHAIN_APPROX_NONE` parameters). If you use the old Python
  interface then these parameters have the `CV_` prefix (for example,
  use `cv.CV_RETR_LIST` and `cv.CV_CHAIN_APPROX_NONE`).

cv::approxPolyDP: |-
  Approximates a polygonal curve(s) with the specified precision.
  
  The functions `approxPolyDP` approximate a curve or a polygon with
  another curve/polygon with less vertices so that the distance between
  them is less or equal to the specified precision. It uses the
  Douglas-Peucker algorithm
  <http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithm>
  
  See
  <http://code.opencv.org/projects/opencv/repository/revisions/master/entry/samples/cpp/contours.cpp>
  for the function usage model.
cv::ApproxChains: |-
  Approximates Freeman chain(s) with a polygonal curve.
  
  This is a standalone contour approximation routine, not represented in
  the new interface. When :ocvFindContours retrieves contours as Freeman
  chains, it calls the function to get approximated contours, represented
  as polygons.
cv::arcLength: |-
  Calculates a contour perimeter or a curve length.
  
  The function computes a curve length or a closed contour perimeter.
cv::boundingRect: |-
  Calculates the up-right bounding rectangle of a point set.
  
  The function calculates and returns the minimal up-right bounding
  rectangle for the specified point set.
cv::contourArea: |
  Calculates a contour area.
  
  The function computes a contour area. Similarly to :ocvmoments , the
  area is computed using the Green formula. Thus, the returned area and
  the number of non-zero pixels, if you draw the contour using
  :ocvdrawContours or :ocvfillPoly , can be different. Also, the function
  will most certainly give a wrong results for contours with
  self-intersections.
  
  Example: :
  
  ```c++
  vector<Point> contour;
  contour.push_back(Point2f(0, 0));
  contour.push_back(Point2f(10, 0));
  contour.push_back(Point2f(10, 10));
  contour.push_back(Point2f(5, 4));
  
  double area0 = contourArea(contour);
  vector<Point> approx;
  approxPolyDP(contour, approx, 5, true);
  double area1 = contourArea(approx);
  
  cout << "area0 =" << area0 << endl <<
          "area1 =" << area1 << endl <<
          "approx poly vertices" << approx.size() << endl;
  ```

cv::convexHull: |-
  Finds the convex hull of a point set.
  
  The functions find the convex hull of a 2D point set using the
  Sklansky's algorithm [Sklansky82]_ that has *O(N logN)* complexity in
  the current implementation. See the OpenCV sample `convexhull.cpp` that
  demonstrates the usage of different function variants.
cv::convexityDefects: |-
  Finds the convexity defects of a contour.
  
  The function finds all convexity defects of the input contour and
  returns a sequence of the `CvConvexityDefect` structures, where
  `CvConvexityDetect` is defined as: :
  
  ```c++
  struct CvConvexityDefect
  {
     CvPoint* start; // point of the contour where the defect begins
     CvPoint* end; // point of the contour where the defect ends
     CvPoint* depth_point; // the farthest from the convex hull point within the defect
     float depth; // distance between the farthest point and the convex hull
  };
  ```
  
  
  The figure below displays convexity defects of a hand contour:
  
  ![image](pics/defects.png)
cv::fitEllipse: |-
  Fits an ellipse around a set of 2D points.
  
  The function calculates the ellipse that fits (in a least-squares sense)
  a set of 2D points best of all. It returns the rotated rectangle in
  which the ellipse is inscribed. The algorithm [Fitzgibbon95]_ is used.
cv::fitLine: |-
  Fits a line to a 2D or 3D point set.
  
  The function `fitLine` fits a line to a 2D or 3D point set by minimizing
  $\sum_i \rho(r_i)$ where $r_i$ is a distance between the $i^{th}$ point,
  the line and $\rho(r)$ is a distance function, one of the following:
  
    distType=CV_DIST_L2
  
  $$\rho (r) = r^2/2  \quad \text{(the simplest and the fastest least-squares method)}$$
  
  
  
    distType=CV_DIST_L1
  
  $$\rho (r) = r$$
  
  
  
    distType=CV_DIST_L12
  
  $$\rho (r) = 2  \cdot ( \sqrt{1 + \frac{r^2}{2}} - 1)$$
  
  
  
    distType=CV_DIST_FAIR
  
  $$\rho \left (r \right ) = C^2  \cdot \left (  \frac{r}{C} -  \log{\left(1 + \frac{r}{C}\right)} \right )  \quad \text{where} \quad C=1.3998$$
  
  
  
    distType=CV_DIST_WELSCH
  
  $$\rho \left (r \right ) =  \frac{C^2}{2} \cdot \left ( 1 -  \exp{\left(-\left(\frac{r}{C}\right)^2\right)} \right )  \quad \text{where} \quad C=2.9846$$
  
  
  
    distType=CV_DIST_HUBER
  
  $$\rho (r) =  \fork{r^2/2}{if $r < C$}{C \cdot (r-C/2)}{otherwise} \quad \text{where} \quad C=1.345$$
  
  
  
  The algorithm is based on the M-estimator (
  <http://en.wikipedia.org/wiki/M-estimator> ) technique that iteratively
  fits the line using the weighted least-squares algorithm. After each
  iteration the weights $w_i$ are adjusted to be inversely proportional to
  $\rho(r_i)$ .
cv::isContourConvex: |-
  Tests a contour convexity.
  
  The function tests whether the input contour is convex or not. The
  contour must be simple, that is, without self-intersections. Otherwise,
  the function output is undefined.
cv::minAreaRect: |-
  Finds a rotated rectangle of the minimum area enclosing the input 2D
  point set.
  
  The function calculates and returns the minimum-area bounding rectangle
  (possibly rotated) for a specified point set. See the OpenCV sample
  `minarea.cpp` .
cv::boxPoints: |-
  Finds the four vertices of a rotated rect. Useful to draw the rotated
  rectangle.
  
  The function finds the four vertices of a rotated rectangle. This
  function is useful to draw the rectangle. In C++, instead of using this
  function, you can directly use box.points() method. Please visit the
  [tutorial on bounding
  rectangle](http://docs.opencv.org/doc/tutorials/imgproc/shapedescriptors/bounding_rects_circles/bounding_rects_circles.html#bounding-rects-circles)
  for more information.
cv::minEnclosingCircle: |-
  Finds a circle of the minimum area enclosing a 2D point set.
  
  The function finds the minimal enclosing circle of a 2D point set using
  an iterative algorithm. See the OpenCV sample `minarea.cpp` .
cv::matchShapes: |-
  Compares two shapes.
  
  The function compares two shapes. All three implemented methods use the
  Hu invariants (see :ocvHuMoments ) as follows ( $A$ denotes
  `object1`,$B$ denotes `object2` ):
  
    method=CV_CONTOURS_MATCH_I1
  
  $$I_1(A,B) =  \sum _{i=1...7}  \left |  \frac{1}{m^A_i} -  \frac{1}{m^B_i} \right |$$
  
  
  
    method=CV_CONTOURS_MATCH_I2
  
  $$I_2(A,B) =  \sum _{i=1...7}  \left | m^A_i - m^B_i  \right |$$
  
  
  
    method=CV_CONTOURS_MATCH_I3
  
  $$I_3(A,B) =  \max _{i=1...7}  \frac{ \left| m^A_i - m^B_i \right| }{ \left| m^A_i \right| }$$
  
  
  
  where
  
  $$\begin{array}{l} m^A_i =  \mathrm{sign} (h^A_i)  \cdot \log{h^A_i} \ m^B_i =  \mathrm{sign} (h^B_i)  \cdot \log{h^B_i} \end{array}$$
  
  and $h^A_i, h^B_i$ are the Hu moments of $A$ and $B$ , respectively.
cv::pointPolygonTest: |-
  Performs a point-in-contour test.
  
  The function determines whether the point is inside a contour, outside,
  or lies on an edge (or coincides with a vertex). It returns positive
  (inside), negative (outside), or zero (on an edge) value,
  correspondingly. When `measureDist=false` , the return value is +1, -1,
  and 0, respectively. Otherwise, the return value is a signed distance
  between the point and the nearest contour edge.
  
  See below a sample output of the function where each image pixel is
  tested against the contour.
  
  ![image](pics/pointpolygon.png)
cv::ocl::Info: this class should be maintained by the user and be passed to getDevice
cv::ocl::getDevice: |-
  Returns the list of devices
  
  the function must be called before any other `cv::ocl` functions; it
  initializes ocl runtime.
cv::ocl::setDevice: Returns void
cv::ocl::setBinpath: |-
  Returns void
  
  If you call this function and set a valid path, the OCL module will save
  the compiled kernel to the address in the first time and reload the
  binary since that. It can save compilation time at the runtime.
cv::ocl::getoclContext: |-
  Returns the pointer to the opencl context
  
  Thefunction are used to get opencl context so that opencv can
  interactive with other opencl program.
cv::ocl::getoclCommandQueue: |-
  Returns the pointer to the opencl command queue
  
  Thefunction are used to get opencl command queue so that opencv can
  interactive with other opencl program. Super Resolution ================
  
  The Super Resolution module contains a set of functions and classes that
  can be used to solve the problem of resolution enhancement. There are a
  few methods implemented, most of them are descibed in the papers
  [Farsiu03]_ and [Mitzel09]_.
cv::superres::SuperResolution: |-
  Base class for Super Resolution algorithms.
  
  The class is only used to define the common interface for the whole
  family of Super Resolution algorithms.
cv::superres::SuperResolution::setInput: Set input frame source for Super Resolution algorithm.
cv::superres::SuperResolution::nextFrame: Process next frame from input and return output result.
cv::superres::SuperResolution::collectGarbage: Clear all inner buffers.
cv::superres::createSuperResolution_BTVL1: |
  Create Bilateral TV-L1 Super Resolution.
  
  This class implements Super Resolution algorithm described in the papers
  [Farsiu03]_ and [Mitzel09]_ .
  
  Here are important members of the class that control the algorithm,
  which you can set after constructing the class instance:
  
    **int scale** Scale factor.
  
  
    **int iterations** Iteration count.
  
  
    **double tau** Asymptotic value of steepest descent method.
  
  
    **double lambda** Weight parameter to balance data term and
  smoothness term.
  
  
    **double alpha** Parameter of spacial distribution in
  Bilateral-TV.
  
  
    **int btvKernelSize** Kernel size of Bilateral-TV filter.
  
  
    **int blurKernelSize** Gaussian blur kernel size.
  
  
    **double blurSigma** Gaussian blur sigma.
  
  
    **int temporalAreaRadius** Radius of the temporal search area.
  
  
    **Ptr<DenseOpticalFlowExt> opticalFlow** Dense optical flow
  algorithm.

cv::superres. Super Resolution: ""
cv::Support Vector Machines: |-
  Originally, support vector machines (SVM) was a technique for building
  an optimal binary (2-class) classifier. Later the technique was extended
  to regression and clustering problems. SVM is a partial case of
  kernel-based methods. It maps feature vectors into a higher-dimensional
  space using a kernel function and builds an optimal linear
  discriminating function in this space or an optimal hyper-plane that
  fits into the training data. In case of SVM, the kernel is not defined
  explicitly. Instead, a distance between any 2 points in the hyper-space
  needs to be defined.
  
  The solution is optimal, which means that the margin between the
  separating hyper-plane and the nearest feature vectors from both classes
  (in case of 2-class classifier) is maximal. The feature vectors that are
  the closest to the hyper-plane are called *support vectors*, which means
  that the position of other vectors does not affect the hyper-plane (the
  decision function).
  
  SVM implementation in OpenCV is based on [LibSVM]_.
cv::CvParamGrid: |-
  The grid determines the following iteration sequence of the statmodel
  parameter values:
  
  $$(min_val, min_val*step, min_val*{step}^2, \dots,  min_val*{step}^n),$$
  
  where $n$ is the maximal index satisfying
  
  $$\texttt{min_val} * \texttt{step} ^n <  \texttt{max_val}$$
  
  The grid is logarithmic, so `step` must always be greater then 1.
cv::CvParamGrid::CvParamGrid: |
  The constructors.
  
  The full constructor initializes corresponding members. The default
  constructor creates a dummy grid:
  
  ```c++
  CvParamGrid::CvParamGrid()
  {
      min_val = max_val = step = 0;
  }
  ```

cv::CvParamGrid::check: |
  Checks validness of the grid.
  
  Returns `true` if the grid is valid and `false` otherwise. The grid is
  valid if and only if:
    Lower bound of the grid is less then the upper one.
  
    Lower bound of the grid is positive.
  
    Grid step is greater then 1.

cv::CvSVMParams: "SVM training parameters.\n\n\
  The structure must be initialized and passed to the training method of\n\
  :ocvCvSVM."
cv::CvSVMParams::CvSVMParams: |-
  The constructors.
  
  The default constructor initialize the structure with following values:
  
  ```c++
  CvSVMParams::CvSVMParams() :
      svm_type(CvSVM::C_SVC), kernel_type(CvSVM::RBF), degree(0),
      gamma(1), coef0(0), C(1), nu(0), p(0), class_weights(0)
  {
      term_crit = cvTermCriteria( CV_TERMCRIT_ITER+CV_TERMCRIT_EPS, 1000, FLT_EPSILON );
  }
  ```
  
  
  A comparison of different kernels on the following 2D test case with
  four classes. Four C_SVC SVMs have been trained (one against rest) with
  auto_train. Evaluation on three different kernels (CHI2, INTER, RBF).
  The color depicts the class with max score. Bright means max-score > 0,
  dark means max-score < 0.
  
  ![image](pics/SVM_Comparison.png)
cv::CvSVM: Support Vector Machines.
cv::CvSVM::CvSVM: "Default and training constructors.\n\n\
  The constructors follow conventions of :ocvCvStatModel::CvStatModel. See\n\
  :ocvCvStatModel::train for parameters descriptions."
cv::CvSVM::train: |-
  Trains an SVM.
  
  The method trains the SVM model. It follows the conventions of the
  generic :ocvCvStatModel::train approach with the following limitations:
  
    Only the `CV_ROW_SAMPLE` data layout is supported.
  
  
    Input variables are all ordered.
  
  
    Output variables can be either categorical
  (`params.svm_type=CvSVM::C_SVC` or `params.svm_type=CvSVM::NU_SVC`),
  or ordered (`params.svm_type=CvSVM::EPS_SVR` or
  `params.svm_type=CvSVM::NU_SVR`), or not required at all
  (`params.svm_type=CvSVM::ONE_CLASS`).
  
  
    Missing measurements are not supported.
  
  
  All the other parameters are gathered in the :ocvCvSVMParams structure.
cv::CvSVM::train_auto: "Trains an SVM with optimal parameters.\n\n\
  The method trains the SVM model automatically by choosing the optimal\n\
  parameters `C`, `gamma`, `p`, `nu`, `coef0`, `degree` from\n\
  :ocvCvSVMParams. Parameters are considered optimal when the\n\
  cross-validation estimate of the test set error is minimal.\n\n\
  If there is no need to optimize a parameter, the corresponding grid step\n\
  should be set to any value less than or equal to 1. For example, to\n\
  avoid optimization in `gamma`, set `gamma_grid.step = 0`,\n\
  `gamma_grid.min_val`, `gamma_grid.max_val` as arbitrary numbers. In this\n\
  case, the value `params.gamma` is taken for `gamma`.\n\n\
  And, finally, if the optimization in a parameter is required but the\n\
  corresponding grid is unknown, you may call the function\n\
  :ocvCvSVM::get_default_grid. To generate a grid, for example, for\n\
  `gamma`, call `CvSVM::get_default_grid(CvSVM::GAMMA)`.\n\n\
  This function works for the classification\n\
  (`params.svm_type=CvSVM::C_SVC` or `params.svm_type=CvSVM::NU_SVC`) as\n\
  well as for the regression (`params.svm_type=CvSVM::EPS_SVR` or\n\
  `params.svm_type=CvSVM::NU_SVR`). If `params.svm_type=CvSVM::ONE_CLASS`,\n\
  no optimization is made and the usual SVM with parameters specified in\n\
  `params` is executed."
cv::CvSVM::predict: |-
  Predicts the response for input sample(s).
  
  If you pass one sample then prediction result is returned. If you want
  to get responses for several samples then you should pass the `results`
  matrix where prediction results will be stored.
  
  The function is parallelized with the TBB library.
cv::CvSVM::get_default_grid: "Generates a grid for SVM parameters.\n\n\
  The function generates a grid for the specified parameter of the SVM\n\
  algorithm. The grid may be passed to the function\n\
  :ocvCvSVM::train_auto."
cv::CvSVM::get_params: |-
  Returns the current SVM parameters.
  
  This function may be used to get the optimal parameters obtained while
  automatically training :ocvCvSVM::train_auto.
cv::CvSVM::get_support_vector: |-
  Retrieves a number of support vectors and the particular vector.
  
  The methods can be used to retrieve a set of support vectors.
cv::CvSVM::get_var_count: Returns the number of used features (variables count).
cv::createTrackbar: |-
  Creates a trackbar and attaches it to the specified window.
  
  The function `createTrackbar` creates a trackbar (a slider or range
  control) with the specified name and range, assigns a variable `value`
  to be a position synchronized with the trackbar and specifies the
  callback function `onChange` to be called on the trackbar position
  change. The created trackbar is displayed in the specified window
  `winname`.
  
  **note**
  
  **[Qt Backend Only]** `winname` can be empty (or NULL) if the trackbar
  should be attached to the control panel.
  
  
  Clicking the label of each trackbar enables editing the trackbar values
  manually.
cv::getTrackbarPos: |
  Returns the trackbar position.
  
  The function returns the current position of the specified trackbar.
  
  **note**
  
  **[Qt Backend Only]** `winname` can be empty (or NULL) if the trackbar
  is attached to the control panel.

cv::imshow: |
  Displays an image in the specified window.
  
  The function `imshow` displays an image in the specified window. If the
  window was created with the `CV_WINDOW_AUTOSIZE` flag, the image is
  shown with its original size. Otherwise, the image is scaled to fit the
  window. The function may scale the image, depending on its depth:
  
    If the image is 8-bit unsigned, it is displayed as is.
  
  
    If the image is 16-bit unsigned or 32-bit integer, the pixels are
  divided by 256. That is, the value range [0,255*256] is mapped to
  [0,255].
  
  
    If the image is 32-bit floating-point, the pixel values are
  multiplied by 255. That is, the value range [0,1] is mapped to
  [0,255].

cv::namedWindow: "\n\n\
  Creates a window.\n\n\
  The function `namedWindow` creates a window that can be used as a\n\
  placeholder for images and trackbars. Created windows are referred to by\n\
  their names.\n\n\
  If a window with the same name already exists, the function does\n\
  nothing.\n\n\
  You can call :ocvdestroyWindow or :ocvdestroyAllWindows to close the\n\
  window and de-allocate any associated memory usage. For a simple\n\
  program, you do not really have to call these functions because all the\n\
  resources and windows of the application are closed automatically by the\n\
  operating system upon exit.\n\n\
  **note**\n\n\
  Qt backend supports additional flags:\n\n  **CV_WINDOW_NORMAL or CV_WINDOW_AUTOSIZE:**\n\
  `CV_WINDOW_NORMAL` enables you to resize the window, whereas\n\
  `CV_WINDOW_AUTOSIZE` adjusts automatically the window size to\n\
  fit the displayed image (see :ocvimshow ), and you cannot change\n\
  the window size manually.\n\n\n  **CV_WINDOW_FREERATIO or CV_WINDOW_KEEPRATIO:**\n\
  `CV_WINDOW_FREERATIO` adjusts the image with no respect to its\n\
  ratio, whereas `CV_WINDOW_KEEPRATIO` keeps the image ratio.\n\n\n  **CV_GUI_NORMAL or CV_GUI_EXPANDED:** `CV_GUI_NORMAL` is the\n\
  old way to draw the window without statusbar and toolbar,\n\
  whereas `CV_GUI_EXPANDED` is a new enhanced GUI.\n\n\n\
  By default,\n\
  `flags == CV_WINDOW_AUTOSIZE | CV_WINDOW_KEEPRATIO | CV_GUI_EXPANDED`\n\n"
cv::destroyWindow: |-
  Destroys a window.
  
  The function `destroyWindow` destroys the window with the given name.
cv::destroyAllWindows: |-
  Destroys all of the HighGUI windows.
  
  The function `destroyAllWindows` destroys all of the opened HighGUI
  windows.
cv::MoveWindow: Moves window to the specified position
cv::ResizeWindow: |
  Resizes window to the specified size
  
  **note**
  
    The specified window size is for the image area. Toolbars are not
  counted.
  
  
    Only windows created without CV_WINDOW_AUTOSIZE flag can be
  resized.

cv::SetMouseCallback: "\n\n\
  Sets mouse handler for the specified window"
cv::setTrackbarPos: |
  Sets the trackbar position.
  
  The function sets the position of the specified trackbar in the
  specified window.
  
  **note**
  
  **[Qt Backend Only]** `winname` can be empty (or NULL) if the trackbar
  is attached to the control panel.

cv::waitKey: |
  Waits for a pressed key.
  
  The function `waitKey` waits for a key event infinitely (when
  $\texttt{delay}\leq 0$ ) or for `delay` milliseconds, when it is
  positive. Since the OS has a minimum time between switching threads, the
  function will not wait exactly `delay` ms, it will wait at least `delay`
  ms, depending on what else is running on your computer at that time. It
  returns the code of the pressed key or -1 if no key was pressed before
  the specified time had elapsed.
  
  **note**
  
  This function is the only method in HighGUI that can fetch and handle
  events, so it needs to be called periodically for normal event
  processing unless HighGUI is used within an environment that takes
  care of event processing.
  
  **note**
  
  The function only works if there is at least one HighGUI window
  created and the window is active. If there are several HighGUI
  windows, any of them can be active.

cv::alignPtr: |-
  Aligns a pointer to the specified number of bytes.
  
  The function returns the aligned pointer of the same type as the input
  pointer:
  
  $$\texttt{(_Tp*)(((size_t)ptr + n-1) & -n)}$$
cv::alignSize: |-
  Aligns a buffer size to the specified number of bytes.
  
  The function returns the minimum number that is greater or equal to `sz`
  and is divisible by `n` :
  
  $$\texttt{(sz + n-1) & -n}$$
cv::allocate: |-
  Allocates an array of elements.
  
  The generic function `allocate` allocates a buffer for the specified
  number of elements. For each element, the default constructor is called.
cv::deallocate: "Deallocates an array of elements.\n\n\
  The generic function `deallocate` deallocates the buffer allocated with\n\
  :ocvallocate . The number of elements must match the number passed to\n\
  :ocvallocate ."
cv::fastAtan2: |-
  Calculates the angle of a 2D vector in degrees.
  
  The function `fastAtan2` calculates the full-range angle of an input 2D
  vector. The angle is measured in degrees and varies from 0 to 360
  degrees. The accuracy is about 0.3 degrees.
cv::cubeRoot: |-
  Computes the cube root of an argument.
  
  The function `cubeRoot` computes $\sqrt[3]{\texttt{val}}$. Negative
  arguments are handled correctly. NaN and Inf are not handled. The
  accuracy approaches the maximum possible accuracy for single-precision
  data.
cv::Ceil: |-
  Rounds floating-point number to the nearest integer not smaller than the
  original.
  
  The function computes an integer `i` such that:
  
  $$i-1 < \texttt{value} \le i$$
cv::Floor: |-
  Rounds floating-point number to the nearest integer not larger than the
  original.
  
  The function computes an integer `i` such that:
  
  $$i \le \texttt{value} < i+1$$
cv::Round: Rounds floating-point number to the nearest integer
cv::IsInf: |-
  Determines if the argument is Infinity.
  
  The function returns 1 if the argument is a plus or minus infinity (as
  defined by IEEE754 standard) and 0 otherwise.
cv::IsNaN: |-
  Determines if the argument is Not A Number.
  
  The function returns 1 if the argument is Not A Number (as defined by
  IEEE754 standard), 0 otherwise.
cv::CV_Assert: |-
  Checks a condition at runtime and throws exception if it fails
  
  The macros `CV_Assert` (and `CV_DbgAssert`) evaluate the specified
  expression. If it is 0, the macros raise an error (see :ocverror ). The
  macro `CV_Assert` checks the condition in both Debug and Release
  configurations while `CV_DbgAssert` is only retained in the Debug
  configuration.
cv::error: |
  Signals an error and raises an exception.
  
  The function and the helper macros `CV_Error` and `CV_Error_`: :
  
  ```c++
  #define CV_Error( code, msg ) error(...)
  #define CV_Error_( code, args ) error(...)
  ```
  
  
  call the error handler. Currently, the error handler prints the error
  code ( `exc.code` ), the context ( `exc.file`,`exc.line` ), and the
  error message `exc.err` to the standard error stream `stderr` . In the
  Debug configuration, it then provokes memory access violation, so that
  the execution stack and all the parameters can be analyzed by the
  debugger. In the Release configuration, the exception `exc` is thrown.
  
  The macro `CV_Error_` can be used to construct an error message on-fly
  to include some dynamic information, for example: :
  
  ```c++
  // note the extra parentheses around the formatted text message
  CV_Error_(CV_StsOutOfRange,
      ("the matrix element (
      i, j, mtx.at<float>(i,j)))
  ```

cv::Exception: |-
  Exception class passed to an error. :
  
  ```c++
  class  Exception
  {
  public:
      // various constructors and the copy operation
      Exception() { code = 0; line = 0; }
      Exception(int _code, const String& _err,
                const String& _func, const String& _file, int _line);
      Exception(const Exception& exc);
      Exception& operator = (const Exception& exc);
  
      // the error code
      int code;
      // the error text message
      String err;
      // function name where the error happened
      String func;
      // the source file name where the error happened
      String file;
      // the source file line where the error happened
      int line;
  };
  ```
  
  
  The class `Exception` encapsulates all or almost all necessary
  information about the error happened in the program. The exception is
  usually constructed and thrown implicitly via `CV_Error` and `CV_Error_`
  macros. See :ocverror .
cv::fastMalloc: |-
  Allocates an aligned memory buffer.
  
  The function allocates the buffer of the specified size and returns it.
  When the buffer size is 16 bytes or more, the returned buffer is aligned
  to 16 bytes.
cv::fastFree: |-
  Deallocates a memory buffer.
  
  The function deallocates the buffer allocated with :ocvfastMalloc . If
  NULL pointer is passed, the function does nothing. C version of the
  function clears the pointer `*pptr` to avoid problems with double memory
  deallocation.
cv::format: |-
  Returns a text string formatted using the `printf`-like expression.
  
  The function acts like `sprintf` but forms and returns an STL string. It
  can be used to form an error message in the :ocvException constructor.
cv::getBuildInformation: |-
  Returns full configuration time cmake output.
  
  Returned value is raw cmake output including version control system
  revision, compiler version, compiler flags, enabled modules and third
  party libraries, etc. Output format depends on target architecture.
cv::checkHardwareSupport: |-
  Returns true if the specified feature is supported by the host hardware.
  
  The function returns true if the host hardware supports the specified
  feature. When user calls `setUseOptimized(false)`, the subsequent calls
  to `checkHardwareSupport()` will return false until
  `setUseOptimized(true)` is called. This way user can dynamically switch
  on and off the optimized code in OpenCV.
cv::getNumberOfCPUs: Returns the number of logical CPUs available for the process.
cv::getNumThreads: "Returns the number of threads used by OpenCV for parallel regions.\n\
  Always returns 1 if OpenCV is built without threading support.\n\n\
  The exact meaning of return value depends on the threading framework\n\
  used by OpenCV library:\n  **TBB** \xE2\x80\x93 The number of threads, that OpenCV will try to use for\n\
  parallel regions. If there is any `tbb::thread_scheduler_init` in\n\
  user code conflicting with OpenCV, then function returns default\n\
  number of threads used by TBB library.\n\n  **OpenMP** \xE2\x80\x93 An upper bound on the number of threads that could be\n\
  used to form a new team.\n\n  **Concurrency** \xE2\x80\x93 The number of threads, that OpenCV will try to\n\
  use for parallel regions.\n\n  **GCD** \xE2\x80\x93 Unsupported; returns the GCD thread pool limit (512) for\n\
  compatibility.\n\n  **C=** \xE2\x80\x93 The number of threads, that OpenCV will try to use for\n\
  parallel regions, if before called `setNumThreads` with\n\
  `threads > 0`, otherwise returns the number of logical CPUs,\n\
  available for the process.\n\n"
cv::getThreadNum: "Returns the index of the currently executed thread within the current\n\
  parallel region. Always returns 0 if called outside of parallel region.\n\n\
  The exact meaning of return value depends on the threading framework\n\
  used by OpenCV library:\n  **TBB** \xE2\x80\x93 Unsupported with current 4.1 TBB release. May be will be\n\
  supported in future.\n\n  **OpenMP** \xE2\x80\x93 The thread number, within the current team, of the\n\
  calling thread.\n\n  **Concurrency** \xE2\x80\x93 An ID for the virtual processor that the current\n\
  context is executing on (0 for master thread and unique number for\n\
  others, but not necessary 1,2,3,...).\n\n  **GCD** \xE2\x80\x93 System calling thread's ID. Never returns 0 inside\n\
  parallel region.\n\n  **C=** \xE2\x80\x93 The index of the current parallel task.\n\n"
cv::getTickCount: "Returns the number of ticks.\n\n\
  The function returns the number of ticks after the certain event (for\n\
  example, when the machine was turned on). It can be used to initialize\n\
  :ocvRNG or to measure a function execution time by reading the tick\n\
  count before and after the function call. See also the tick frequency."
cv::getTickFrequency: |
  Returns the number of ticks per second.
  
  The function returns the number of ticks per second. That is, the
  following code computes the execution time in seconds: :
  
  ```c++
  double t = (double)getTickCount();
  // do something ...
  t = ((double)getTickCount() - t)/getTickFrequency();
  ```

cv::getCPUTickCount: |-
  Returns the number of CPU ticks.
  
  The function returns the current number of CPU ticks on some
  architectures (such as x86, x64, PowerPC). On other platforms the
  function is equivalent to `getTickCount`. It can also be used for very
  accurate time measurements, as well as for RNG initialization. Note that
  in case of multi-CPU systems a thread, from which `getCPUTickCount` is
  called, can be suspended and resumed at another CPU with its own
  counter. So, theoretically (and practically) the subsequent calls to the
  function do not necessary return the monotonously increasing values.
  Also, since a modern CPU varies the CPU frequency depending on the load,
  the number of CPU clocks spent in some code cannot be directly converted
  to time units. Therefore, `getTickCount` is generally a preferable
  solution for measuring execution time.
cv::saturate_cast: |-
  Template function for accurate conversion from one primitive type to
  another.
  
  The functions `saturate_cast` resemble the standard C++ cast operations,
  such as `static_cast<T>()` and others. They perform an efficient and
  accurate conversion from one primitive type to another (see the
  introduction chapter). `saturate` in the name means that when the input
  value `v` is out of the range of the target type, the result is not
  formed just by taking low bits of the input, but instead the value is
  clipped. For example: :
  
  ```c++
  uchar a = saturate_cast<uchar>(-100); // a = 0 (UCHAR_MIN)
  short b = saturate_cast<short>(33333.33333); // b = 32767 (SHRT_MAX)
  ```
  
  
  Such clipping is done when the target type is `unsigned char` ,
  `signed char` , `unsigned short` or `signed short` . For 32-bit
  integers, no clipping is done.
  
  When the parameter is a floating-point value and the target type is an
  integer (8-, 16- or 32-bit), the floating-point value is first rounded
  to the nearest integer and then clipped if needed (when the target type
  is 8- or 16-bit).
  
  This operation is used in the simplest or most complex image processing
  functions in OpenCV.
cv::setNumThreads: "OpenCV will try to set the number of threads for the next parallel\n\
  region. If `threads == 0`, OpenCV will disable threading optimizations\n\
  and run all it's functions sequentially. Passing `threads < 0` will\n\
  reset threads number to system default. This function must be called\n\
  outside of parallel region.\n\n\
  OpenCV will try to run it's functions with specified threads number, but\n\
  some behaviour differs from framework:\n  **TBB** \xE2\x80\x93 User-defined parallel constructions will run with the\n\
  same threads number, if another does not specified. If late on\n\
  user creates own scheduler, OpenCV will be use it.\n\n  **OpenMP** \xE2\x80\x93 No special defined behaviour.\n\n  **Concurrency** \xE2\x80\x93 If `threads == 1`, OpenCV will disable threading\n\
  optimizations and run it's functions sequentially.\n\n  **GCD** \xE2\x80\x93 Supports only values <= 0.\n\n  **C=** \xE2\x80\x93 No special defined behaviour.\n\n"
cv::setUseOptimized: |-
  Enables or disables the optimized code.
  
  The function can be used to dynamically turn on and off optimized code
  (code that uses SSE2, AVX, and other instructions on the platforms that
  support it). It sets a global flag that is further checked by OpenCV
  functions. Since the flag is not checked in the inner OpenCV loops, it
  is only safe to call the function on the very top level in your
  application where you can be sure that no other OpenCV function is
  currently executed.
  
  By default, the optimized code is enabled unless you disable it in
  CMake. The current status can be retrieved using `useOptimized`.
cv::useOptimized: |-
  Returns the status of optimized code usage.
  
  The function returns `true` if the optimized code is enabled. Otherwise,
  it returns `false`.
cv::gpucodec::VideoReader: Video reader interface.
cv::gpucodec::VideoReader::nextFrame: |-
  Grabs, decodes and returns the next video frame.
  
  If no frames has been grabbed (there are no more frames in video file),
  the methods return `false` . The method throws :ocvException if error
  occurs.
cv::gpucodec::VideoReader::format: Returns information about video file format.
cv::gpucodec::Codec: Video codecs supported by :ocvgpucodec::VideoReader .
cv::gpucodec::ChromaFormat: Chroma formats supported by :ocvgpucodec::VideoReader .
cv::gpucodec::FormatInfo: |
  Struct providing information about video file format. :
  
  ```c++
  struct FormatInfo
  {
      Codec codec;
      ChromaFormat chromaFormat;
      int width;
      int height;
  };
  ```

cv::gpucodec::createVideoReader: |-
  Creates video reader.
  
  FFMPEG is used to read videos. User can implement own demultiplexing
  with :ocvgpucodec::RawVideoSource .
cv::gpucodec::RawVideoSource: |-
  Interface for video demultiplexing. :
  
  ```c++
  class RawVideoSource
  {
  public:
      virtual ~RawVideoSource() {}
  
      virtual bool getNextPacket(unsigned char** data, int* size, bool* endOfFile) = 0;
  
      virtual FormatInfo format() const = 0;
  };
  ```
  
  
  User can implement own demultiplexing by implementing this interface.
cv::gpucodec::RawVideoSource::getNextPacket: Returns next packet with RAW video frame.
cv::gpucodec::RawVideoSource::format: Returns information about video file format.
cv::gpucodec::VideoWriter: |
  Video writer interface.
  
  The implementation uses H264 video codec.
  
  **note**
  
  Currently only Windows platform is supported.

cv::gpucodec::VideoWriter::write: |-
  Writes the next video frame.
  
  The method write the specified image to video file. The image must have
  the same size and the same surface format as has been specified when
  opening the video writer.
cv::gpucodec::createVideoWriter: "Creates video writer.\n\n\
  The constructors initialize video writer. FFMPEG is used to write\n\
  videos. User can implement own multiplexing with\n\
  :ocvgpucodec::EncoderCallBack ."
cv::gpucodec::EncoderParams: |
  Different parameters for CUDA video encoder. :
  
  ```c++
  struct EncoderParams
  {
      int       P_Interval;      //    NVVE_P_INTERVAL,
      int       IDR_Period;      //    NVVE_IDR_PERIOD,
      int       DynamicGOP;      //    NVVE_DYNAMIC_GOP,
      int       RCType;          //    NVVE_RC_TYPE,
      int       AvgBitrate;      //    NVVE_AVG_BITRATE,
      int       PeakBitrate;     //    NVVE_PEAK_BITRATE,
      int       QP_Level_Intra;  //    NVVE_QP_LEVEL_INTRA,
      int       QP_Level_InterP; //    NVVE_QP_LEVEL_INTER_P,
      int       QP_Level_InterB; //    NVVE_QP_LEVEL_INTER_B,
      int       DeblockMode;     //    NVVE_DEBLOCK_MODE,
      int       ProfileLevel;    //    NVVE_PROFILE_LEVEL,
      int       ForceIntra;      //    NVVE_FORCE_INTRA,
      int       ForceIDR;        //    NVVE_FORCE_IDR,
      int       ClearStat;       //    NVVE_CLEAR_STAT,
      int       DIMode;          //    NVVE_SET_DEINTERLACE,
      int       Presets;         //    NVVE_PRESETS,
      int       DisableCabac;    //    NVVE_DISABLE_CABAC,
      int       NaluFramingType; //    NVVE_CONFIGURE_NALU_FRAMING_TYPE
      int       DisableSPSPPS;   //    NVVE_DISABLE_SPS_PPS
  
      EncoderParams();
      explicit EncoderParams(const String& configFile);
  
      void load(const String& configFile);
      void save(const String& configFile) const;
  };
  ```

cv::gpucodec::EncoderParams::EncoderParams: |-
  Constructors.
  
  Creates default parameters or reads parameters from config file.
cv::gpucodec::EncoderParams::load: Reads parameters from config file.
cv::gpucodec::EncoderParams::save: Saves parameters to config file.
cv::gpucodec::EncoderCallBack: |
  Callbacks for CUDA video encoder. :
  
  ```c++
  class EncoderCallBack
  {
  public:
      enum PicType
      {
          IFRAME = 1,
          PFRAME = 2,
          BFRAME = 3
      };
  
      virtual ~EncoderCallBack() {}
  
      virtual unsigned char* acquireBitStream(int* bufferSize) = 0;
      virtual void releaseBitStream(unsigned char* data, int size) = 0;
      virtual void onBeginFrame(int frameNumber, PicType picType) = 0;
      virtual void onEndFrame(int frameNumber, PicType picType) = 0;
  };
  ```

cv::gpucodec::EncoderCallBack::acquireBitStream: |-
  Callback function to signal the start of bitstream that is to be
  encoded.
  
  Callback must allocate buffer for CUDA encoder and return pointer to it
  and it's size.
cv::gpucodec::EncoderCallBack::releaseBitStream: |-
  Callback function to signal that the encoded bitstream is ready to be
  written to file.
cv::gpucodec::EncoderCallBack::onBeginFrame: |-
  Callback function to signal that the encoding operation on the frame has
  started.
cv::gpucodec::EncoderCallBack::onEndFrame: |-
  Callback function signals that the encoding operation on the frame has
  finished.
cv::detail::RotationWarper: |
  Rotation-only model image warper interface. :
  
  ```c++
  class CV_EXPORTS RotationWarper
  {
  public:
      virtual ~RotationWarper() {}
  
      virtual Point2f warpPoint(const Point2f &pt, const Mat &K, const Mat &R) = 0;
  
      virtual Rect buildMaps(Size src_size, const Mat &K, const Mat &R, Mat &xmap, Mat &ymap) = 0;
  
      virtual Point warp(const Mat &src, const Mat &K, const Mat &R, int interp_mode, int border_mode,
                         Mat &dst) = 0;
  
      virtual void warpBackward(const Mat &src, const Mat &K, const Mat &R, int interp_mode, int border_mode,
                                Size dst_size, Mat &dst) = 0;
  
      virtual Rect warpRoi(Size src_size, const Mat &K, const Mat &R) = 0;
  };
  ```

cv::detail::RotationWarper::warpPoint: Projects the image point.
cv::detail::RotationWarper::buildMaps: Builds the projection maps according to the given camera data.
cv::detail::RotationWarper::warp: Projects the image.
cv::detail::RotationWarper::warpBackward: Projects the image backward.
cv::detail::ProjectorBase: |
  Base class for warping logic implementation. :
  
  ```c++
  struct CV_EXPORTS ProjectorBase
  {
      void setCameraParams(const Mat &K = Mat::eye(3, 3, CV_32F),
                          const Mat &R = Mat::eye(3, 3, CV_32F),
                          const Mat &T = Mat::zeros(3, 1, CV_32F));
  
      float scale;
      float k[9];
      float rinv[9];
      float r_kinv[9];
      float k_rinv[9];
      float t[3];
  };
  ```

cv::detail::RotationWarperBase: |
  Base class for rotation-based warper using a detail::ProjectorBase_
  derived class. :
  
  ```c++
  template <class P>
  class CV_EXPORTS RotationWarperBase : public RotationWarper
  {
  public:
      Point2f warpPoint(const Point2f &pt, const Mat &K, const Mat &R);
  
      Rect buildMaps(Size src_size, const Mat &K, const Mat &R, Mat &xmap, Mat &ymap);
  
      Point warp(const Mat &src, const Mat &K, const Mat &R, int interp_mode, int border_mode,
              Mat &dst);
  
      void warpBackward(const Mat &src, const Mat &K, const Mat &R, int interp_mode, int border_mode,
                      Size dst_size, Mat &dst);
  
      Rect warpRoi(Size src_size, const Mat &K, const Mat &R);
  
  protected:
  
      // Detects ROI of the destination image. It's correct for any projection.
      virtual void detectResultRoi(Size src_size, Point &dst_tl, Point &dst_br);
  
      // Detects ROI of the destination image by walking over image border.
      // Correctness for any projection isn't guaranteed.
      void detectResultRoiByBorder(Size src_size, Point &dst_tl, Point &dst_br);
  
      P projector_;
  };
  ```

cv::detail::PlaneWarper: |
  Warper that maps an image onto the z = 1 plane. :
  
  ```c++
  class CV_EXPORTS PlaneWarper : public RotationWarperBase<PlaneProjector>
  {
  public:
      PlaneWarper(float scale = 1.f) { projector_.scale = scale; }
  
      void setScale(float scale) { projector_.scale = scale; }
  
      Point2f warpPoint(const Point2f &pt, const Mat &K, const Mat &R, const Mat &T);
  
      Rect buildMaps(Size src_size, const Mat &K, const Mat &R, const Mat &T, Mat &xmap, Mat &ymap);
  
      Point warp(const Mat &src, const Mat &K, const Mat &R, const Mat &T, int interp_mode, int border_mode,
                 Mat &dst);
  
      Rect warpRoi(Size src_size, const Mat &K, const Mat &R, const Mat &T);
  
  protected:
      void detectResultRoi(Size src_size, Point &dst_tl, Point &dst_br);
  };
  ```

cv::detail::PlaneWarper::PlaneWarper: Construct an instance of the plane warper class.
cv::detail::SphericalWarper: |
  Warper that maps an image onto the unit sphere located at the origin. :
  
  ```c++
  class CV_EXPORTS SphericalWarper : public RotationWarperBase<SphericalProjector>
  {
  public:
      SphericalWarper(float scale) { projector_.scale = scale; }
  
  protected:
      void detectResultRoi(Size src_size, Point &dst_tl, Point &dst_br);
  };
  ```

cv::detail::SphericalWarper::SphericalWarper: Construct an instance of the spherical warper class.
cv::detail::CylindricalWarper: |
  Warper that maps an image onto the x*x + z*z = 1 cylinder. :
  
  ```c++
  class CV_EXPORTS CylindricalWarper : public RotationWarperBase<CylindricalProjector>
  {
  public:
      CylindricalWarper(float scale) { projector_.scale = scale; }
  
  protected:
      void detectResultRoi(Size src_size, Point &dst_tl, Point &dst_br)
      {
          RotationWarperBase<CylindricalProjector>::detectResultRoiByBorder(src_size, dst_tl, dst_br);
      }
  };
  ```

cv::detail::CylindricalWarper::CylindricalWarper: Construct an instance of the cylindrical warper class.
cv::gpu::remap: |-
  Applies a generic geometrical transformation to an image.
  
  The function transforms the source image using the specified map:
  
  $$\texttt{dst} (x,y) =  \texttt{src} (xmap(x,y), ymap(x,y))$$
  
  Values of pixels with non-integer coordinates are computed using the
  bilinear interpolation.
cv::gpu::resize: Resizes an image.
cv::gpu::warpAffine: Applies an affine transformation to an image.
cv::gpu::buildWarpAffineMaps: Builds transformation maps for affine transformation.
cv::gpu::warpPerspective: Applies a perspective transformation to an image.
cv::gpu::buildWarpPerspectiveMaps: Builds transformation maps for perspective transformation.
cv::gpu::rotate: Rotates an image around the origin (0,0) and then shifts it.
cv::gpu::buildWarpPlaneMaps: Builds plane warping maps.
cv::gpu::buildWarpCylindricalMaps: Builds cylindrical warping maps.
cv::gpu::buildWarpSphericalMaps: Builds spherical warping maps.
cv::gpu::pyrDown: Smoothes an image and downsamples it.
cv::gpu::pyrUp: Upsamples an image and then smoothes it.
cv::XML/YAML file storages. Writing to a file storage.: |
  You can store and then restore various OpenCV data structures to/from
  XML (<http://www.w3c.org/XML>) or YAML (<http://www.yaml.org>) formats.
  Also, it is possible store and load arbitrarily complex data structures,
  which include OpenCV data structures, as well as primitive data types
  (integer and floating-point numbers and text strings) as their elements.
  
  Use the following procedure to write something to XML or YAML:
  :   1.  Create new :ocvFileStorage and open it for writing. It can be
          done with a single call to :ocvFileStorage::FileStorage
          constructor that takes a filename, or you can use the default
          constructor and then call :ocvFileStorage::open. Format of the
          file (XML or YAML) is determined from the filename extension
          (".xml" and ".yml"/".yaml", respectively)
      2.  Write all the data you want using the streaming operator `>>`,
          just like in the case of STL streams.
      3.  Close the file using :ocvFileStorage::release. `FileStorage`
          destructor also closes the file.
  
  Here is an example: :
  
  ```c++
  #include "opencv2/opencv.hpp"
  #include <time.h>
  
  using namespace cv;
  
  int main(int, char** argv)
  {
      FileStorage fs("test.yml", FileStorage::WRITE);
  
      fs << "frameCount" << 5;
      time_t rawtime; time(&rawtime);
      fs << "calibrationDate" << asctime(localtime(&rawtime));
      Mat cameraMatrix = (Mat_<double>(3,3) << 1000, 0, 320, 0, 1000, 240, 0, 0, 1);
      Mat distCoeffs = (Mat_<double>(5,1) << 0.1, 0.01, -0.001, 0, 0);
      fs << "cameraMatrix" << cameraMatrix << "distCoeffs" << distCoeffs;
      fs << "features" << "[";
      for( int i = 0; i < 3; i++ )
      {
          int x = rand() % 640;
          int y = rand() % 480;
          uchar lbp = rand() % 256;
  
          fs << "{:" << "x" << x << "y" << y << "lbp" << "[:";
          for( int j = 0; j < 8; j++ )
              fs << ((lbp >> j) & 1);
          fs << "]" << "}";
      }
      fs << "]";
      fs.release();
      return 0;
  }
  ```
  
  
  The sample above stores to XML and integer, text string (calibration
  date), 2 matrices, and a custom structure "feature", which includes
  feature coordinates and LBP (local binary pattern) value. Here is output
  of the sample:
  
  ~~~~ {.sourceCode .yaml}
  %YAML:1.0
  frameCount: 5
  calibrationDate: "Fri Jun 17 14:09:29 2011\n"
  cameraMatrix: !!opencv-matrix
     rows: 3
     cols: 3
     dt: d
     data: [ 1000., 0., 320., 0., 1000., 240., 0., 0., 1. ]
  distCoeffs: !!opencv-matrix
     rows: 5
     cols: 1
     dt: d
     data: [ 1.0000000000000001e-01, 1.0000000000000000e-02,
         -1.0000000000000000e-03, 0., 0. ]
  features:
     - { x:167, y:49, lbp:[ 1, 0, 0, 1, 1, 0, 1, 1 ] }
     - { x:298, y:130, lbp:[ 0, 0, 0, 1, 0, 0, 1, 1 ] }
     - { x:344, y:158, lbp:[ 1, 1, 0, 0, 0, 0, 1, 0 ] }
  ~~~~
  
  As an exercise, you can replace ".yml" with ".xml" in the sample above
  and see, how the corresponding XML file will look like.
  
  Several things can be noted by looking at the sample code and the output:
  :   *
      :   The produced YAML (and XML) consists of heterogeneous
          collections that can be nested. There are 2 types of
          collections: named collections (mappings) and unnamed
          collections (sequences). In mappings each element has a name and
          is accessed by name. This is similar to structures and
          `std::map` in C/C++ and dictionaries in Python. In sequences
          elements do not have names, they are accessed by indices. This
          is similar to arrays and `std::vector` in C/C++ and lists,
          tuples in Python. "Heterogeneous" means that elements of each
          single collection can have different types.
  
  ```c++
      Top-level collection in YAML/XML is a mapping. Each matrix is
      stored as a mapping, and the matrix elements are stored as a
      sequence. Then, there is a sequence of features, where each
      feature is represented a mapping, and lbp value in a nested
      sequence.
  
  \*
  :   When you write to a mapping (a structure), you write element
      name followed by its value. When you write to a sequence, you
      simply write the elements one by one. OpenCV data structures
      (such as cv::Mat) are written in absolutely the same way as
      simple C data structures - using **`<<`** operator.
  
  \*
  :   To write a mapping, you first write the special string **"{"**
      to the storage, then write the elements as pairs
      (`fs << <element_name> << <element_value>`) and then write the
      closing **"}"**.
  
  \*
  :   To write a sequence, you first write the special string **"["**,
      then write the elements, then write the closing **"]"**.
  
  \*
  :   In YAML (but not XML), mappings and sequences can be written in
      a compact Python-like inline form. In the sample above matrix
      elements, as well as each feature, including its lbp value, is
      stored in such inline form. To store a mapping/sequence in a
      compact form, put ":" after the opening character, e.g. use
      **"{:"** instead of **"{"** and **"[:"** instead of **"["**.
      When the data is written to XML, those extra ":" are ignored.
  ```

cv::Reading data from a file storage.: |
  To read the previously written XML or YAML file, do the following:
  
  #.
  :   Open the file storage using :ocvFileStorage::FileStorage
      constructor or :ocvFileStorage::open method. In the current
      implementation the whole file is parsed and the whole
      representation of file storage is built in memory as a hierarchy
      of file nodes (see :ocvFileNode)
  
  #.
  :   Read the data you are interested in. Use
      :ocvFileStorage::operator [], :ocvFileNode::operator [] and/or
      :ocvFileNodeIterator.
  
  #.
  :   Close the storage using :ocvFileStorage::release.
  
  Here is how to read the file created by the code sample above: :
  
  
  ```c++
  FileStorage fs2("test.yml", FileStorage::READ);
  
  // first method: use (type) operator on FileNode.
  int frameCount = (int)fs2["frameCount"];
  
  String date;
  // second method: use FileNode::operator >>
  fs2["calibrationDate"] >> date;
  
  Mat cameraMatrix2, distCoeffs2;
  fs2["cameraMatrix"] >> cameraMatrix2;
  fs2["distCoeffs"] >> distCoeffs2;
  
  cout << "frameCount: " << frameCount << endl
       << "calibration date: " << date << endl
       << "camera matrix: " << cameraMatrix2 << endl
       << "distortion coeffs: " << distCoeffs2 << endl;
  
  FileNode features = fs2["features"];
  FileNodeIterator it = features.begin(), it_end = features.end();
  int idx = 0;
  std::vector<uchar> lbpval;
  
  // iterate through a sequence using FileNodeIterator
  for( ; it != it_end; ++it, idx++ )
  {
      cout << "feature #" << idx << ": ";
      cout << "x=" << (int)(*it)["x"] << ", y=" << (int)(*it)["y"] << ", lbp: (";
      // you can also easily read numerical arrays using FileNode >> std::vector operator.
      (*it)["lbp"] >> lbpval;
      for( int i = 0; i < (int)lbpval.size(); i++ )
          cout << " " << (int)lbpval[i];
      cout << ")" << endl;
  }
  fs.release();
  ```

cv::FileStorage: |-
  XML/YAML file storage class that encapsulates all the information
  necessary for writing or reading data to/from a file.
cv::FileStorage::FileStorage: |-
  The constructors.
  
  The full constructor opens the file. Alternatively you can use the
  default constructor and then call :ocvFileStorage::open.
cv::FileStorage::open: |-
  Opens a file.
  
  See description of parameters in :ocvFileStorage::FileStorage. The
  method calls :ocvFileStorage::release before opening the file.
cv::FileStorage::isOpened: |-
  Checks whether the file is opened.
  
  It is a good practice to call this method after you tried to open a
  file.
cv::FileStorage::release: |-
  Closes the file and releases all the memory buffers.
  
  Call this method after all I/O operations with the storage are finished.
cv::FileStorage::releaseAndGetString: |-
  Closes the file and releases all the memory buffers.
  
  Call this method after all I/O operations with the storage are finished.
  If the storage was opened for writing data and `FileStorage::WRITE` was
  specified
cv::FileStorage::getFirstTopLevelNode: Returns the first element of the top-level mapping.
cv::FileStorage::root: Returns the top-level mapping
cv::FileStorage::operator[]: Returns the specified element of the top-level mapping.
cv::FileStorage::operator*: Returns the obsolete C FileStorage structure.
cv::FileStorage::writeRaw: "Writes multiple numbers.\n\n\
  Writes one or more numbers of the specified format to the currently\n\
  written structure. Usually it is more convenient to use\n\
  :ocvoperator << instead of this method."
cv::FileStorage::writeObj: |-
  Writes the registered C structure (CvMat, CvMatND, CvSeq).
  
  See :ocvWrite for details.
cv::FileStorage::getDefaultObjectName: Returns the normalized object name for the specified name of a file.
cv::operator <<: |-
  Writes data to a file storage.
  
  It is the main function to write data to a file storage. See an example
  of its usage at the beginning of the section.
cv::operator >>: |-
  Reads data from a file storage.
  
  It is the main function to read data from a file storage. See an example
  of its usage at the beginning of the section.
cv::FileNode: "File Storage Node class. The node is used to store each and every\n\
  element of the file storage opened for reading. When XML/YAML file is\n\
  read, it is first parsed and stored in the memory as a hierarchical\n\
  collection of nodes. Each node can be a \xE2\x80\x9Cleaf\xE2\x80\x9D that is contain a single\n\
  number or a string, or be a collection of other nodes. There can be\n\
  named collections (mappings) where each element has a name and it is\n\
  accessed by a name, and ordered collections (sequences) where elements\n\
  do not have names but rather accessed by index. Type of the file node\n\
  can be determined using :ocvFileNode::type method.\n\n\
  Note that file nodes are only used for navigating file storages opened\n\
  for reading. When a file storage is opened for writing, no data is\n\
  stored in memory after it is written."
cv::FileNode::FileNode: |-
  The constructors.
  
  These constructors are used to create a default file node, construct it
  from obsolete structures or from the another file node.
cv::FileNode::operator[]: Returns element of a mapping node or a sequence node.
cv::FileNode::type: Returns type of the node.
cv::FileNode::empty: Checks whether the node is empty.
cv::FileNode::isNone: Checks whether the node is a "none" object
cv::FileNode::isSeq: Checks whether the node is a sequence.
cv::FileNode::isMap: Checks whether the node is a mapping.
cv::FileNode::isInt: Checks whether the node is an integer.
cv::FileNode::isReal: Checks whether the node is a floating-point number.
cv::FileNode::isString: Checks whether the node is a text string.
cv::FileNode::isNamed: Checks whether the node has a name.
cv::FileNode::name: Returns the node name.
cv::FileNode::size: Returns the number of elements in the node.
cv::FileNode::operator int: Returns the node content as an integer.
cv::FileNode::operator float: Returns the node content as float.
cv::FileNode::operator double: Returns the node content as double.
cv::FileNode::operator String: Returns the node content as text string.
cv::FileNode::operator*: Returns pointer to the underlying obsolete file node structure.
cv::FileNode::begin: Returns the iterator pointing to the first node element.
cv::FileNode::end: |-
  Returns the iterator pointing to the element following the last node
  element.
cv::FileNode::readRaw: |-
  Reads node elements to the buffer with the specified format.
  
  Usually it is more convenient to use :ocvoperator >> instead of this
  method.
cv::FileNode::readObj: |-
  Reads the registered object.
  
  See :ocvRead for details.
cv::FileNodeIterator: |-
  The class `FileNodeIterator` is used to iterate through sequences and
  mappings. A standard STL notation, with `node.begin()`, `node.end()`
  denoting the beginning and the end of a sequence, stored in `node`. See
  the data reading sample in the beginning of the section.
cv::FileNodeIterator::FileNodeIterator: |-
  The constructors.
  
  These constructors are used to create a default iterator, set it to
  specific element in a file node or construct it from another iterator.
cv::FileNodeIterator::operator*: Returns the currently observed element.
cv::FileNodeIterator::operator->: Accesses methods of the currently observed element.
cv::FileNodeIterator::operator ++: |-
  Moves iterator to the next node.
  
  FileNodeIterator::operator -------------------------------Moves iterator
  to the previous node.
cv::FileNodeIterator::operator +=: Moves iterator forward by the specified offset.
cv::FileNodeIterator::operator -=: Moves iterator backward by the specified offset (possibly negative).
cv::FileNodeIterator::readRaw: |-
  Reads node elements to the buffer with the specified format.
  
  Usually it is more convenient to use :ocvoperator >> instead of this
  method.
